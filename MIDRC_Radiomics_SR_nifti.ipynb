{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project  bwh-midrc-rapid-res-1655321320"
      ],
      "metadata": {
        "id": "QZr5-mEOOxyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8424b26c-8054-49e9-a6fa-a6e865e91b9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Kk0d3lMmDp3"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bbM0Cf_94rqz"
      },
      "outputs": [],
      "source": [
        "  project_name = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  bucket_name = \"midrc-analysis-bwh\"\n",
        "  bucket_path = \"bpr-results/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZYEUFWcW6pG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "b65caa28-4cf1-4194-d573-905ddf906699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pynrrd) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from pynrrd) (1.21.6)\n",
            "Collecting nptyping\n",
            "  Downloading nptyping-2.3.1-py3-none-any.whl (32 kB)\n",
            "Collecting numpy>=1.11.1\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 8.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, nptyping, pynrrd\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "Successfully installed nptyping-2.3.1 numpy-1.21.5 pynrrd-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#install nrrd\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j79_pPeVRjkB",
        "outputId": "540c84fa-5873-4c34-8c00-632485a3380a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 52.8 MB 55.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Plastimatch\n",
        "import os\n",
        "\n",
        "!sudo apt install plastimatch \n",
        "!echo $(plastimatch --version)\n",
        "\n",
        "if os.path.isdir('/content/pyplastimatch'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/pyplastimatch')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"pyplastimatch\", err.strerror)) \n",
        "# !git clone https://github.com/denbonte/pyplastimatch/ pyplastimatch\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch.git \n",
        "\n",
        "# from pyplastimatch import pyplastimatch as pypla\n",
        "from pyplastimatch.pyplastimatch import pyplastimatch as pypla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-1TXwbUGiKL",
        "outputId": "51f42e7c-eb37-4007-c27b-c9d3dbbdddd6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "plastimatch is already the newest version (1.7.0+dfsg.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "plastimatch version 1.7.0\n",
            "Cloning into 'pyplastimatch'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 97 (delta 32), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.58 MiB | 21.11 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyradiomics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7xHab-k895-",
        "outputId": "4b0d2ff1-865d-40d4-dd4c-d5fa07416c5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyradiomics\n",
            "  Downloading pyradiomics-3.0.1-cp37-cp37m-manylinux1_x86_64.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.21.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.3.0)\n",
            "Requirement already satisfied: SimpleITK>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (2.2.0)\n",
            "Collecting pykwalify>=1.6.0\n",
            "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting ruamel.yaml>=0.16.0\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 50.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=2a28183c04edadc69302fcd6f6bd080cb17717d03eacda14266f531145125fe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, docopt, pykwalify, pyradiomics\n",
            "Successfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QImQTiwVnvxr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dicomweb-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HKGUrG1iAo5f"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import six\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.cloud import storage\n",
        "import nrrd\n",
        "import SimpleITK as sitk\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import transforms\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import radiomics\n",
        "from radiomics import featureextractor  # This module is used for interaction with pyradiomics\n",
        "\n",
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sR8c3D6j1Vvh"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project=bwh-midrc-rapid-res-1655321320 ct_limited_open_a1_r1\n",
        "\n",
        "WITH\n",
        "  nlst_instances_per_series AS (\n",
        "    SELECT\n",
        "      DISTINCT(StudyInstanceUID),\n",
        "      SeriesInstanceUID,\n",
        "      COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "      MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "      MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "      MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "      MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "      STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "    FROM\n",
        "      bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "    WHERE\n",
        "      (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "    GROUP BY\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID\n",
        "      ), \n",
        "  nlst_values_per_series AS (\n",
        "    SELECT \n",
        "    ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "    dicom_all.SeriesInstanceUID,\n",
        "    ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "    ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "    ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned\n",
        "  FROM\n",
        "    bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "  JOIN\n",
        "    nlst_instances_per_series\n",
        "  ON\n",
        "    dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "  WHERE\n",
        "    min_SliceThickness >= 1.5 \n",
        "    AND max_SliceThickness <= 3.5 \n",
        "    AND nlst_instances_per_series.num_instances > 100\n",
        "    AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "    AND nlst_instances_per_series.orientation_count = 1\n",
        "    AND has_localizer = \"false\"\n",
        "  GROUP BY\n",
        "    SeriesInstanceUID\n",
        "  )\n",
        "  SELECT \n",
        "    dicom_all.PatientID,\n",
        "    dicom_all.StudyInstanceUID,\n",
        "    dicom_all.SeriesInstanceUID,\n",
        "    dicom_all.SOPInstanceUID,\n",
        "    dicom_all.collection_id,\n",
        "    dicom_all.PatientAge,\n",
        "    dicom_all.PatientWeight,\n",
        "    nlst_values_per_series.num_instances,\n",
        "    nlst_values_per_series.SliceThickness,\n",
        "    nlst_values_per_series.PatientHeightScanned\n",
        "  FROM\n",
        "    bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "  JOIN\n",
        "    nlst_values_per_series \n",
        "  ON\n",
        "    dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_series(study_instance_uid, series_instance_uid, sop_instance_uids, dest_dir):\n",
        "  import pydicom\n",
        "  token = !gcloud auth print-access-token\n",
        "  token = token[0]\n",
        "\n",
        "  PROJECT_ID=\"bwh-midrc-rapid-res-1655321320\"\n",
        "  REGION=\"us-central1\"\n",
        "\n",
        "  DATASET_ID=\"midrc\"\n",
        "  DICOM_STORE_ID=\"midrc-dicom\"\n",
        "  \n",
        "  my_project = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  location = \"us-central1\"\n",
        "  dataset_id = \"midrc\"\n",
        "  dicom_store_id = \"midrc-dicom\"\n",
        "\n",
        "  url = f\"https://healthcare.googleapis.com/v1/projects/{my_project}/locations/{location}/datasets/{dataset_id}/dicomStores/{dicom_store_id}/dicomWeb\"\n",
        "  headers = {\n",
        "      \"Authorization\" : \"Bearer %s\" % token\n",
        "  }\n",
        "\n",
        "  import dicomweb_client\n",
        "\n",
        "  client = dicomweb_client.api.DICOMwebClient(url, headers=headers)\n",
        "\n",
        "  idx=0\n",
        "  for sop_instance_uid in sop_instance_uids:\n",
        "    retrievedInstance = client.retrieve_instance(\n",
        "                study_instance_uid=study_instance_uid,\n",
        "                series_instance_uid=series_instance_uid,\n",
        "                sop_instance_uid=sop_instance_uid)\n",
        "    pydicom.filewriter.write_file(f\"{dest_dir}/file{idx}.dcm\", retrievedInstance)\n",
        "    idx+=1"
      ],
      "metadata": {
        "id": "KE05zkHQnxrn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0q6ovBXnRWeY"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching %s for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "  \n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "urllib3_logger = logging.getLogger('urllib3')\n",
        "urllib3_logger.setLevel(logging.CRITICAL) # suppress messages upon download"
      ],
      "metadata": {
        "id": "RQjDEsQN_z71"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project = project_name)\n",
        "#bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "series_instance_uids = []\n",
        "\n",
        "#blobs = storage_client.list_blobs(bucket)\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path) #, delimiter='/') #don't search in non-axial sub-folder\n",
        "for blob in blobs:\n",
        "    bn = blob.name \n",
        "    # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "    bs = bn.split('.')\n",
        "    if bs[-1] == 'json':\n",
        "      #print(bn)\n",
        "      bss = bn.split('/')[1]\n",
        "      #print(bss)\n",
        "      bs3 = bss.split('.')\n",
        "      bs4 = '.'.join(bs3[:-1])\n",
        "      series_instance_uids.append(bs4)\n",
        "\n",
        "print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hEvqjcFUMoF",
        "outputId": "d541cdb0-e8e7-4a41-8e33-148712680824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(series_instance_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHEqz7uhwsq0",
        "outputId": "3542b56a-f9a9-42e9-e2f4-89c05908420f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project = project_name)\n",
        "#bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "series_instance_uids = []\n",
        "\n",
        "#blobs = storage_client.list_blobs(bucket)\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path, delimiter='/')\n",
        "for blob in blobs:\n",
        "    bn = blob.name \n",
        "    # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "    (r, ext) = os.path.splitext(bn)\n",
        "    #print(r,ext)\n",
        "    if ext == '.json':\n",
        "      u = os.path.split(r)[-1]\n",
        "      #print(u)\n",
        "      series_instance_uids.append(u)\n",
        "\n",
        "print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOCZFuo4YG6C",
        "outputId": "b4aabe67-ccd3-4a14-9935-b2a5af13a0f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(series_instance_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bZ63W3pNZyX",
        "outputId": "ef80aae2-539f-42c8-a44e-feb8c4d19f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = ct_limited_open_a1_r1\n",
        "k=0\n",
        "\n",
        "good_seriesuids = []\n",
        "\n",
        "#series_instance_uids = ['1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715']\n",
        "\n",
        "for seriesuid in series_instance_uids:\n",
        "  #print(seriesuid)\n",
        "  studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "  if len(studyuids)>0:\n",
        "    k=k+1\n",
        "    #print(seriesuid)\n",
        "    studyuid = studyuids[0]\n",
        "    sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "    print(len(sops))\n",
        "    #download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "    good_seriesuids.append(seriesuid)\n",
        "print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xxeSrvYoouz",
        "outputId": "670e2623-4ed3-433b-b982-3c6c8befc1c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178\n",
            "180\n",
            "145\n",
            "193\n",
            "111\n",
            "184\n",
            "152\n",
            "262\n",
            "194\n",
            "272\n",
            "152\n",
            "206\n",
            "271\n",
            "194\n",
            "114\n",
            "658\n",
            "106\n",
            "233\n",
            "997\n",
            "466\n",
            "209\n",
            "214\n",
            "427\n",
            "915\n",
            "172\n",
            "735\n",
            "262\n",
            "194\n",
            "115\n",
            "133\n",
            "133\n",
            "165\n",
            "704\n",
            "189\n",
            "199\n",
            "199\n",
            "202\n",
            "252\n",
            "118\n",
            "153\n",
            "122\n",
            "171\n",
            "165\n",
            "704\n",
            "165\n",
            "707\n",
            "170\n",
            "143\n",
            "158\n",
            "158\n",
            "301\n",
            "243\n",
            "123\n",
            "123\n",
            "107\n",
            "187\n",
            "132\n",
            "132\n",
            "115\n",
            "229\n",
            "103\n",
            "205\n",
            "163\n",
            "144\n",
            "141\n",
            "186\n",
            "118\n",
            "232\n",
            "265\n",
            "162\n",
            "115\n",
            "119\n",
            "131\n",
            "103\n",
            "186\n",
            "165\n",
            "140\n",
            "163\n",
            "237\n",
            "160\n",
            "131\n",
            "146\n",
            "199\n",
            "199\n",
            "365\n",
            "158\n",
            "137\n",
            "185\n",
            "201\n",
            "152\n",
            "163\n",
            "194\n",
            "208\n",
            "178\n",
            "217\n",
            "116\n",
            "113\n",
            "139\n",
            "137\n",
            "280\n",
            "163\n",
            "156\n",
            "132\n",
            "233\n",
            "124\n",
            "229\n",
            "113\n",
            "199\n",
            "149\n",
            "175\n",
            "143\n",
            "107\n",
            "175\n",
            "123\n",
            "207\n",
            "164\n",
            "114\n",
            "105\n",
            "157\n",
            "157\n",
            "130\n",
            "162\n",
            "126\n",
            "116\n",
            "194\n",
            "120\n",
            "149\n",
            "599\n",
            "145\n",
            "170\n",
            "137\n",
            "232\n",
            "141\n",
            "159\n",
            "179\n",
            "143\n",
            "167\n",
            "143\n",
            "135\n",
            "111\n",
            "321\n",
            "178\n",
            "171\n",
            "218\n",
            "190\n",
            "209\n",
            "319\n",
            "122\n",
            "150\n",
            "140\n",
            "212\n",
            "164\n",
            "230\n",
            "182\n",
            "200\n",
            "104\n",
            "143\n",
            "151\n",
            "301\n",
            "101\n",
            "178\n",
            "151\n",
            "179\n",
            "156\n",
            "209\n",
            "115\n",
            "172\n",
            "200\n",
            "219\n",
            "158\n",
            "204\n",
            "184\n",
            "233\n",
            "118\n",
            "168\n",
            "239\n",
            "126\n",
            "128\n",
            "136\n",
            "204\n",
            "140\n",
            "273\n",
            "176\n",
            "127\n",
            "149\n",
            "207\n",
            "128\n",
            "158\n",
            "146\n",
            "139\n",
            "164\n",
            "163\n",
            "168\n",
            "205\n",
            "135\n",
            "135\n",
            "140\n",
            "146\n",
            "173\n",
            "113\n",
            "156\n",
            "125\n",
            "139\n",
            "119\n",
            "216\n",
            "599\n",
            "141\n",
            "125\n",
            "123\n",
            "193\n",
            "200\n",
            "146\n",
            "153\n",
            "293\n",
            "194\n",
            "217\n",
            "201\n",
            "123\n",
            "261\n",
            "114\n",
            "107\n",
            "199\n",
            "145\n",
            "185\n",
            "103\n",
            "218\n",
            "189\n",
            "205\n",
            "108\n",
            "178\n",
            "115\n",
            "171\n",
            "120\n",
            "162\n",
            "189\n",
            "403\n",
            "229\n",
            "188\n",
            "108\n",
            "118\n",
            "194\n",
            "283\n",
            "200\n",
            "115\n",
            "143\n",
            "218\n",
            "136\n",
            "153\n",
            "130\n",
            "114\n",
            "136\n",
            "157\n",
            "137\n",
            "141\n",
            "207\n",
            "209\n",
            "233\n",
            "180\n",
            "113\n",
            "179\n",
            "145\n",
            "122\n",
            "105\n",
            "200\n",
            "179\n",
            "108\n",
            "108\n",
            "145\n",
            "149\n",
            "143\n",
            "150\n",
            "140\n",
            "182\n",
            "113\n",
            "181\n",
            "118\n",
            "284\n",
            "145\n",
            "116\n",
            "194\n",
            "163\n",
            "141\n",
            "137\n",
            "217\n",
            "113\n",
            "190\n",
            "199\n",
            "172\n",
            "167\n",
            "152\n",
            "114\n",
            "191\n",
            "208\n",
            "173\n",
            "219\n",
            "368\n",
            "163\n",
            "139\n",
            "209\n",
            "150\n",
            "218\n",
            "194\n",
            "149\n",
            "143\n",
            "101\n",
            "139\n",
            "218\n",
            "150\n",
            "124\n",
            "115\n",
            "160\n",
            "185\n",
            "193\n",
            "150\n",
            "150\n",
            "180\n",
            "133\n",
            "224\n",
            "199\n",
            "208\n",
            "204\n",
            "149\n",
            "146\n",
            "108\n",
            "155\n",
            "107\n",
            "179\n",
            "150\n",
            "163\n",
            "270\n",
            "132\n",
            "217\n",
            "229\n",
            "170\n",
            "151\n",
            "151\n",
            "445\n",
            "160\n",
            "163\n",
            "132\n",
            "211\n",
            "145\n",
            "218\n",
            "148\n",
            "150\n",
            "209\n",
            "130\n",
            "148\n",
            "207\n",
            "200\n",
            "178\n",
            "217\n",
            "167\n",
            "119\n",
            "265\n",
            "180\n",
            "178\n",
            "190\n",
            "142\n",
            "297\n",
            "141\n",
            "217\n",
            "193\n",
            "103\n",
            "330\n",
            "154\n",
            "218\n",
            "202\n",
            "196\n",
            "140\n",
            "129\n",
            "197\n",
            "142\n",
            "210\n",
            "143\n",
            "118\n",
            "163\n",
            "148\n",
            "119\n",
            "166\n",
            "139\n",
            "218\n",
            "132\n",
            "130\n",
            "154\n",
            "200\n",
            "170\n",
            "170\n",
            "158\n",
            "208\n",
            "204\n",
            "109\n",
            "199\n",
            "176\n",
            "108\n",
            "136\n",
            "182\n",
            "217\n",
            "138\n",
            "212\n",
            "153\n",
            "150\n",
            "233\n",
            "122\n",
            "200\n",
            "216\n",
            "151\n",
            "200\n",
            "204\n",
            "118\n",
            "131\n",
            "112\n",
            "113\n",
            "132\n",
            "142\n",
            "146\n",
            "207\n",
            "211\n",
            "135\n",
            "143\n",
            "107\n",
            "114\n",
            "125\n",
            "107\n",
            "148\n",
            "137\n",
            "182\n",
            "140\n",
            "739\n",
            "154\n",
            "289\n",
            "150\n",
            "137\n",
            "185\n",
            "217\n",
            "137\n",
            "218\n",
            "200\n",
            "126\n",
            "146\n",
            "128\n",
            "199\n",
            "204\n",
            "297\n",
            "200\n",
            "209\n",
            "154\n",
            "115\n",
            "161\n",
            "146\n",
            "134\n",
            "120\n",
            "161\n",
            "337\n",
            "201\n",
            "122\n",
            "149\n",
            "163\n",
            "142\n",
            "145\n",
            "157\n",
            "243\n",
            "139\n",
            "145\n",
            "107\n",
            "135\n",
            "140\n",
            "141\n",
            "136\n",
            "133\n",
            "172\n",
            "208\n",
            "140\n",
            "157\n",
            "149\n",
            "145\n",
            "194\n",
            "171\n",
            "136\n",
            "116\n",
            "123\n",
            "843\n",
            "208\n",
            "176\n",
            "142\n",
            "154\n",
            "226\n",
            "149\n",
            "126\n",
            "161\n",
            "165\n",
            "165\n",
            "209\n",
            "107\n",
            "494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of seriesuid's in the bucket\n",
        "storage_client = storage.Client()\n",
        "blobs = storage_client.list_blobs(bucket_name)\n",
        "s = []\n",
        "k=0\n",
        "for b in blobs:\n",
        "  #print('raw', b.name)\n",
        "  p = os.path.split(b.name)\n",
        "  #print(p)\n",
        "  # skip non-axial and folders\n",
        "  # temporarily autoremove any slashes from bucket_path\n",
        "  if p[0]==bucket_path.replace('/','') and p[1].split('.')[-1]=='nrrd':\n",
        "    k +=1\n",
        "    seriesuid_tmp = '.'.join(p[1].split('.')[:-1])\n",
        "    s.append(seriesuid_tmp)\n",
        "    if k%10==0:\n",
        "      print(k,' ', end='')\n",
        "    if k%120==0:\n",
        "        print('')\n",
        "seriesuid_list = list(set([x.split('_')[1] for x in s ]))\n",
        "print('\\n',len(seriesuid_list))"
      ],
      "metadata": {
        "id": "Ku6ZTZsevVDR",
        "outputId": "62c64dfa-a5ae-498d-cd61-3af821b371d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10  20  30  40  50  60  70  80  90  100  110  120  \n",
            "130  140  150  160  170  180  190  200  210  220  230  240  \n",
            "250  260  270  280  290  300  310  320  330  340  350  360  \n",
            "370  380  390  400  410  420  430  440  450  460  470  480  \n",
            "490  500  510  520  530  540  550  560  570  580  590  600  \n",
            "610  620  630  640  650  660  670  680  690  700  710  720  \n",
            "730  740  750  760  770  780  790  800  810  820  830  840  \n",
            "850  860  870  880  890  900  910  920  930  940  950  960  \n",
            "970  980  \n",
            " 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload files:\n",
        "# - parameter file for radiomics \n",
        "# - meta json for segmentation\n",
        "# - segment code mapping\n",
        "# - shape feature code mapping\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZjUBvc61FIuB",
        "outputId": "6007a009-9c6c-463a-e9b7-5235c6bf3bff"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17440d6f-1cc1-4971-b14e-ea8947bcf37a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17440d6f-1cc1-4971-b14e-ea8947bcf37a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving param_ct.yaml to param_ct.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the extractor\n",
        "  extractor = featureextractor.RadiomicsFeatureExtractor(fn_param)\n",
        "\n",
        "  # lung masks\n",
        "  lungs = fn_seg_nrrd\n",
        "  lungs, lungsheader = nrrd.read(lungs)\n",
        "\n",
        "  right_lung = np.zeros(lungs.shape)\n",
        "  temp =np.where(lungs==1) \n",
        "  right_lung[temp]=1\n",
        "\n",
        "  left_lung = np.zeros(lungs.shape)\n",
        "  temp =np.where(lungs==2) \n",
        "  left_lung[temp]=1\n",
        "\n",
        "  fn_right_lung = os.path.join(path_nrrd, 'rlung_%s.nrrd' % seriesuid)\n",
        "  fn_left_lung =  os.path.join(path_nrrd, 'llung_%s.nrrd' % seriesuid)\n",
        "\n",
        "  nrrd.write(fn_right_lung, right_lung, header = lungsheader)\n",
        "  nrrd.write(fn_left_lung, left_lung, header = lungsheader)\n",
        "\n",
        "  # process the lungs\n",
        "  result = extractor.execute(fn_ct_nrrd, fn_right_lung)\n",
        "  for key, val in six.iteritems(result):\n",
        "    print(\"\\t%s: %s\" %(key, val))\n",
        "  fn_out = '/content/radiomics_features_right_lung_%s.csv' % seriesuid\n",
        "  df = pd.DataFrame.from_dict(six.iteritems(result))# columns = ['Feature','Value'])\n",
        "  df.to_csv(fn_out, index=False)\n",
        "\n",
        "  result = extractor.execute(fn_ct_nrrd, fn_left_lung)\n",
        "  for key, val in six.iteritems(result):\n",
        "    print(\"\\t%s: %s\" %(key, val))\n",
        "  fn_out = '/content/radiomics_features_left_lung_%s.csv' % seriesuid\n",
        "  df = pd.DataFrame.from_dict(six.iteritems(result))# columns = ['Feature','Value'])\n",
        "  df.to_csv(fn_out, index=False)\n"
      ],
      "metadata": {
        "id": "xo_uITXHFlX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall highdicom\n",
        "!git clone https://github.com/herrmannlab/highdicom.git\n",
        "#!cd highdicom && python setup.py install\n",
        "!cd highdicom && pip install .\n",
        "\n",
        "!pip install pydicom\n"
      ],
      "metadata": {
        "id": "juNDFA2y988w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0c6bfb-4ba8-40d4-f055-eab984af0943"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping highdicom as it is not installed.\u001b[0m\n",
            "Cloning into 'highdicom'...\n",
            "remote: Enumerating objects: 5617, done.\u001b[K\n",
            "remote: Counting objects: 100% (2155/2155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (552/552), done.\u001b[K\n",
            "remote: Total 5617 (delta 1831), reused 1707 (delta 1529), pack-reused 3462\u001b[K\n",
            "Receiving objects: 100% (5617/5617), 3.08 MiB | 9.86 MiB/s, done.\n",
            "Resolving deltas: 100% (3651/3651), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/highdicom\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: pydicom>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (1.21.5)\n",
            "Requirement already satisfied: pillow>=8.3 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (9.3.0)\n",
            "Collecting pillow-jpls>=1.0\n",
            "  Downloading pillow_jpls-1.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: highdicom\n",
            "  Building wheel for highdicom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for highdicom: filename=highdicom-0.20.0-py3-none-any.whl size=800099 sha256=d95511b592d27855b7447cd213e6b5ccbdecf2e30686730300f1f48f6ec82c85\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lxxsqcv2/wheels/2c/3f/a0/cadbe6603e979b07733495973de6e1f45b81d2295bf6a358a3\n",
            "Successfully built highdicom\n",
            "Installing collected packages: pillow-jpls, highdicom\n",
            "Successfully installed highdicom-0.20.0 pillow-jpls-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for the structured report \n",
        "\n",
        "import highdicom\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import highdicom as hd\n",
        "\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "\n",
        "from highdicom.sr.content import (\n",
        "    FindingSite,\n",
        "    ImageRegion,\n",
        "    ImageRegion3D,\n",
        "    SourceImageForRegion,\n",
        "    SourceImageForMeasurement,\n",
        "    SourceImageForMeasurementGroup\n",
        ")\n",
        "from highdicom.sr.enum import GraphicTypeValues3D\n",
        "from highdicom.sr.enum import GraphicTypeValues\n",
        "from highdicom.sr.sop import Comprehensive3DSR, ComprehensiveSR\n",
        "from highdicom.sr.templates import (\n",
        "    DeviceObserverIdentifyingAttributes,\n",
        "    Measurement,\n",
        "    MeasurementProperties,\n",
        "    MeasurementReport,\n",
        "    MeasurementsAndQualitativeEvaluations,\n",
        "    ObservationContext,\n",
        "    ObserverContext,\n",
        "    PersonObserverIdentifyingAttributes,\n",
        "    PlanarROIMeasurementsAndQualitativeEvaluations,\n",
        "    RelationshipTypeValues,\n",
        "    TrackingIdentifier,\n",
        "    QualitativeEvaluation,\n",
        "    ImageLibrary,\n",
        "    ImageLibraryEntryDescriptors\n",
        ")\n",
        "from highdicom.sr.value_types import (\n",
        "    CodedConcept,\n",
        "    CodeContentItem,\n",
        ")\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"highdicom.sr.sop\")\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "F5TvCItyDlxs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_and_names_from_metadata_json(dicomseg_json):\n",
        "\n",
        "  \"\"\"Returns two lists containing the label values and the corresponding\n",
        "     CodeMeaning values\n",
        "\n",
        "  Inputs: \n",
        "    dicomseg_json : metajson file\n",
        "\n",
        "  Outputs:\n",
        "    label_values  : label values from the metajson file \n",
        "    label_names   : the corresponding CodeMeaning values \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  f = open(dicomseg_json)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  print(meta_json)\n",
        "\n",
        "  num_regions = len(meta_json['segmentAttributes'][0])\n",
        "  print ('num_regions: ' + str(num_regions))\n",
        "\n",
        "  label_values = []\n",
        "  label_names = [] \n",
        "  for n in range(0,num_regions):\n",
        "    # label_values.append(n)\n",
        "    label_value = meta_json['segmentAttributes'][0][n]['labelID']\n",
        "    #label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning']\n",
        "    # NS - \n",
        "    label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning'] +'_'+str(label_value)\n",
        "    label_values.append(label_value)\n",
        "    label_names.append(label_name)\n",
        "\n",
        "  return label_values, label_names"
      ],
      "metadata": {
        "id": "iO0T1wRcFVou"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uwTtO_ch2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_nii(input_file, output_directory, label_names):\n",
        "\n",
        "  \"\"\"Function to split a single multilabel nii into individual nii files. Used\n",
        "     for pyradiomics feature extraction. \n",
        "\n",
        "  Inputs: \n",
        "    input_file       : input multi-label nii file \n",
        "    output_directory : where to save the individual nii segments \n",
        "    label_names      : the names of the labels that correspond to the order of \n",
        "                       the values in the nii input_file \n",
        "\n",
        "  Outputs:\n",
        "    saves the individual nii files to the output_directory \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.isdir(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "  # save with the values in the files \n",
        "  nii = nib.load(input_file)\n",
        "  header = nii.header \n",
        "  img = nii.get_fdata() \n",
        "  unique_labels = list(np.unique(img))\n",
        "  unique_labels.remove(0) # remove the background \n",
        "\n",
        "  # split and save \n",
        "  num_labels = len(unique_labels)\n",
        "  for n in range(0,num_labels):\n",
        "    ind = np.where(img==unique_labels[n])\n",
        "    vol = np.zeros((img.shape))\n",
        "    vol[ind] = 1\n",
        "    new_img = nib.Nifti1Image(vol, nii.affine, nii.header)\n",
        "    output_filename = os.path.join(output_directory, label_names[n] + '.nii.gz')\n",
        "    nib.save(new_img, output_filename)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "Rm49POmEFeQe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    split_pred_nifti_path, \n",
        "                                    nnunet_shape_features_code_mapping_df):\n",
        "\n",
        "  \"\"\"Function to compute pyradiomics 3D features for each label in a nifti file. \n",
        "     \n",
        "\n",
        "  Inputs: \n",
        "    ct_nifti_path            : the CT nifti file \n",
        "    label_values             : the label value for each of the segments from the json file \n",
        "    label_names              : the corresponding label name for each of the segments \n",
        "    split_pred_nifti_path    : where to save the individual nii segments needed \n",
        "                               for pyradiomics\n",
        "    nnunet_shape_features_code_mapping_df : the df where we will obtain the \n",
        "                                            list of the shape features to \n",
        "                                            compute\n",
        "\n",
        "  Outputs:\n",
        "    Writes the features_csv_path_nnunet to disk. \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the names of the features from the nnunet_shape_features_code_mapping_df\n",
        "  shape_features = list(nnunet_shape_features_code_mapping_df['shape_feature'].values)\n",
        "\n",
        "  # Instantiate the extractor and modify the settings to keep the 3D shape features\n",
        "  extractor = featureextractor.RadiomicsFeatureExtractor(fn_param)\n",
        "  extractor.settings['minimumROIDimensions'] = 3 \n",
        "  extractor.disableAllFeatures()\n",
        "  extractor.enableFeaturesByName(shape=shape_features) \n",
        "\n",
        "  # Calculate features for each label and create a dataframe\n",
        "  num_labels = len([f for f in os.listdir(split_pred_nifti_path) if f.endswith('.nii.gz')]) # was .nii.gz\n",
        "  print(num_labels)\n",
        "  df_list = [] \n",
        "  for n in range(0,num_labels):\n",
        "    mask_path = os.path.join(split_pred_nifti_path, label_names[n] + '.nii.gz')  # was .nii.gz\n",
        "    #NS\n",
        "    print(mask_path)\n",
        "    print(ct_nifti_path)\n",
        "    print('zzzz')\n",
        "    # Run the extractor \n",
        "    result = extractor.execute(ct_nifti_path, mask_path) # dictionary\n",
        "    # keep only the features we want\n",
        "    # Get the corresponding label number -- all might not be present \n",
        "    corresponding_label_value = label_values[label_names.index(label_names[n])] \n",
        "    dict_keep = {'ReferencedSegment': corresponding_label_value, \n",
        "                 'label_name': label_names[n]}\n",
        "    keys_keep = [f for f in result.keys() if 'original_shape' in f]\n",
        "    # Just keep the feature keys we want\n",
        "    dict_keep_new_values = {key_keep: result[key_keep] for key_keep in keys_keep}\n",
        "    dict_keep.update(dict_keep_new_values)\n",
        "    df1 = pd.DataFrame([dict_keep])\n",
        "    # change values of columns to remove original_shape_\n",
        "    df1.columns = df1.columns.str.replace('original_shape_', '')\n",
        "    # Append to the ReferencedSegment and label_name df \n",
        "    df_list.append(df1)\n",
        "\n",
        "  # concat all label features \n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "SBIdR3ZpKhd9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted"
      ],
      "metadata": {
        "id": "HYjMey5mKq6O"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                         SOPInstanceUID_seg,\n",
        "                                                         seg_file, \n",
        "                                                         dcm_directory, \n",
        "                                                         segments_code_mapping_df,\n",
        "                                                         shape_features_code_mapping_df,\n",
        "                                                         df_features, \n",
        "                                                         SegmentAlgorithmName\n",
        "                                                         ):\n",
        "  \n",
        "  \"\"\"Function that creates the metajson necessary for the creation of a\n",
        "  structured report from a pandas dataframe of label names and features for \n",
        "  each. \n",
        "\n",
        "  Inputs: \n",
        "    SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                      file \n",
        "    SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "    seg_file                        : filename of SEG DCM file \n",
        "    dcm_directory                   : ct directory that will be sorted in \n",
        "                                      terms of axial ordering according to the \n",
        "                                      ImagePositionPatient and ImageOrientation \n",
        "                                      fields\n",
        "    segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                      segments and the associated code values etc.\n",
        "    shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                      features and the associated code values etc. \n",
        "    df_features                     : a pandas dataframe holding the segments and a \n",
        "                                      set of 3D shape features for each \n",
        "    SegmentAlgorithmName            : the name of the algorithm used to create the \n",
        "                                      segmentations - e.g. '3d_fullres_tta_nnUnet'\n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "  \"\"\" \n",
        "\n",
        "  # --- Get the version number for the pyradiomics package --- #\n",
        "\n",
        "  pyradiomics_version_number = str(radiomics.__version__)\n",
        "  \n",
        "  # --- Sort the dcm files first according to --- # \n",
        "  # --- ImagePositionPatient and ImageOrientation --- #\n",
        "\n",
        "  files_sorted, sop_all_sorted, pos_all_sorted = order_dicom_files_image_position(dcm_directory)\n",
        "  files_sorted = [os.path.basename(f) for f in files_sorted]\n",
        "\n",
        "  # --- Create the header for the json --- # \n",
        "  \n",
        "  inputMetadata = {}\n",
        "  inputMetadata[\"@schema\"]= \"https://raw.githubusercontent.com/qiicr/dcmqi/master/doc/schemas/sr-tid1500-schema.json#\"\n",
        "  # inputMetadata[\"SeriesDescription\"] = \"Measurements\"\n",
        "  inputMetadata[\"SeriesDescription\"] = SegmentAlgorithmName + '_' + \"Measurements\"\n",
        "  inputMetadata[\"SeriesNumber\"] = \"1001\"\n",
        "  inputMetadata[\"InstanceNumber\"] = \"1\"\n",
        "\n",
        "  inputMetadata[\"compositeContext\"] = [seg_file] # not full path\n",
        "\n",
        "  inputMetadata[\"imageLibrary\"] = files_sorted # not full path \n",
        "\n",
        "   # inputMetadata[\"observerContext\"] = {\n",
        "  #                                     \"ObserverType\": \"PERSON\",\n",
        "  #                                     \"PersonObserverName\": \"Reader1\"\n",
        "  #                                   }\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                     \"ObserverType\": \"DEVICE\",\n",
        "  #                     \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                     \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                   }\n",
        "  inputMetadata[\"observerContext\"] = {\n",
        "                      \"ObserverType\": \"DEVICE\",\n",
        "                      \"DeviceObserverName\": \"pyradiomics\",\n",
        "                      \"DeviceObserverModelName\": pyradiomics_version_number\n",
        "                    }\n",
        "\n",
        "  inputMetadata[\"VerificationFlag\"]  = \"UNVERIFIED\"\n",
        "  inputMetadata[\"CompletionFlag\"] =  \"COMPLETE\"\n",
        "  inputMetadata[\"activitySession\"] = \"1\"\n",
        "  inputMetadata[\"timePoint\"] = \"1\"\n",
        "\n",
        "  # ------------------------------------------------------------------------- # \n",
        "  # --- Create the measurement_dict for each segment - holds all features --- # \n",
        "\n",
        "  measurement = [] \n",
        "\n",
        "  # --- Now create the dict for all features and all segments --- #\n",
        "\n",
        "  # --- Loop over the number of segments --- #\n",
        "\n",
        "  # number of rows in the df_features \n",
        "  num_segments = df_features.shape[0]\n",
        "\n",
        "  # Array of dictionaries - one dictionary for each segment \n",
        "  measurement_across_segments_combined = [] \n",
        "\n",
        "  for segment_id in range(0,num_segments):\n",
        "\n",
        "    ReferencedSegment = df_features['ReferencedSegment'].values[segment_id]\n",
        "    FindingSite = df_features['label_name'].values[segment_id]\n",
        "\n",
        "    print('segment_id: ' + str(segment_id))\n",
        "    print('ReferencedSegment: ' + str(ReferencedSegment))\n",
        "    print('FindingSite: ' + str(FindingSite))\n",
        "\n",
        "    # --- Create the dict for the Measurements group --- # \n",
        "    TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment)\n",
        "\n",
        "    segment_row = segments_code_mapping_df[segments_code_mapping_df[\"segment\"] == FindingSite]\n",
        "    # print(segment_row)\n",
        "        \n",
        "    my_dict = {\n",
        "      \"TrackingIdentifier\": str(TrackingIdentifier),\n",
        "      \"ReferencedSegment\": int(ReferencedSegment),\n",
        "      \"SourceSeriesForImageSegmentation\": str(SeriesInstanceUID),\n",
        "      \"segmentationSOPInstanceUID\": str(SOPInstanceUID_seg),\n",
        "      \"Finding\": {\n",
        "        \"CodeValue\": \"113343008\",\n",
        "        \"CodingSchemeDesignator\": \"SCT\",\n",
        "        \"CodeMeaning\": \"Organ\"\n",
        "      }, \n",
        "      \"FindingSite\": {\n",
        "        \"CodeValue\": str(segment_row[\"FindingSite_CodeValue\"].values[0]),\n",
        "        \"CodingSchemeDesignator\": str(segment_row[\"FindingSite_CodingSchemeDesignator\"].values[0]),\n",
        "        \"CodeMeaning\": str(segment_row[\"FindingSite_CodeMeaning\"].values[0])\n",
        "      }\n",
        "    }\n",
        "\n",
        "    measurement = []  \n",
        "    # number of features - number of columns in df_features - 2 (label_name and ReferencedSegment)\n",
        "    num_values = len(df_features.columns)-2 \n",
        "\n",
        "    feature_list = df_features.columns[2:] # remove first two \n",
        "\n",
        "\n",
        "    # For each measurement per region segment\n",
        "    for n in range(0,num_values): \n",
        "      measurement_dict = {}\n",
        "      row = df_features.loc[df_features['label_name'] == FindingSite]\n",
        "      feature_row = shape_features_code_mapping_df.loc[shape_features_code_mapping_df[\"shape_feature\"] == feature_list[n]]\n",
        "      value = str(np.round(row[feature_list[n]].values[0],3))\n",
        "      measurement_dict[\"value\"] = value\n",
        "      measurement_dict[\"quantity\"] = {}\n",
        "      measurement_dict[\"quantity\"][\"CodeValue\"] = str(feature_row[\"quantity_CodeValue\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodingSchemeDesignator\"] = str(feature_row[\"quantity_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodeMeaning\"] = str(feature_row[\"quantity_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"units\"] = {}\n",
        "      measurement_dict[\"units\"][\"CodeValue\"] = str(feature_row[\"units_CodeValue\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodingSchemeDesignator\"] = str(feature_row[\"units_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodeMeaning\"] = str(feature_row[\"units_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"] = {}\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmName\"] = \"pyradiomics\"\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmVersion\"] = str(pyradiomics_version_number)\n",
        "      measurement.append(measurement_dict) \n",
        "\n",
        "    measurement_combined_dict = {}\n",
        "    measurement_combined_dict['measurementItems'] = measurement # measurement is an array of dictionaries \n",
        "\n",
        "    output_dict_one_segment = {**my_dict, **measurement_combined_dict}\n",
        "\n",
        "    # append to array for all segments \n",
        "\n",
        "    measurement_across_segments_combined.append(output_dict_one_segment)\n",
        "\n",
        "  # --- Add the measurement data --- # \n",
        "\n",
        "  inputMetadata[\"Measurements\"] = {}\n",
        "  inputMetadata[\"Measurements\"] = measurement_across_segments_combined\n",
        "\n",
        "  return inputMetadata\n"
      ],
      "metadata": {
        "id": "CDU5sEH3F9QK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7kCysKk3GwX",
        "outputId": "83b712df-a3b7-493b-f45d-3cdfacae8824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "gsutil cp gs://midrc-analysis-bwh/bpr-results/seg_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh/bpr-results/ct_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh/bpr-results/dcmseg_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952 /content/dcm_data\n",
            "converting NRRD to NII\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/nrrd_data/ct_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nrrd\n",
            "  --output-img /content/nifti_data/ct_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nii\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/nrrd_data/seg_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nrrd\n",
            "  --interpolation nn\n",
            "  --output-img /content/nifti_data/seg_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nii\n",
            "... Done.\n"
          ]
        }
      ],
      "source": [
        "# Processing series\n",
        "\n",
        "path_downloaded = '/content/downloaded_data'\n",
        "path_nifti = '/content/nifti_data'\n",
        "path_json =  '/content/json_data'\n",
        "path_nrrd = '/content/nrrd_data'\n",
        "path_dcm = '/content/dcm_data'\n",
        "path_labels = '/content/labels'\n",
        "\n",
        "good_seriesuids = ['1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952']\n",
        "\n",
        "#good_seriesuids = [seriesuid_list[0]]\n",
        "\n",
        "k=0\n",
        "for seriesuid in good_seriesuids:\n",
        "  k+=1\n",
        "  print(k)\n",
        "\n",
        "  # clean up from previous iterations and recreate temp directories\n",
        "  if 1:\n",
        "    for x in [path_downloaded, path_nifti, path_json, path_nrrd, path_dcm, path_labels]:\n",
        "      if os.path.isdir(x):\n",
        "        try:\n",
        "          shutil.rmtree(x)\n",
        "        except OSError as err:\n",
        "          print(\"Error: %s : %s\" % (x, err.strerror))  \n",
        "      os.mkdir(x)\n",
        "\n",
        "  file_gs_uri_seg = \"gs://%s/%sseg_%s.nrrd\" % (bucket_name, bucket_path, seriesuid) # \"/\"  exists in the path\n",
        "  file_gs_uri_ct = \"gs://%s/%sct_%s.nrrd\" % (bucket_name, bucket_path, seriesuid)\n",
        "  file_gs_uri_dcmseg = \"gs://%s/%sdcmseg_%s\" % (bucket_name, bucket_path, seriesuid)\n",
        "  \n",
        "  cmd1 = \"gsutil cp %s %s\" % (file_gs_uri_seg, path_nrrd)\n",
        "  print(cmd1)\n",
        "  os.system(cmd1)\n",
        "  \n",
        "  cmd2 = \"gsutil cp %s %s\" % (file_gs_uri_ct, path_nrrd)\n",
        "  print(cmd2)\n",
        "  os.system(cmd2)\n",
        "  \n",
        "  cmd3 = \"gsutil cp %s %s\" % (file_gs_uri_dcmseg, path_dcm)\n",
        "  print(cmd3)\n",
        "  os.system(cmd3)\n",
        "\n",
        "  fn_seg_nrrd = os.path.join(path_nrrd, 'seg_%s.nrrd' % seriesuid)\n",
        "  fn_ct_nrrd = os.path.join(path_nrrd, 'ct_%s.nrrd' % seriesuid)\n",
        "  fn_param = os.path.join('/content','param_ct.yaml')\n",
        "\n",
        "  print('converting NRRD to NII')\n",
        "  fn_ct_nifti = os.path.join(path_nifti,  \"ct_%s.nii\" % seriesuid)\n",
        "  log_file_path_ct_nifti = os.path.join(path_nifti, 'pypla_ct.log')\n",
        "  convert_args_ct = {\"input\" : fn_ct_nrrd, \"output-img\" : fn_ct_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_ct_nifti, **convert_args_ct)\n",
        "  \n",
        "  fn_seg_nifti = os.path.join(path_nifti,  \"seg_%s.nii\" % seriesuid)\n",
        "  log_file_path_seg_nifti = os.path.join(path_nifti, 'pypla_seg.log')\n",
        "  convert_args_seg = {\"input\" : fn_seg_nrrd, \"interpolation\" : \"nn\", \"output-img\" : fn_seg_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_seg_nifti, **convert_args_seg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fn_seg_dcm = os.path.join(path_dcm, 'dcmseg_%s' % seriesuid)\n",
        "fn_seg_code_mapping = os.path.join('/content','segments_code_mapping.csv')\n",
        "fn_feature_code_mapping = os.path.join('/content','shape_features_code_mapping.csv')\n",
        "fn_seg_meta = os.path.join('/content','lung_seg_meta.json')\n",
        "\n",
        "seg_code_mapping_df = pd.read_csv(fn_seg_code_mapping)\n",
        "feature_code_mapping_df = pd.read_csv(fn_feature_code_mapping)\n"
      ],
      "metadata": {
        "id": "zfWpUuKmRHGV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_values, label_names = get_label_and_names_from_metadata_json(fn_seg_meta)\n",
        "#label_names = get_label_and_names_from_metadata_json(fn_seg_meta)"
      ],
      "metadata": {
        "id": "AFC2mG2aiATA",
        "outputId": "daa2b51e-1bf4-4d42-cffc-be57ecb95e67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ContentCreatorName': 'MIDRC', 'ClinicalTrialSeriesID': '0', 'ClinicalTrialTimePointID': '0', 'SeriesDescription': 'Segmentation', 'SeriesNumber': '300', 'InstanceNumber': '1', 'BodyPartExamined': 'Chest', 'segmentAttributes': [[{'labelID': 1, 'SegmentDescription': 'Right lung', 'SegmentAlgorithmType': 'AUTOMATIC', 'SegmentAlgorithmName': 'lungmask R231', 'SegmentedPropertyCategoryCodeSequence': {'CodeValue': '123037004', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Anatomical Structure'}, 'SegmentedPropertyTypeCodeSequence': {'CodeValue': '39607008', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Lung'}, 'SegmentedPropertyTypeModifierCodeSequence': {'CodeValue': '24028007', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Right'}, 'recommendedDisplayRGBValue': [11, 156, 191]}, {'labelID': 2, 'SegmentDescription': 'Left lung', 'SegmentAlgorithmType': 'AUTOMATIC', 'SegmentAlgorithmName': 'lungmask R231', 'SegmentedPropertyCategoryCodeSequence': {'CodeValue': '123037004', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Anatomical Structure'}, 'SegmentedPropertyTypeCodeSequence': {'CodeValue': '39607008', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Lung'}, 'SegmentedPropertyTypeModifierCodeSequence': {'CodeValue': '7771000', 'CodingSchemeDesignator': 'SCT', 'CodeMeaning': 'Left'}, 'recommendedDisplayRGBValue': [91, 154, 223]}]], 'ContentLabel': 'SEGMENTATION', 'ContentDescription': 'Image segmentation', 'ClinicalTrialCoordinatingCenterName': 'dcmqi'}\n",
            "num_regions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib"
      ],
      "metadata": {
        "id": "w9oxkEpOsZBk"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_nii( fn_seg_nifti, path_labels, label_names )"
      ],
      "metadata": {
        "id": "eRtpIqJ3sBQK"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_values, label_names"
      ],
      "metadata": {
        "id": "R9lBlCHYmXY-",
        "outputId": "2cf09f0f-574d-4f09-9cfb-bd66e190af51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2], ['Lung_1', 'Lung_2'])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = dcmread(fn_seg_dcm)\n",
        "sop = ds.SOPInstanceUID\n",
        "print(sop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wke0StbFZhPf",
        "outputId": "90e874a1-6cb3-45df-b5df-3c204f94c6c7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.276.0.7230010.3.1.4.481034752.4023.1667497562.213485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.exists(fn_ct_nifti), path_nifti, path_labels"
      ],
      "metadata": {
        "id": "vfTURG5loFRX",
        "outputId": "4846c81a-5ad5-4190-a23c-7504d6031a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, '/content/nifti_data', '/content/labels')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = compute_pyradiomics_3D_features(fn_ct_nifti, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    path_labels, \n",
        "                                    feature_code_mapping_df)"
      ],
      "metadata": {
        "id": "V26sJMBRg7XG",
        "outputId": "63a9470f-7dba-42aa-e1a2-e877fbce4757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:radiomics.featureextractor:Loading parameter file /content/param_ct.yaml\n",
            "INFO:radiomics.featureextractor:Calculating features with label: 1\n",
            "INFO:radiomics.featureextractor:Loading image and mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "/content/labels/Lung_1.nii.gz\n",
            "/content/nifti_data/ct_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nii\n",
            "zzzz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:radiomics.imageoperations:Applying resampling from spacing [0.703125 0.703125 2.5     ] and size [512 512 145] to spacing [1. 1. 1.] and size [211, 232, 309]\n",
            "INFO:radiomics.featureextractor:Computing shape\n",
            "Feature Compactness1 is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature Compactness1 is deprecated, use with caution!\n",
            "Feature Compactness2 is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature Compactness2 is deprecated, use with caution!\n",
            "Feature SphericalDisproportion is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature SphericalDisproportion is deprecated, use with caution!\n",
            "INFO:radiomics.featureextractor:Adding image type \"Original\" with custom settings: {}\n",
            "INFO:radiomics.featureextractor:Adding image type \"LoG\" with custom settings: {'sigma': [1.0, 2.0, 3.0, 4.0, 5.0]}\n",
            "INFO:radiomics.featureextractor:Adding image type \"Wavelet\" with custom settings: {}\n",
            "INFO:radiomics.featureextractor:Calculating features for original image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 1\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-1-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 2\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-2-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 3\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-3-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 4\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-4-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 5\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-5-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LLH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LLH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LHL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LHL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LHH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LHH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HLL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HLL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HLH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HLH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HHL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HHL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HHH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HHH image\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LLL image\n",
            "INFO:radiomics.featureextractor:Calculating features with label: 1\n",
            "INFO:radiomics.featureextractor:Loading image and mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/labels/Lung_2.nii.gz\n",
            "/content/nifti_data/ct_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.nii\n",
            "zzzz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:radiomics.imageoperations:Applying resampling from spacing [0.703125 0.703125 2.5     ] and size [512 512 145] to spacing [1. 1. 1.] and size [156, 215, 232]\n",
            "INFO:radiomics.featureextractor:Computing shape\n",
            "Feature Compactness1 is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature Compactness1 is deprecated, use with caution!\n",
            "Feature Compactness2 is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature Compactness2 is deprecated, use with caution!\n",
            "Feature SphericalDisproportion is deprecated, use with caution!\n",
            "WARNING:radiomics.shape:Feature SphericalDisproportion is deprecated, use with caution!\n",
            "INFO:radiomics.featureextractor:Adding image type \"Original\" with custom settings: {}\n",
            "INFO:radiomics.featureextractor:Adding image type \"LoG\" with custom settings: {'sigma': [1.0, 2.0, 3.0, 4.0, 5.0]}\n",
            "INFO:radiomics.featureextractor:Adding image type \"Wavelet\" with custom settings: {}\n",
            "INFO:radiomics.featureextractor:Calculating features for original image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 1\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-1-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 2\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-2-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 3\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-3-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 4\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-4-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing LoG with sigma 5\n",
            "INFO:radiomics.featureextractor:Calculating features for log-sigma-5-0-mm-3D image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LLH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LLH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LHL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LHL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet LHH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LHH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HLL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HLL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HLH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HLH image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HHL\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HHL image\n",
            "INFO:radiomics.imageoperations:Computing Wavelet HHH\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-HHH image\n",
            "INFO:radiomics.featureextractor:Calculating features for wavelet-LLL image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_structured_report_metajson_for_shape_features(seriesuid, \n",
        "                                                         sop,\n",
        "                                                         fn_seg_dcm, \n",
        "                                                         path_downloaded, \n",
        "                                                         seg_code_mapping_df,\n",
        "                                                         feature_code_mapping_df,\n",
        "                                                         df_out, \n",
        "                                                         'lungmask'\n",
        "                                                         )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "sxhI-IX7GbOi",
        "outputId": "f3b83d11-ceee-4fa1-f718-fc6eb1fc63b9"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segment_id: 0\n",
            "ReferencedSegment: 1\n",
            "FindingSite: Lung_1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-4d6232a3e02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                          \u001b[0mfeature_code_mapping_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                          \u001b[0mdf_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                          \u001b[0;34m'lungmask'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                          )\n",
            "\u001b[0;32m<ipython-input-28-5f03ee41d531>\u001b[0m in \u001b[0;36mcreate_structured_report_metajson_for_shape_features\u001b[0;34m(SeriesInstanceUID, SOPInstanceUID_seg, seg_file, dcm_directory, segments_code_mapping_df, shape_features_code_mapping_df, df_features, SegmentAlgorithmName)\u001b[0m\n\u001b[1;32m    120\u001b[0m       }, \n\u001b[1;32m    121\u001b[0m       \"FindingSite\": {\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;34m\"CodeValue\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FindingSite_CodeValue\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;34m\"CodingSchemeDesignator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FindingSite_CodingSchemeDesignator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"CodeMeaning\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FindingSite_CodeMeaning\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_structured_report_for_shape_features(SeriesInstanceUID, \n",
        "                                              SOPInstanceUID_seg, \n",
        "                                              pred_dicomseg_path, \n",
        "                                              dicomseg_json_path, \n",
        "                                              dcm_directory, \n",
        "                                              pred_nifti_path, \n",
        "                                              nnunet_base_path, \n",
        "                                              ct_nifti_path, \n",
        "                                              segments_code_mapping_df,\n",
        "                                              shape_features_code_mapping_df,\n",
        "                                              sr_json_path,\n",
        "                                              sr_path, \n",
        "                                              SegmentAlgorithmName\n",
        "                                              ):\n",
        "  \n",
        "  \"\"\" This function creates the SR necessary for the nnUNet shape features \n",
        "\n",
        "  Inputs: \n",
        "  SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                    file \n",
        "  SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "  pred_dicomseg_path              : filename of DICOM SEG file \n",
        "  dicomseg_json_path              : json file for DICOM SEG file \n",
        "  dcm_directory                   : list of ct files that will be sorted in \n",
        "                                    terms of axial ordering according to the \n",
        "                                    ImagePositionPatient and ImageOrientation \n",
        "                                    fields\n",
        "  pred_nifti_path                 : predictions in nifti format \n",
        "  nnunet_base_path                : path to hold the split nifti files \n",
        "  ct_nifti_path                   : filename for CT nifti file\n",
        "  segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                    segments and the associated code values etc.\n",
        "  shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                    features and the associated code values etc. \n",
        "  sr_json_path                    : the path that the metajson for the SR for \n",
        "                                    the 3D shape features will be saved \n",
        "  sr_path                         : the path that the SR for the 3D shape \n",
        "                                    features will be saved \n",
        "  SegmentAlgorithmName            : the name of the algorithm used to create the \n",
        "                                    segmentations - e.g. '3d_fullres_tta_nnUnet'\n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # --- get label values and names from metajson file --- #\n",
        "  label_values, label_names = get_label_and_names_from_metadata_json(dicomseg_json_path)\n",
        "\n",
        "  # --- split the multilabel nifti into individual files --- #\n",
        "  split_pred_nii_path = os.path.join(nnunet_base_path, \"split_nii\")\n",
        "  if not os.path.isdir(split_pred_nii_path): \n",
        "    os.mkdir(split_pred_nii_path)\n",
        "  split_nii(pred_nifti_path, split_pred_nii_path, label_names)\n",
        "\n",
        "  # --- compute features and save csv for each region --- #\n",
        "  if not os.path.isdir(features_csv_path_nnunet):\n",
        "    os.mkdir(features_csv_path_nnunet) \n",
        "  df_features = compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                                label_values, \n",
        "                                                label_names,\n",
        "                                                split_pred_nii_path, \n",
        "                                                nnunet_shape_features_code_mapping_df)\n",
        "  print ('created df_features')\n",
        "  \n",
        "  # --- upload csv file to bucket --- #\n",
        "  # !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $pred_features_csv_path $gs_uri_features_csv_file\n",
        "\n",
        "  # remove nii files after saving out pyradiomics results\n",
        "  !rm $split_pred_nii_path/*\n",
        "  # remove csv \n",
        "  # !rm $pred_features_csv_path\n",
        "\n",
        "  # --- Create the final metadata for the SR --- #\n",
        "  inputMetadata = create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                                       SOPInstanceUID_seg,\n",
        "                                                                       pred_dicomseg_path, \n",
        "                                                                       dcm_directory, \n",
        "                                                                       nnunet_segments_code_mapping_df, \n",
        "                                                                       nnunet_shape_features_code_mapping_df,\n",
        "                                                                       df_features, \n",
        "                                                                       SegmentAlgorithmName)\n",
        "\n",
        "  print ('created SR json for shape features')\n",
        "\n",
        "  # --- Write out json --- #\n",
        "\n",
        "  with open(sr_json_path, 'w') as f:\n",
        "    json.dump(inputMetadata, f, indent=2)\n",
        "  print ('wrote out json for shape features')\n",
        "\n",
        "  # --- Save the SR for nnUNet shape features --- # \n",
        "  # inputImageLibraryDirectory = os.path.join(\"/content\", \"raw\")\n",
        "  # outputDICOM = os.path.join(\"/content\",\"features_sr.dcm\")\n",
        "  # inputCompositeContextDirectory = os.path.join(\"/content\",\"seg\")\n",
        "  inputImageLibraryDirectory = dcm_directory\n",
        "  # outputDICOM = sr_json_path\n",
        "  outputDICOM = sr_path\n",
        "  # the name of the folder where the seg files are located \n",
        "  inputCompositeContextDirectory = os.path.basename(pred_dicomseg_path) # might need to check this\n",
        "  inputMetadata_json = sr_json_path \n",
        "\n",
        "  print ('inputImageLibraryDirectory: ' + str(inputImageLibraryDirectory))\n",
        "  print ('outputDICOM: ' + str(outputDICOM))\n",
        "  print ('inputCompositeContextDirectory: ' + str(inputCompositeContextDirectory))\n",
        "  print ('inputMetadata_json: ' + str(inputMetadata_json)) \n",
        "  !tid1500writer --inputImageLibraryDirectory $inputImageLibraryDirectory \\\n",
        "                --outputDICOM $outputDICOM  \\\n",
        "                --inputCompositeContextDirectory $inputCompositeContextDirectory \\\n",
        "                --inputMetadata $inputMetadata_json\n",
        "  print ('wrote out SR for shape features')\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "ht9QRarlbQf9"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}