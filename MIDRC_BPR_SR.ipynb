{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project  bwh-midrc-rapid-res-1655321320"
      ],
      "metadata": {
        "id": "QZr5-mEOOxyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866b9652-c763-4df7-d570-9eb1cf195b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Kk0d3lMmDp3"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bbM0Cf_94rqz"
      },
      "outputs": [],
      "source": [
        "  project_name = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  bucket_name = \"midrc-analysis-bwh\"\n",
        "  bucket_path = \"bpr-results/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZYEUFWcW6pG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f493be0c-a46f-449d-d848-7bc38f47d1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pynrrd in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: nptyping in /usr/local/lib/python3.7/dist-packages (from pynrrd) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from pynrrd) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pynrrd) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "#install nrrd\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j79_pPeVRjkB",
        "outputId": "ffafa7f1-e55a-41eb-e5c4-60dd6aa6d1ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBBXdFGY-tqE",
        "outputId": "33ab6f50-2767-4da7-82d5-35334cc906de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64PiMsLhibpM"
      },
      "outputs": [],
      "source": [
        "# Install BPR\n",
        "import shutil\n",
        "import os\n",
        "if os.path.isdir('/content/BodyPartRegression'):\n",
        "  shutil.rmtree('/content/BodyPartRegression')\n",
        "!pip install torch==1.8.1 pytorch-lightning==1.2.10 torchtext==0.9.1 torchvision==0.9.1 torchaudio==0.8.1 dataclasses==0.6\n",
        "!pip install bpreg\n",
        "!git clone https://github.com/MIC-DKFZ/BodyPartRegression.git\n",
        "# !pip install torch==1.8.1 pytorch-lightning==1.2.10 torchtext==0.9.1\n",
        "\n",
        "import bpreg \n",
        "import seaborn as sb \n",
        "\n",
        "!pip install opencv-python-headless==4.1.2.30 # https://stackoverflow.com/questions/70537488/cannot-import-name-registermattype-from-cv2-cv2/70547274\n",
        "from BodyPartRegression.docs.notebooks.utils import * \n",
        "\n",
        "# Get revision number for bpreg \n",
        "!pip show bpreg > '/content/bpreg_output.txt'\n",
        "f = open('/content/bpreg_output.txt')\n",
        "d = {}\n",
        "for line in f:\n",
        "  (key, val) = line.split(': ')\n",
        "  d[key] = val\n",
        "bpr_revision_number = d['Version'].strip()\n",
        "print(bpr_revision_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QImQTiwVnvxr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dicomweb-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HKGUrG1iAo5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.cloud import storage\n",
        "import nrrd\n",
        "import SimpleITK as sitk\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import transforms\n",
        "from matplotlib.colors import ListedColormap\n",
        "import csv\n",
        "import pandas as pd\n",
        "import pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sR8c3D6j1Vvh"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project=bwh-midrc-rapid-res-1655321320 ct_limited_open_a1_r1\n",
        "\n",
        "WITH\n",
        "  nlst_instances_per_series AS (\n",
        "    SELECT\n",
        "      DISTINCT(StudyInstanceUID),\n",
        "      SeriesInstanceUID,\n",
        "      COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "      MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "      MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "      MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "      MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "      STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "    FROM\n",
        "      bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "    WHERE\n",
        "      (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "    GROUP BY\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID\n",
        "      ), \n",
        "  nlst_values_per_series AS (\n",
        "    SELECT \n",
        "    ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "    dicom_all.SeriesInstanceUID,\n",
        "    ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "    ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "    ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned\n",
        "  FROM\n",
        "    bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "  JOIN\n",
        "    nlst_instances_per_series\n",
        "  ON\n",
        "    dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "  WHERE\n",
        "    min_SliceThickness >= 1.5 \n",
        "    AND max_SliceThickness <= 3.5 \n",
        "    AND nlst_instances_per_series.num_instances > 100\n",
        "    AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "    AND nlst_instances_per_series.orientation_count = 1\n",
        "    AND has_localizer = \"false\"\n",
        "  GROUP BY\n",
        "    SeriesInstanceUID\n",
        "  )\n",
        "  SELECT \n",
        "    dicom_all.PatientID,\n",
        "    dicom_all.StudyInstanceUID,\n",
        "    dicom_all.SeriesInstanceUID,\n",
        "    dicom_all.SOPInstanceUID,\n",
        "    dicom_all.collection_id,\n",
        "    dicom_all.PatientAge,\n",
        "    dicom_all.PatientWeight,\n",
        "    nlst_values_per_series.num_instances,\n",
        "    nlst_values_per_series.SliceThickness,\n",
        "    nlst_values_per_series.PatientHeightScanned\n",
        "  FROM\n",
        "    bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "  JOIN\n",
        "    nlst_values_per_series \n",
        "  ON\n",
        "    dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_series(study_instance_uid, series_instance_uid, sop_instance_uids, dest_dir):\n",
        "  import pydicom\n",
        "  token = !gcloud auth print-access-token\n",
        "  token = token[0]\n",
        "\n",
        "  PROJECT_ID=\"bwh-midrc-rapid-res-1655321320\"\n",
        "  REGION=\"us-central1\"\n",
        "\n",
        "  DATASET_ID=\"midrc\"\n",
        "  DICOM_STORE_ID=\"midrc-dicom\"\n",
        "  \n",
        "  my_project = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  location = \"us-central1\"\n",
        "  dataset_id = \"midrc\"\n",
        "  dicom_store_id = \"midrc-dicom\"\n",
        "\n",
        "  url = f\"https://healthcare.googleapis.com/v1/projects/{my_project}/locations/{location}/datasets/{dataset_id}/dicomStores/{dicom_store_id}/dicomWeb\"\n",
        "  headers = {\n",
        "      \"Authorization\" : \"Bearer %s\" % token\n",
        "  }\n",
        "\n",
        "  import dicomweb_client\n",
        "\n",
        "  client = dicomweb_client.api.DICOMwebClient(url, headers=headers)\n",
        "\n",
        "  idx=0\n",
        "  for sop_instance_uid in sop_instance_uids:\n",
        "    retrievedInstance = client.retrieve_instance(\n",
        "                study_instance_uid=study_instance_uid,\n",
        "                series_instance_uid=series_instance_uid,\n",
        "                sop_instance_uid=sop_instance_uid)\n",
        "    pydicom.filewriter.write_file(f\"{dest_dir}/file{idx}.dcm\", retrievedInstance)\n",
        "    idx+=1"
      ],
      "metadata": {
        "id": "KE05zkHQnxrn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0q6ovBXnRWeY"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching %s for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "  \n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "urllib3_logger = logging.getLogger('urllib3')\n",
        "urllib3_logger.setLevel(logging.CRITICAL) # suppress messages upon download"
      ],
      "metadata": {
        "id": "RQjDEsQN_z71"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZjUBvc61FIuB",
        "outputId": "85b3762b-41fb-407c-b9d4-c325b7afac03"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0acd44d-fbfd-4baa-aee4-933392d0ab7a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0acd44d-fbfd-4baa-aee4-933392d0ab7a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bpr_landmarks_code_mapping.csv to bpr_landmarks_code_mapping.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project = project_name)\n",
        "#bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "series_instance_uids = []\n",
        "\n",
        "#blobs = storage_client.list_blobs(bucket)\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path) #, delimiter='/') #don't search in non-axial sub-folder\n",
        "for blob in blobs:\n",
        "    bn = blob.name \n",
        "    # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "    bs = bn.split('.')\n",
        "    if bs[-1] == 'json':\n",
        "      #print(bn)\n",
        "      bss = bn.split('/')[1]\n",
        "      #print(bss)\n",
        "      bs3 = bss.split('.')\n",
        "      bs4 = '.'.join(bs3[:-1])\n",
        "      series_instance_uids.append(bs4)\n",
        "\n",
        "print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hEvqjcFUMoF",
        "outputId": "d541cdb0-e8e7-4a41-8e33-148712680824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(series_instance_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHEqz7uhwsq0",
        "outputId": "3542b56a-f9a9-42e9-e2f4-89c05908420f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project = project_name)\n",
        "#bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "series_instance_uids = []\n",
        "\n",
        "#blobs = storage_client.list_blobs(bucket)\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path, delimiter='/')\n",
        "for blob in blobs:\n",
        "    bn = blob.name \n",
        "    # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "    (r, ext) = os.path.splitext(bn)\n",
        "    #print(r,ext)\n",
        "    if ext == '.json':\n",
        "      u = os.path.split(r)[-1]\n",
        "      #print(u)\n",
        "      series_instance_uids.append(u)\n",
        "\n",
        "print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOCZFuo4YG6C",
        "outputId": "99f14de1-939a-4e60-c71b-0cb16cf1dea1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(series_instance_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bZ63W3pNZyX",
        "outputId": "adc7f60e-8c80-497b-db3d-6994b8643c65"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = ct_limited_open_a1_r1\n",
        "k=0\n",
        "\n",
        "good_seriesuids = []\n",
        "\n",
        "#series_instance_uids = ['1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715']\n",
        "\n",
        "for seriesuid in series_instance_uids:\n",
        "  #print(seriesuid)\n",
        "  studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "  if len(studyuids)>0:\n",
        "    k=k+1\n",
        "    #print(seriesuid)\n",
        "    studyuid = studyuids[0]\n",
        "    sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "    print(len(sops))\n",
        "    #download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "    good_seriesuids.append(seriesuid)\n",
        "print(k)"
      ],
      "metadata": {
        "id": "4xxeSrvYoouz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall highdicom\n",
        "!git clone https://github.com/herrmannlab/highdicom.git\n",
        "#!cd highdicom && python setup.py install\n",
        "!cd highdicom && pip install ."
      ],
      "metadata": {
        "id": "juNDFA2y988w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for the structured report \n",
        "\n",
        "import highdicom\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import highdicom as hd\n",
        "\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "\n",
        "from highdicom.sr.content import (\n",
        "    FindingSite,\n",
        "    ImageRegion,\n",
        "    ImageRegion3D,\n",
        "    SourceImageForRegion,\n",
        "    SourceImageForMeasurement,\n",
        "    SourceImageForMeasurementGroup\n",
        ")\n",
        "from highdicom.sr.enum import GraphicTypeValues3D\n",
        "from highdicom.sr.enum import GraphicTypeValues\n",
        "from highdicom.sr.sop import Comprehensive3DSR, ComprehensiveSR\n",
        "from highdicom.sr.templates import (\n",
        "    DeviceObserverIdentifyingAttributes,\n",
        "    Measurement,\n",
        "    MeasurementProperties,\n",
        "    MeasurementReport,\n",
        "    MeasurementsAndQualitativeEvaluations,\n",
        "    ObservationContext,\n",
        "    ObserverContext,\n",
        "    PersonObserverIdentifyingAttributes,\n",
        "    PlanarROIMeasurementsAndQualitativeEvaluations,\n",
        "    RelationshipTypeValues,\n",
        "    TrackingIdentifier,\n",
        "    QualitativeEvaluation,\n",
        "    ImageLibrary,\n",
        "    ImageLibraryEntryDescriptors\n",
        ")\n",
        "from highdicom.sr.value_types import (\n",
        "    CodedConcept,\n",
        "    CodeContentItem,\n",
        ")\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"highdicom.sr.sop\")\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "GqSSJZ0ULbWt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR regions"
      ],
      "metadata": {
        "id": "lpxpOp2-yNWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes as input a json file and slice_index, returns the regions assigned to the slice \n",
        "def convert_slice_to_region(bpr_data, slice_index):\n",
        "\n",
        "  \"\"\" \n",
        "  Given the slice_index, this returns a list of corresponding regions that match\n",
        "\n",
        "  Inputs: \n",
        "    bpr_data    : a dictionary, where for each of the six regions, a list of \n",
        "                  slice indices are provided \n",
        "    slice_index : slice number you want to obtain the list of classified regions \n",
        "                  for\n",
        "  Returns\n",
        "    regions     : list of regions for that slice_index\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # find where the slice_index appears across all regions\n",
        "  # get the names of the regions\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  region_names = list(bpr_data.keys())\n",
        "  regions = [] \n",
        "\n",
        "  for n in range(0,num_regions):\n",
        "    vals = bpr_data[region_names[n]]\n",
        "    if slice_index in vals:\n",
        "      regions.append(region_names[n])\n",
        "\n",
        "  return regions"
      ],
      "metadata": {
        "id": "1KHbBceCei7Y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_regions(files, \n",
        "                                                              json_file, \n",
        "                                                              output_SR_file, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              bpr_regions_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "  # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # print(ds['0x0020','0x0037'].value)\n",
        "    ImageOrientation = ds['0x0020','0x0037'].value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    sop = ds['0x0008', '0x0018'].value\n",
        "    sop_all.append(sop)\n",
        "\n",
        "#----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  bpr_data = json_data['body part examined']\n",
        "\n",
        "  # return a list where each entry is per slice and is a array of possible regions \n",
        "  bpr_slice_scores = json_data['cleaned slice scores']\n",
        "  num_slices = len(bpr_slice_scores)\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  regions = [] \n",
        "\n",
        "  # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "  for slice_index in range(0,num_slices):\n",
        "    region = convert_slice_to_region(bpr_data, slice_index)\n",
        "    regions.append(region)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  observer_person_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Person,\n",
        "      observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "          name='Anonymous^Reader'\n",
        "      )\n",
        "  )\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "  #     )\n",
        "  observer_device_context = ObserverContext(\n",
        "    observer_type=codes.DCM.Device,\n",
        "    observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "        uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "        model_name = bpr_revision_number\n",
        "    )\n",
        "  )\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  tracking_uid = generate_uid()\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  print('num_slices: ' + str(num_slices))\n",
        "\n",
        "#----------- Per slice ---------#\n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    slice_region = regions[n]\n",
        "\n",
        "    # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "    # ----- per region ---- # \n",
        "    qualitative_evaluations = [] \n",
        "    # num_regions = len(regions)\n",
        "    num_regions = len(slice_region)\n",
        "\n",
        "    # for region in regions: \n",
        "    for region in slice_region: \n",
        "      row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "      qualitative_evaluations.append(\n",
        "          QualitativeEvaluation(\n",
        "              CodedConcept(\n",
        "                            value=str(row[\"target_CodeValue\"].values[0]),\n",
        "                            meaning=str(row[\"target_CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "                            # meaning = \"Target Region\",\n",
        "                            scheme_designator=str(row[\"target_CodingSchemeDesignator\"].values[0])\n",
        "                            ), \n",
        "              CodedConcept(\n",
        "                            value=str(row[\"CodeValue\"].values[0]),\n",
        "                            meaning=str(row[\"CodeMeaning\"].values[0]),\n",
        "                            scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "                          )\n",
        "              )\n",
        "          )\n",
        "\n",
        "    # In the correct order \n",
        "    reference_dcm_file = files_sorted[n]\n",
        "    image_dataset = dcmread(reference_dcm_file)\n",
        "    evidence.append(image_dataset)\n",
        "\n",
        "    # NS\n",
        "    print(evidence[0])\n",
        "\n",
        "    src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "    # tracking_id = \"Annotations group x\"\n",
        "    tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "    measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                  tracking_identifier=TrackingIdentifier(\n",
        "                      uid=tracking_uid,\n",
        "                      identifier=tracking_id\n",
        "                  ),\n",
        "                  qualitative_evaluations=qualitative_evaluations,\n",
        "                  source_images=[src_image]\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    imaging_measurements.append(\n",
        "      measurements_group\n",
        "            )\n",
        "    \n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=100,\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='MIDRC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR region annotations'\n",
        "  )\n",
        "  \n",
        "\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset"
      ],
      "metadata": {
        "id": "1opa_coW8an5"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR landmarks"
      ],
      "metadata": {
        "id": "tU89HKmmyAYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(json_file):\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "WmzQFV7_63U3"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indices_from_json(filename, tag_start, tag_end):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of the particular anatomy specified by the tag_start and \n",
        "  tag_end for a particular patient. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "    tag_start : required - the string for the start of the anatomical region \n",
        "    tag_end   : required - the string for the end of the anatomical region \n",
        "\n",
        "  Outputs:\n",
        "    min_index : the minimum index in the patient coordinate system\n",
        "    max_index : the maximum index in the patient coordinate system \n",
        "  \"\"\"\n",
        "\n",
        "  # These scores are the same for all patients  \n",
        "  x = load_json(filename)\n",
        "\n",
        "  start_score = x[\"look-up table\"][tag_start][\"mean\"]\n",
        "  end_score = x[\"look-up table\"][tag_end][\"mean\"]\n",
        "\n",
        "  # The actual indices \n",
        "  min_index, max_index = crop_scores(x[\"cleaned slice scores\"], start_score, end_score)\n",
        "\n",
        "  return min_index, max_index"
      ],
      "metadata": {
        "id": "9dkNVhl_xMIW"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_from_json(filename):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of each landmark in the patient json filename and converts \n",
        "  it to slice indices. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "\n",
        "  Outputs:\n",
        "    list of the corresponding landmark slices in the space of the patient. \n",
        "    Landmarks at the extreme ends - most inferior and most superior axial slices\n",
        "    are removed. \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the cleaned slice scores to determine the number of slices \n",
        "  x = load_json(filename)\n",
        "  num_slices = len(x[\"cleaned slice scores\"])\n",
        "\n",
        "  # Get the list of landmarks \n",
        "  landmarks = list(x[\"look-up table\"].keys())\n",
        "  num_landmarks = len(landmarks)\n",
        "\n",
        "  # Get the expected z_spacing - if less than 0, slices are in reverse order \n",
        "  valid_z_spacing = x[\"valid z-spacing\"]\n",
        "\n",
        "  # Get values for all tags \n",
        "  # Reorder the landmarks according to the mean values in ascending order \n",
        "  landmarks_dict_sorted = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    landmarks_dict_sorted[landmark] = x[\"look-up table\"][landmark]['mean']\n",
        "  landmark_dict_sorted = dict(sorted(landmarks_dict_sorted.items(), key=lambda item: item[1]))\n",
        "  landmarks = list(landmark_dict_sorted.keys())\n",
        "\n",
        "  # Calculate the actual slice indices of each landmark \n",
        "  landmark_indices = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    score = landmark_dict_sorted[landmark] # x[\"look-up table\"][landmark][\"mean\"]\n",
        "    min_index, max_index = crop_scores(x[\"cleaned slice scores\"], score, score)\n",
        "    if (valid_z_spacing > 0): \n",
        "      landmark_indices[landmark] = min_index \n",
        "    else: \n",
        "      landmark_indices[landmark] = num_slices - min_index \n",
        "\n",
        "  # Programmatically remove values from dictionary if it is at most inferior\n",
        "  # or most superior slice \n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    if (landmark_indices[landmark]==0):\n",
        "      del landmark_indices[landmark]\n",
        "    elif (landmark_indices[landmark]==num_slices-1):\n",
        "      del landmark_indices[landmark]\n",
        "\n",
        "  return landmark_indices"
      ],
      "metadata": {
        "id": "9XwAA1btxUen"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_list_in_slice(landmark_indices, slice_number):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the list of landmarks that are assigned to a particular slice number.\n",
        "\n",
        "  Arguments:\n",
        "    flandmark_indices  : the dictionary holding the slice number for each \n",
        "                         landmark\n",
        "    slice_number       : the particular slice to obtain the list of landmarks\n",
        "                         for\n",
        "\n",
        "  Outputs:\n",
        "    list of the landmarks that correspond to the slice_number\n",
        "  \"\"\"\n",
        "\n",
        "  key_list = list()\n",
        "  items_list = landmark_indices.items()\n",
        "  for item in items_list:\n",
        "    if item[1] == slice_number:\n",
        "        key_list.append(item[0])\n",
        "\n",
        "  return key_list\n",
        "\n",
        "def convert_landmarks_list_to_qualitative_evaluations(landmark_list,\n",
        "                                                      landmarks_df):\n",
        "\n",
        "  \"\"\"\n",
        "  Converts the list of landmarks to a qualitative_evaluations for the \n",
        "  structured report. \n",
        "\n",
        "  Arguments:\n",
        "    landmark_list  : list of landmarks that correspond to a particular slice\n",
        "    landmarks_df   : the dataframe holding the bpr landmarks metadata needed to\n",
        "                     create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    list of QualitativeEvaluation\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  qualitative_evaluations = [] \n",
        "  num_landmarks_in_slice = len(landmark_list)\n",
        "  for landmark in landmark_list: \n",
        "    if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "      print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "      break\n",
        "    else:\n",
        "      landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "      # landmark_code = CodedConcept(value = str(int(landmark_row[\"CodeValue\"].values[0])),\n",
        "      #                              meaning = landmark_row[\"CodeMeaning\"].values[0],\n",
        "      #                              scheme_designator = landmark_row[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "      #print(landmarks_df[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_modifier_code = None\n",
        "      if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "        # landmark_modifier_code = CodedConcept(value = str(int(landmark_row[\"modifier_CodeValue\"].values[0])),\n",
        "        #                              meaning = landmark_row[\"modifier_CodeMeaning\"].values[0],\n",
        "        #                              scheme_designator = landmark_row[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "        #print(landmarks_df[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        qual = QualitativeEvaluation(CodedConcept(\n",
        "                value=\"123014\",\n",
        "                meaning=\"Target Region\",\n",
        "                scheme_designator=\"DCM\"  \n",
        "                ), \n",
        "                landmark_code\n",
        "              )\n",
        "    \n",
        "    if landmark_modifier_code is not None:\n",
        "      qual_modifier = CodeContentItem(\n",
        "          name=CodedConcept(\n",
        "              value='106233006',\n",
        "              meaning='Topographical modifier',\n",
        "              scheme_designator='SCT',\n",
        "          ),\n",
        "          value=landmark_modifier_code,\n",
        "          relationship_type=RelationshipTypeValues.HAS_CONCEPT_MOD\n",
        "      )\n",
        "      qual.append(qual_modifier)\n",
        "\n",
        "    qualitative_evaluations.append(qual)\n",
        "\n",
        "  return qualitative_evaluations\n"
      ],
      "metadata": {
        "id": "REfEiAnUxeop"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "                                                                json_file, \n",
        "                                                                output_SR_file, \n",
        "                                                                bpr_revision_number,\n",
        "                                                                landmarks_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "                          create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "    #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    sop = ds['0x0008', '0x0018'].value\n",
        "    sop_all.append(sop)\n",
        "\n",
        "    #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #----- Get the landmarks as indices -----# \n",
        "\n",
        "  landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  # observer_person_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Person,\n",
        "  #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "  #         name='Anonymous^Reader'\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  observer_device_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Device,\n",
        "      observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "          uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "          model_name = bpr_revision_number\n",
        "      )\n",
        "  )\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                   \"ObserverType\": \"DEVICE\",\n",
        "  #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                 }\n",
        "\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  tracking_uid = generate_uid()\n",
        "\n",
        "  #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "  num_slices = len(files_sorted)\n",
        "  annotation_count = 1 \n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "    landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "    # Only include if there is a landmark for a slice \n",
        "    if (landmark_indices_list):\n",
        "    \n",
        "      # Create QualitativeEvaluations\n",
        "      qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "                                                                                  landmarks_df)\n",
        "\n",
        "      # In the correct order \n",
        "      reference_dcm_file = files_sorted[n]\n",
        "      image_dataset = dcmread(reference_dcm_file)\n",
        "      evidence.append(image_dataset)\n",
        "\n",
        "      src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "      # tracking_id = \"Annotations group x\"\n",
        "      # tracking_id = \"Annotations group landmarks\"\n",
        "      # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "      tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "      measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                    tracking_identifier=TrackingIdentifier(\n",
        "                        uid=tracking_uid,\n",
        "                        identifier=tracking_id\n",
        "                    ),\n",
        "                    qualitative_evaluations=qualitative_evaluations,\n",
        "                    source_images=[src_image]\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      imaging_measurements.append(\n",
        "        measurements_group\n",
        "              )\n",
        "      \n",
        "      annotation_count += 1 # keep track of number of annotations\n",
        "\n",
        "       #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=101, # was 100 for regions\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR landmark annotations'\n",
        "  )\n",
        "  # series_description='BPR landmark annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset"
      ],
      "metadata": {
        "id": "R6qri9J-xo3o"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing series\n",
        "\n",
        "path_downloaded = '/content/downloaded_data'\n",
        "path_nifti = '/content/nifti_data'\n",
        "path_json =  '/content/json_data'\n",
        "path_nrrd = '/content/nrrd_data'\n",
        "path_sr = '/content/sr_data'\n",
        "\n",
        "df = ct_limited_open_a1_r1\n",
        "\n",
        "good_seriesuids = ['1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952']\n",
        "\n",
        "for seriesuid in good_seriesuids:\n",
        "\n",
        "  # clean up from previous iterations and recreate temp directories\n",
        "  if 1:\n",
        "    for x in [path_downloaded, path_nifti, path_json, path_nrrd]:\n",
        "      if os.path.isdir(x):\n",
        "        try:\n",
        "          shutil.rmtree(x)\n",
        "        except OSError as err:\n",
        "          print(\"Error: %s : %s\" % (x, err.strerror))  \n",
        "      os.mkdir(x)\n",
        "\n",
        "      studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "  if len(studyuids)>0:\n",
        "    print(seriesuid)\n",
        "  studyuid = studyuids[0]\n",
        "  sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "  print(len(sops))\n",
        "\n",
        "  download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "\n",
        "  file_gs_uri_json = \"gs://%s/%s%s.json\" % (bucket_name, bucket_path, seriesuid)\n",
        "  \n",
        "  cmd1 = \"gsutil cp %s %s\" % (file_gs_uri_json, path_json)\n",
        "  print(cmd1)\n",
        "  os.system(cmd1)\n",
        "\n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  #print(dcm_input_list)\n",
        "  \n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  fn_json = os.path.join(path_json, '%s.json' % seriesuid)\n",
        "  fn_sr_regions = os.path.join(path_sr, 'sr_regions_%s.dcm' % seriesuid)\n",
        "  df_regions = pd.read_csv('/content/bpr_regions_code_mapping.csv')\n",
        "  df_landmarks = pd.read_csv('/content/bpr_landmarks_code_mapping.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4owL01weBFvx",
        "outputId": "b9eee2e9-b12b-495f-c413-47cfd412be78"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952\n",
            "145\n",
            "gsutil cp gs://midrc-analysis-bwh/bpr-results/1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.json /content/json_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_structured_report_for_body_part_regression_regions(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_regions, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              df_regions)"
      ],
      "metadata": {
        "id": "AX2JKuBGBtvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_structured_report_for_body_part_regression_landmarks(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_regions, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              df_landmarks)"
      ],
      "metadata": {
        "id": "B_1FWIIKyd88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26wzhkPecuKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}