{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakri201/MIDRC_colab/blob/main/MIDRC_create_tables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook creates the necessary tables for further analysis: \n",
        "1. Export SEG and SR data from the DICOM store (dataset deepa, dicom store deepa_seg_and_sr) to BQ table \n",
        "2. measurement_groups_features_dk \n",
        "3. quantitative_measurements_features_dk \n",
        "4. measurement_groups_regions_dk\n",
        "5. qualitative_measurements_regions_dk\n",
        "6. measurement_groups_landmarks_dk\n",
        "7. qualitative_measurements_landmarks_dk\n",
        "\n",
        "I then add the ohif urls to the dataframes and save as additional tables that can be used for Data Studio. \n",
        "\n",
        "For now I am using the same queries that I used for NLST/NSCLC. "
      ],
      "metadata": {
        "id": "NDdiq85kr9oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameterization"
      ],
      "metadata": {
        "id": "RT3dbAGiO1A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project  bwh-midrc-rapid-res-1655321320"
      ],
      "metadata": {
        "id": "QZr5-mEOOxyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8978074-ad40-4b9b-ce9b-32626695ba00"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3Kk0d3lMmDp3"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bbM0Cf_94rqz"
      },
      "outputs": [],
      "source": [
        "project_name = \"bwh-midrc-rapid-res-1655321320\"\n",
        "# bucket_name = \"midrc-analysis-bwh\"\n",
        "bucket_name = \"midrc-analysis-bwh-dk\"\n",
        "# bucket_path = \"bpr-results\"\n",
        "bucket_path = \"bpr-results/\"\n",
        "\n",
        "dataset_table_id = \"midrc_dicom_us\" # This already exists \n",
        "table_view_id_name = \"ct_limited_open_a1_r1_dk\"\n",
        "# table_id = \"bwh-midrc-rapid-res-1655321320.midrc_dicom_us.ct_limited_open_a1_r1_dk\"\n",
        "\n",
        "location_id = \"us-central1\"\n",
        "\n",
        "my_dataset_id = \"deepa\"\n",
        "my_dicom_store_id = \"deepa\"\n",
        "my_dicom_store_id_seg_and_sr = \"deepa_seg_and_sr\"\n",
        "\n",
        "# Name of the table with the original exported DICOM metadata (the dataset is dataset_table_id = \"midrc_dicom_us\") \n",
        "dicom_metadata_table = \"midrc_with_seg_sr_dk\"\n",
        "# Shape features SR - measurement \n",
        "measurement_group_features_table = 'measurement_group_features_dk'\n",
        "quantitative_measurements_features_table = 'quantitative_measurements_features_dk'\n",
        "# BPR regions \n",
        "measurement_group_regions_table = 'measurement_group_regions_dk'\n",
        "qualitative_measurements_regions_table = 'qualitative_measurements_regions_dk'\n",
        "# BPR landmarks\n",
        "measurement_group_landmarks_table = 'measurement_group_landmarks_dk'\n",
        "qualitative_measurements_landmarks_table = 'qualitative_measurements_landmarks_dk'\n",
        "\n",
        "# Tables with ohif urls \n",
        "quantitative_measurements_features_table_with_ohif = 'quantitative_measurements_features_dk_with_ohif'\n",
        "qualitative_measurements_regions_table_with_ohif = 'qualitative_measurements_regions_dk_with_ohif'\n",
        "qualitative_measurements_landmarks_table_with_ohif = 'qualitative_measurements_landmarks_dk_with_ohif'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries for the measurement and qualitative/quantitative tables"
      ],
      "metadata": {
        "id": "pzg8mYmH3jli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir(\"/content/queries\"):\n",
        "  os.mkdir(\"/content/queries\")"
      ],
      "metadata": {
        "id": "c9RDQqh95zoo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measurement_group_features query\n",
        "query_download_path = \"/content/queries/measurement_group_features_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_measurement_group_features_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_features_query.txt\n",
        "\n",
        "# quantitative_measurements_features query \n",
        "query_download_path = \"/content/queries/quantitative_measurements_features_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_quantitative_measurements_features_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/quantitative_measurements_features_query.txt\n",
        "\n",
        "# measurement_group_regions query \n",
        "query_download_path = \"/content/queries/measurement_group_regions_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_measurement_group_regions_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_regions_query.txt\n",
        "\n",
        "# qualitative_measurements_regions query \n",
        "query_download_path = \"/content/queries/qualitative_measurements_regions_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_qualitative_measurements_regions_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/qualitative_measurements_regions_query.txt\n",
        "\n",
        "# measurement_group_landmarks query \n",
        "query_download_path = \"/content/queries/measurement_group_landmarks_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_measurement_group_landmarks_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_landmarks_query.txt\n",
        "\n",
        "# qualitative_measurements_landmarks query \n",
        "query_download_path = \"/content/queries/qualitative_measurements_landmarks_query.txt\"\n",
        "# !wget -O $query_download_path https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/common/queries/NSCLC_Radiomics_qualitative_measurements_landmarks_query.txt\n",
        "!wget -O $query_download_path https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/qualitative_measurements_landmarks_query.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOiyEYMU3v47",
        "outputId": "1c186a85-4c9c-4d70-c95e-cad40c2ab9c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 21:32:22--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_features_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6867 (6.7K) [text/plain]\n",
            "Saving to: ‘/content/queries/measurement_group_features_query.txt’\n",
            "\n",
            "/content/queries/me 100%[===================>]   6.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:23 (46.3 MB/s) - ‘/content/queries/measurement_group_features_query.txt’ saved [6867/6867]\n",
            "\n",
            "--2023-01-22 21:32:23--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/quantitative_measurements_features_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439 (7.3K) [text/plain]\n",
            "Saving to: ‘/content/queries/quantitative_measurements_features_query.txt’\n",
            "\n",
            "/content/queries/qu 100%[===================>]   7.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:23 (73.2 MB/s) - ‘/content/queries/quantitative_measurements_features_query.txt’ saved [7439/7439]\n",
            "\n",
            "--2023-01-22 21:32:23--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_regions_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7240 (7.1K) [text/plain]\n",
            "Saving to: ‘/content/queries/measurement_group_regions_query.txt’\n",
            "\n",
            "/content/queries/me 100%[===================>]   7.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:23 (48.5 MB/s) - ‘/content/queries/measurement_group_regions_query.txt’ saved [7240/7240]\n",
            "\n",
            "--2023-01-22 21:32:23--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/qualitative_measurements_regions_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2784 (2.7K) [text/plain]\n",
            "Saving to: ‘/content/queries/qualitative_measurements_regions_query.txt’\n",
            "\n",
            "/content/queries/qu 100%[===================>]   2.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:24 (30.7 MB/s) - ‘/content/queries/qualitative_measurements_regions_query.txt’ saved [2784/2784]\n",
            "\n",
            "--2023-01-22 21:32:24--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/measurement_group_landmarks_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7240 (7.1K) [text/plain]\n",
            "Saving to: ‘/content/queries/measurement_group_landmarks_query.txt’\n",
            "\n",
            "/content/queries/me 100%[===================>]   7.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:24 (71.1 MB/s) - ‘/content/queries/measurement_group_landmarks_query.txt’ saved [7240/7240]\n",
            "\n",
            "--2023-01-22 21:32:24--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/queries/qualitative_measurements_landmarks_query.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3671 (3.6K) [text/plain]\n",
            "Saving to: ‘/content/queries/qualitative_measurements_landmarks_query.txt’\n",
            "\n",
            "/content/queries/qu 100%[===================>]   3.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-22 21:32:24 (32.8 MB/s) - ‘/content/queries/qualitative_measurements_landmarks_query.txt’ saved [3671/3671]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "RPzSLKyPOzKN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjQ88PgUTFkX"
      },
      "source": [
        "**Install BodyPartRegression package**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64PiMsLhibpM",
        "outputId": "52d1b373-4405-4110-94d4-2eeaa7b2876b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.2.10\n",
            "  Downloading pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.9/841.9 KB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.9.1\n",
            "  Downloading torchtext-0.9.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses==0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (1.21.6)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.2.10) (2.9.1)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.2.10) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.2.10) (2022.11.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.2.10) (4.64.1)\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.2.10) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.1) (2.25.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (3.8.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (2.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.51.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (0.38.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1) (4.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch-lightning==1.2.10) (3.0.9)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.10) (4.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.2.10) (3.2.2)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=610ad90330c66549ba98491d7447d8dab8b1cda5fa70da2caa265bbfbcab4196\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n",
            "Successfully built future\n",
            "Installing collected packages: dataclasses, torch, future, torchvision, torchtext, torchmetrics, torchaudio, pytorch-lightning\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "Successfully installed dataclasses-0.6 future-0.18.3 pytorch-lightning-1.2.10 torch-1.8.1 torchaudio-0.8.1 torchmetrics-0.2.0 torchtext-0.9.1 torchvision-0.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bpreg\n",
            "  Downloading bpreg-1.1.0.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m177.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytorch_lightning==1.2.10 in /usr/local/lib/python3.8/dist-packages (from bpreg) (1.2.10)\n",
            "Collecting nibabel==3.2.1\n",
            "  Downloading nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.0\n",
            "  Downloading scipy-1.7.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting albumentations==0.5.2\n",
            "  Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.8/dist-packages (from bpreg) (0.6)\n",
            "Collecting pandas==1.2.1\n",
            "  Downloading pandas-1.2.1-cp38-cp38-manylinux1_x86_64.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.8/dist-packages (from bpreg) (1.8.1)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.8/dist-packages (from bpreg) (0.9.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.5.2->bpreg) (0.18.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.5.2->bpreg) (4.7.0.68)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.5.2->bpreg) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.5.2->bpreg) (1.21.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==0.5.2->bpreg) (6.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from nibabel==3.2.1->bpreg) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.1->bpreg) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.1->bpreg) (2.8.2)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->bpreg) (2022.11.0)\n",
            "Requirement already satisfied: torchmetrics==0.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->bpreg) (0.2.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->bpreg) (4.64.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->bpreg) (0.18.3)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->bpreg) (2.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1->bpreg) (4.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1->bpreg) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (3.8.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->bpreg) (2.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->bpreg) (2.9.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->bpreg) (4.6.0.66)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->bpreg) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->bpreg) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.3->nibabel==3.2.1->bpreg) (3.0.9)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2->bpreg) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2->bpreg) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2->bpreg) (2022.10.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (2.16.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (1.51.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (0.4.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (2.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (6.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2->bpreg) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2->bpreg) (1.4.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->bpreg) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->bpreg) (3.2.2)\n",
            "Building wheels for collected packages: bpreg\n",
            "  Building wheel for bpreg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bpreg: filename=bpreg-1.1.0-py3-none-any.whl size=71923 sha256=b7fc5a8c3b560d429482d748ceed35a15c2801a8348309b179909ec550fa6767\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/95/52/56eb832830d17afcefdac9ddc005abd7e1729618e2424b4a13\n",
            "Successfully built bpreg\n",
            "Installing collected packages: scipy, pandas, nibabel, albumentations, bpreg\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires pandas>=1.3, but you have pandas 1.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-0.5.2 bpreg-1.1.0 nibabel-3.2.1 pandas-1.2.1 scipy-1.7.0\n",
            "Cloning into 'BodyPartRegression'...\n",
            "remote: Enumerating objects: 2378, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 2378 (delta 191), reused 364 (delta 191), pack-reused 2014\u001b[K\n",
            "Receiving objects: 100% (2378/2378), 12.23 MiB | 18.57 MiB/s, done.\n",
            "Resolving deltas: 100% (1530/1530), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-headless==4.1.2.30\n",
            "  Downloading opencv_python_headless-4.1.2.30-cp38-cp38-manylinux1_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.7.0.68\n",
            "    Uninstalling opencv-python-headless-4.7.0.68:\n",
            "      Successfully uninstalled opencv-python-headless-4.7.0.68\n",
            "Successfully installed opencv-python-headless-4.1.2.30\n",
            "1.2.1\n"
          ]
        }
      ],
      "source": [
        "#%%capture \n",
        "import shutil\n",
        "import os\n",
        "if os.path.isdir('/content/BodyPartRegression'):\n",
        "  shutil.rmtree('/content/BodyPartRegression')\n",
        "!pip install torch==1.8.1 pytorch-lightning==1.2.10 torchtext==0.9.1 torchvision==0.9.1 torchaudio==0.8.1 dataclasses==0.6\n",
        "!pip install bpreg\n",
        "!git clone https://github.com/MIC-DKFZ/BodyPartRegression.git\n",
        "# !pip install torch==1.8.1 pytorch-lightning==1.2.10 torchtext==0.9.1\n",
        "!pip install SimpleITK\n",
        "!pip install pydicom\n",
        "\n",
        "import bpreg \n",
        "import seaborn as sb \n",
        "import pandas as pd \n",
        "import SimpleITK as sitk\n",
        "import glob\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "!pip install opencv-python-headless==4.1.2.30 # https://stackoverflow.com/questions/70537488/cannot-import-name-registermattype-from-cv2-cv2/70547274\n",
        "from BodyPartRegression.docs.notebooks.utils import * \n",
        "\n",
        "print (pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "duQSF98RvxIv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MudNQIu_SwqD",
        "outputId": "b102304d-f6f8-4e56-ffbc-bb5c8aac541f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip\n",
            "  Downloading https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m50.4 kB\u001b[0m \u001b[31m396.2 kB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.8/dist-packages (from dcmweb==1.0.2) (2.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from dcmweb==1.0.2) (2.25.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hurry.filesize\n",
            "  Downloading hurry.filesize-0.9.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire->dcmweb==1.0.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->dcmweb==1.0.2) (2.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (0.2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from hurry.filesize->dcmweb==1.0.2) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (4.0.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators->dcmweb==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth->dcmweb==1.0.2) (0.4.8)\n",
            "Building wheels for collected packages: dcmweb, fire, hurry.filesize, validators\n",
            "  Building wheel for dcmweb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dcmweb: filename=dcmweb-1.0.2-py3-none-any.whl size=26439 sha256=7cadcedb816b6965667f5c72b7c59e7528cd05284ff25cda99e01481ee7affe1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-agodw0tv/wheels/72/cb/c0/8e1b6fd05e3af6d751c98b4a316bbe3c5658baedd3f5af98ad\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=0733825bff2f47c6eae79d8d6d99d4bdfa9e6d914d4ddf1591a9ef15a9f88de2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4134 sha256=e6c82e571aa064d678f82a5c548b8a826d37a4f28c2cd4069684ffd4a3e04787\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/4b/2b/e1eaf7375b72542a9a3f3c3fa66b7098cc9e8048fe345deace\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=587fd49c8abe5bbb1fd102bed81e0d37b5432b518433de707b917f1fcc5a94c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "Successfully built dcmweb fire hurry.filesize validators\n",
            "Installing collected packages: validators, hurry.filesize, fire, dcmweb\n",
            "Successfully installed dcmweb-1.0.2 fire-0.5.0 hurry.filesize-0.9 validators-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3dwDGSxdC4Mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8023c7-b35a-4443-e273-656ddb0650bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libyaml-cpp0.6\n",
            "Suggested packages:\n",
            "  pigz\n",
            "The following NEW packages will be installed:\n",
            "  dcm2niix libyaml-cpp0.6\n",
            "0 upgraded, 2 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 300 kB of archives.\n",
            "After this operation, 1,110 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libyaml-cpp0.6 amd64 0.6.2-4ubuntu1 [124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 dcm2niix amd64 1.0.20181125-1build1 [176 kB]\n",
            "Fetched 300 kB in 1s (253 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libyaml-cpp0.6:amd64.\n",
            "(Reading database ... 129504 files and directories currently installed.)\n",
            "Preparing to unpack .../libyaml-cpp0.6_0.6.2-4ubuntu1_amd64.deb ...\n",
            "Unpacking libyaml-cpp0.6:amd64 (0.6.2-4ubuntu1) ...\n",
            "Selecting previously unselected package dcm2niix.\n",
            "Preparing to unpack .../dcm2niix_1.0.20181125-1build1_amd64.deb ...\n",
            "Unpacking dcm2niix (1.0.20181125-1build1) ...\n",
            "Setting up libyaml-cpp0.6:amd64 (0.6.2-4ubuntu1) ...\n",
            "Setting up dcm2niix (1.0.20181125-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "# install dcm2niix\n",
        "!sudo apt-get install dcm2niix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KZYEUFWcW6pG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6bb177-52a7-401f-9bde-1cf7515cb264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting nptyping\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from pynrrd) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pynrrd) (4.4.0)\n",
            "Installing collected packages: nptyping, pynrrd\n",
            "Successfully installed nptyping-2.4.1 pynrrd-1.0.0\n"
          ]
        }
      ],
      "source": [
        "#install nrrd\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CU-D744Jl1dG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b56ebc-02f9-4b15-878f-071fdeb18602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libdcmtk14 libdlib-data libdlib19 libfftw3-single3 libinsighttoolkit4.13\n",
            "  libminc2-5.2.0 libnifti2\n",
            "Suggested packages:\n",
            "  libfftw3-bin libfftw3-dev\n",
            "The following NEW packages will be installed:\n",
            "  libdcmtk14 libdlib-data libdlib19 libfftw3-single3 libinsighttoolkit4.13\n",
            "  libminc2-5.2.0 libnifti2 plastimatch\n",
            "0 upgraded, 8 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 80.1 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdcmtk14 amd64 3.6.4-2.1build2 [4,682 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdlib-data all 19.10-3build1 [63.4 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdlib19 amd64 19.10-3build1 [3,773 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libfftw3-single3 amd64 3.3.8-2ubuntu1 [756 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libminc2-5.2.0 amd64 2.4.03-2build3 [209 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libnifti2 amd64 2.0.0-3ubuntu1 [102 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libinsighttoolkit4.13 amd64 4.13.2-dfsg1-8 [4,426 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 plastimatch amd64 1.8.0+dfsg.1-2build1 [2,699 kB]\n",
            "Fetched 80.1 MB in 5s (14.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdcmtk14.\n",
            "(Reading database ... 129516 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdcmtk14_3.6.4-2.1build2_amd64.deb ...\n",
            "Unpacking libdcmtk14 (3.6.4-2.1build2) ...\n",
            "Selecting previously unselected package libdlib-data.\n",
            "Preparing to unpack .../1-libdlib-data_19.10-3build1_all.deb ...\n",
            "Unpacking libdlib-data (19.10-3build1) ...\n",
            "Selecting previously unselected package libdlib19:amd64.\n",
            "Preparing to unpack .../2-libdlib19_19.10-3build1_amd64.deb ...\n",
            "Unpacking libdlib19:amd64 (19.10-3build1) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../3-libfftw3-single3_3.3.8-2ubuntu1_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.8-2ubuntu1) ...\n",
            "Selecting previously unselected package libminc2-5.2.0:amd64.\n",
            "Preparing to unpack .../4-libminc2-5.2.0_2.4.03-2build3_amd64.deb ...\n",
            "Unpacking libminc2-5.2.0:amd64 (2.4.03-2build3) ...\n",
            "Selecting previously unselected package libnifti2.\n",
            "Preparing to unpack .../5-libnifti2_2.0.0-3ubuntu1_amd64.deb ...\n",
            "Unpacking libnifti2 (2.0.0-3ubuntu1) ...\n",
            "Selecting previously unselected package libinsighttoolkit4.13.\n",
            "Preparing to unpack .../6-libinsighttoolkit4.13_4.13.2-dfsg1-8_amd64.deb ...\n",
            "Unpacking libinsighttoolkit4.13 (4.13.2-dfsg1-8) ...\n",
            "Selecting previously unselected package plastimatch.\n",
            "Preparing to unpack .../7-plastimatch_1.8.0+dfsg.1-2build1_amd64.deb ...\n",
            "Unpacking plastimatch (1.8.0+dfsg.1-2build1) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.8-2ubuntu1) ...\n",
            "Setting up libdcmtk14 (3.6.4-2.1build2) ...\n",
            "Setting up libminc2-5.2.0:amd64 (2.4.03-2build3) ...\n",
            "Setting up libnifti2 (2.0.0-3ubuntu1) ...\n",
            "Setting up libinsighttoolkit4.13 (4.13.2-dfsg1-8) ...\n",
            "Setting up libdlib-data (19.10-3build1) ...\n",
            "Setting up libdlib19:amd64 (19.10-3build1) ...\n",
            "Setting up plastimatch (1.8.0+dfsg.1-2build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "plastimatch version 1.8.0\n",
            "Cloning into 'pyplastimatch'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 97 (delta 32), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.58 MiB | 17.83 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ],
      "source": [
        "#Install Plastimatch\n",
        "\n",
        "!sudo apt install plastimatch \n",
        "!echo $(plastimatch --version)\n",
        "\n",
        "if os.path.isdir('/content/pyplastimatch'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/pyplastimatch')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"pyplastimatch\", err.strerror)) \n",
        "# !git clone https://github.com/denbonte/pyplastimatch/ pyplastimatch\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch.git \n",
        "\n",
        "# from pyplastimatch import pyplastimatch as pypla\n",
        "from pyplastimatch.pyplastimatch import pyplastimatch as pypla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HKGUrG1iAo5f"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from bpreg.scripts.bpreg_inference import bpreg_inference\n",
        "from google.cloud import storage\n",
        "import nrrd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import transforms\n",
        "from matplotlib.colors import ListedColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "r6WoxrbeOwbu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sR8c3D6j1Vvh"
      },
      "outputs": [],
      "source": [
        "# %%bigquery ct_limited_open_a1_r1 --project=bwh-midrc-rapid-res-1655321320 \n",
        "\n",
        "# WITH\n",
        "#   nlst_instances_per_series AS (\n",
        "#     SELECT\n",
        "#       DISTINCT(StudyInstanceUID),\n",
        "#       SeriesInstanceUID,\n",
        "#       COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "#       MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "#       MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#       MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "#       MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "#       STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#     FROM\n",
        "#       bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "#     WHERE\n",
        "#       (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "#     GROUP BY\n",
        "#       StudyInstanceUID,\n",
        "#       SeriesInstanceUID\n",
        "#       ), \n",
        "#   nlst_values_per_series AS (\n",
        "#     SELECT \n",
        "#     ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#     ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "#     ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_instances_per_series\n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "#   WHERE\n",
        "#     min_SliceThickness >= 1.5 \n",
        "#     AND max_SliceThickness <= 3.5 \n",
        "#     AND nlst_instances_per_series.num_instances > 100\n",
        "#     AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "#     AND nlst_instances_per_series.orientation_count = 1\n",
        "#     AND has_localizer = \"false\"\n",
        "#   GROUP BY\n",
        "#     SeriesInstanceUID\n",
        "#   )\n",
        "#   SELECT \n",
        "#     dicom_all.PatientID,\n",
        "#     dicom_all.StudyInstanceUID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     dicom_all.SOPInstanceUID,\n",
        "#     dicom_all.collection_id,\n",
        "#     dicom_all.PatientAge,\n",
        "#     dicom_all.PatientWeight,\n",
        "#     nlst_values_per_series.num_instances,\n",
        "#     nlst_values_per_series.SliceThickness,\n",
        "#     nlst_values_per_series.PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_values_per_series \n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(set(ct_limited_open_a1_r1['PatientID'].values)))\n",
        "# print(len(set(ct_limited_open_a1_r1['StudyInstanceUID'].values)))\n",
        "# print(len(set(ct_limited_open_a1_r1['SeriesInstanceUID'].values)))"
      ],
      "metadata": {
        "id": "7eT3qvYMlHIY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bigquery ct_limited_open_a1_r1 --project=bwh-midrc-rapid-res-1655321320 \n",
        "\n",
        "query = \"\"\"\n",
        "WITH\n",
        "  nlst_instances_per_series AS (\n",
        "    SELECT\n",
        "      # DISTINCT(StudyInstanceUID),\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID,\n",
        "      COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(PixelSpacing,\"/\"))) AS pixel_spacing_count,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "      MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "      MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "      MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "      MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "      STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer,\n",
        "      ANY_VALUE(dicom_all.ImageOrientationPatient) AS ImageOrientationPatient\n",
        "    FROM\n",
        "      bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "    WHERE\n",
        "      (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "    GROUP BY\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID\n",
        "      ), \n",
        "\n",
        "    distinct_slice_location_difference_values AS (\n",
        "  SELECT \n",
        "      # DISTINCT(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC) - LAG(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC),1) OVER(partition by SeriesInstanceUID ORDER BY SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC) DESC)) AS SliceLocation_difference,\n",
        "      DISTINCT(TRUNC(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC),1) - LAG(TRUNC(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC),1),1) OVER(partition by SeriesInstanceUID ORDER BY TRUNC(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS NUMERIC),1) DESC)) AS SliceLocation_difference,\n",
        "      SeriesInstanceUID,\n",
        "      StudyInstanceUID\n",
        "  FROM\n",
        "      # `bigquery-public-data.idc_current.dicom_all`\n",
        "      bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "\n",
        "  ),\n",
        "\n",
        "  nlst_values_per_series AS (\n",
        "  SELECT \n",
        "    # ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "    # dicom_all.SeriesInstanceUID,\n",
        "    distinct_slice_location_difference_values.SeriesInstanceUID AS SeriesInstanceUID, \n",
        "    ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "    ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "    ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned, \n",
        "    COUNT(distinct_slice_location_difference_values.SliceLocation_difference) as num_differences,\n",
        "    MAX(ABS(distinct_slice_location_difference_values.SliceLocation_difference)) as max_difference,\n",
        "    MIN(ABS(distinct_slice_location_difference_values.SliceLocation_difference)) as min_difference,\n",
        "    ANY_VALUE(nlst_instances_per_series.ImageOrientationPatient) AS ImageOrientationPatient\n",
        "  FROM\n",
        "    # bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "    nlst_instances_per_series\n",
        "  JOIN\n",
        "    # nlst_instances_per_series\n",
        "    distinct_slice_location_difference_values\n",
        "  ON\n",
        "    # dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "    nlst_instances_per_series.SeriesInstanceUID = distinct_slice_location_difference_values.SeriesInstanceUID \n",
        "  WHERE\n",
        "    nlst_instances_per_series.min_SliceThickness >= 1.5\n",
        "    AND nlst_instances_per_series.max_SliceThickness <= 3.5\n",
        "    AND nlst_instances_per_series.num_instances > 100\n",
        "    AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "    AND nlst_instances_per_series.pixel_spacing_count = 1\n",
        "    AND nlst_instances_per_series.orientation_count = 1\n",
        "    AND has_localizer = \"false\" \n",
        "    AND ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(0)] AS float64)) > ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(1)] AS float64))\n",
        "    AND ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(0)] AS float64)) > ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(2)] AS float64))\n",
        "    AND ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(4)] AS float64)) > ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(3)] AS float64))\n",
        "    AND ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(4)] AS float64)) > ABS(SAFE_CAST(nlst_instances_per_series.ImageOrientationPatient[SAFE_OFFSET(5)] AS float64))\n",
        "  GROUP BY\n",
        "    distinct_slice_location_difference_values.SeriesInstanceUID\n",
        "\n",
        "  )\n",
        "\n",
        "  SELECT \n",
        "    dicom_all.PatientID,\n",
        "    dicom_all.StudyInstanceUID,\n",
        "    dicom_all.SeriesInstanceUID,\n",
        "    dicom_all.SOPInstanceUID, \n",
        "    dicom_all.collection_id, \n",
        "    dicom_all.PatientAge,\n",
        "    dicom_all.PatientWeight,\n",
        "    nlst_values_per_series.num_instances,\n",
        "    nlst_values_per_series.SliceThickness, \n",
        "    nlst_values_per_series.PatientHeightScanned,\n",
        "    nlst_values_per_series.num_differences,\n",
        "    nlst_values_per_series.max_difference, \n",
        "    nlst_values_per_series.min_difference\n",
        "  FROM \n",
        "    bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "  JOIN \n",
        "    nlst_values_per_series\n",
        "  ON \n",
        "    dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID \n",
        "  WHERE\n",
        "    nlst_values_per_series.num_differences <= 2\n",
        "    AND nlst_values_per_series.max_difference/nlst_values_per_series.min_difference < 2\n",
        "\"\"\"\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, table_view_id_name])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(query, job_config=job_config) \n",
        "  result = query_job.result()  \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt4OGuNKNsuI",
        "outputId": "6a1df63c-4a84-49aa-d887-a8702a0a09ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.ct_limited_open_a1_r1_dk\n",
            "table bwh-midrc-rapid-res-1655321320.midrc_dicom_us.ct_limited_open_a1_r1_dk already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataframe from the table \n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "table_id = '.'.join([project_name, dataset_table_id, table_view_id_name])\n",
        "\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    * \n",
        "  FROM \n",
        "    {table_id}\n",
        "  \"\"\" \n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "ct_limited_open_a1_r1 = result.to_dataframe()\n"
      ],
      "metadata": {
        "id": "KbgoXkRZwI69"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1sSH_iQr3hIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "1eecfa89-3c61-4297-ad25-9aead960ff18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              PatientID                                   StudyInstanceUID  \\\n",
              "0      10000364-1310150  2.16.840.1.114274.1818.56277717926260378856921...   \n",
              "1      10000364-1310150  2.16.840.1.114274.1818.56277717926260378856921...   \n",
              "2      10000364-1310150  2.16.840.1.114274.1818.56277717926260378856921...   \n",
              "3      10000364-1310150  2.16.840.1.114274.1818.56277717926260378856921...   \n",
              "4      10000364-1310150  2.16.840.1.114274.1818.56277717926260378856921...   \n",
              "...                 ...                                                ...   \n",
              "75550  10000364-6509816  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75551  10000364-6509816  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75552  10000364-6509816  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75553  10000364-6509816  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75554  10000364-6509816  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "\n",
              "                                       SeriesInstanceUID  \\\n",
              "0      2.16.840.1.114274.1818.46652472987550009305366...   \n",
              "1      2.16.840.1.114274.1818.46652472987550009305366...   \n",
              "2      2.16.840.1.114274.1818.46652472987550009305366...   \n",
              "3      2.16.840.1.114274.1818.46652472987550009305366...   \n",
              "4      2.16.840.1.114274.1818.46652472987550009305366...   \n",
              "...                                                  ...   \n",
              "75550  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75551  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75552  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75553  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "75554  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...   \n",
              "\n",
              "                                          SOPInstanceUID collection_id  \\\n",
              "0      2.16.840.1.114274.1818.46611042193109500771675...       Open-A1   \n",
              "1      2.16.840.1.114274.1818.56958767487481437501222...       Open-A1   \n",
              "2      2.16.840.1.114274.1818.51535999809052778591562...       Open-A1   \n",
              "3      2.16.840.1.114274.1818.53325924708374518603071...       Open-A1   \n",
              "4      2.16.840.1.114274.1818.53878510159989617247990...       Open-A1   \n",
              "...                                                  ...           ...   \n",
              "75550  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...       Open-A1   \n",
              "75551  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...       Open-A1   \n",
              "75552  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...       Open-A1   \n",
              "75553  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...       Open-A1   \n",
              "75554  1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...       Open-A1   \n",
              "\n",
              "      PatientAge PatientWeight  num_instances  SliceThickness  \\\n",
              "0           062Y        94.348            152             1.5   \n",
              "1           062Y        94.348            152             1.5   \n",
              "2           062Y        94.348            152             1.5   \n",
              "3           062Y        94.348            152             1.5   \n",
              "4           062Y        94.348            152             1.5   \n",
              "...          ...           ...            ...             ...   \n",
              "75550       None          None            229             1.5   \n",
              "75551       None          None            229             1.5   \n",
              "75552       None          None            229             1.5   \n",
              "75553       None          None            229             1.5   \n",
              "75554       None          None            229             1.5   \n",
              "\n",
              "       PatientHeightScanned  num_differences max_difference min_difference  \n",
              "0                     226.5                1    1.500000000    1.500000000  \n",
              "1                     226.5                1    1.500000000    1.500000000  \n",
              "2                     226.5                1    1.500000000    1.500000000  \n",
              "3                     226.5                1    1.500000000    1.500000000  \n",
              "4                     226.5                1    1.500000000    1.500000000  \n",
              "...                     ...              ...            ...            ...  \n",
              "75550                 342.0                1    1.500000000    1.500000000  \n",
              "75551                 342.0                1    1.500000000    1.500000000  \n",
              "75552                 342.0                1    1.500000000    1.500000000  \n",
              "75553                 342.0                1    1.500000000    1.500000000  \n",
              "75554                 342.0                1    1.500000000    1.500000000  \n",
              "\n",
              "[75555 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f643d86c-474b-413b-b169-f4fdf723f248\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PatientID</th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>SeriesInstanceUID</th>\n",
              "      <th>SOPInstanceUID</th>\n",
              "      <th>collection_id</th>\n",
              "      <th>PatientAge</th>\n",
              "      <th>PatientWeight</th>\n",
              "      <th>num_instances</th>\n",
              "      <th>SliceThickness</th>\n",
              "      <th>PatientHeightScanned</th>\n",
              "      <th>num_differences</th>\n",
              "      <th>max_difference</th>\n",
              "      <th>min_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000364-1310150</td>\n",
              "      <td>2.16.840.1.114274.1818.56277717926260378856921...</td>\n",
              "      <td>2.16.840.1.114274.1818.46652472987550009305366...</td>\n",
              "      <td>2.16.840.1.114274.1818.46611042193109500771675...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>062Y</td>\n",
              "      <td>94.348</td>\n",
              "      <td>152</td>\n",
              "      <td>1.5</td>\n",
              "      <td>226.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000364-1310150</td>\n",
              "      <td>2.16.840.1.114274.1818.56277717926260378856921...</td>\n",
              "      <td>2.16.840.1.114274.1818.46652472987550009305366...</td>\n",
              "      <td>2.16.840.1.114274.1818.56958767487481437501222...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>062Y</td>\n",
              "      <td>94.348</td>\n",
              "      <td>152</td>\n",
              "      <td>1.5</td>\n",
              "      <td>226.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000364-1310150</td>\n",
              "      <td>2.16.840.1.114274.1818.56277717926260378856921...</td>\n",
              "      <td>2.16.840.1.114274.1818.46652472987550009305366...</td>\n",
              "      <td>2.16.840.1.114274.1818.51535999809052778591562...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>062Y</td>\n",
              "      <td>94.348</td>\n",
              "      <td>152</td>\n",
              "      <td>1.5</td>\n",
              "      <td>226.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000364-1310150</td>\n",
              "      <td>2.16.840.1.114274.1818.56277717926260378856921...</td>\n",
              "      <td>2.16.840.1.114274.1818.46652472987550009305366...</td>\n",
              "      <td>2.16.840.1.114274.1818.53325924708374518603071...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>062Y</td>\n",
              "      <td>94.348</td>\n",
              "      <td>152</td>\n",
              "      <td>1.5</td>\n",
              "      <td>226.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10000364-1310150</td>\n",
              "      <td>2.16.840.1.114274.1818.56277717926260378856921...</td>\n",
              "      <td>2.16.840.1.114274.1818.46652472987550009305366...</td>\n",
              "      <td>2.16.840.1.114274.1818.53878510159989617247990...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>062Y</td>\n",
              "      <td>94.348</td>\n",
              "      <td>152</td>\n",
              "      <td>1.5</td>\n",
              "      <td>226.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75550</th>\n",
              "      <td>10000364-6509816</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>229</td>\n",
              "      <td>1.5</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75551</th>\n",
              "      <td>10000364-6509816</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>229</td>\n",
              "      <td>1.5</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75552</th>\n",
              "      <td>10000364-6509816</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>229</td>\n",
              "      <td>1.5</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75553</th>\n",
              "      <td>10000364-6509816</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>229</td>\n",
              "      <td>1.5</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75554</th>\n",
              "      <td>10000364-6509816</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>1.3.6.1.4.1.5962.99.1.3492129840.282517718.161...</td>\n",
              "      <td>Open-A1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>229</td>\n",
              "      <td>1.5</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000000</td>\n",
              "      <td>1.500000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75555 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f643d86c-474b-413b-b169-f4fdf723f248')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f643d86c-474b-413b-b169-f4fdf723f248 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f643d86c-474b-413b-b169-f4fdf723f248');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "ct_limited_open_a1_r1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(set(ct_limited_open_a1_r1['PatientID'].values)))\n",
        "print(len(set(ct_limited_open_a1_r1['StudyInstanceUID'].values)))\n",
        "print(len(set(ct_limited_open_a1_r1['SeriesInstanceUID'].values)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTvcwVVbwRMD",
        "outputId": "e7b13d49-ae23-47fd-a766-f30e8140bbc4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188\n",
            "199\n",
            "389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YzA5uAf4vcU",
        "outputId": "ae653123-60a0-481f-9424-d2c9cf0d3e80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
              "       'collection_id', 'PatientAge', 'PatientWeight', 'num_instances',\n",
              "       'SliceThickness', 'PatientHeightScanned', 'num_differences',\n",
              "       'max_difference', 'min_difference'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "ct_limited_open_a1_r1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qptg3h_PxubB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "04d1067a-8799-410e-965a-13b2de20f98d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       num_instances  SliceThickness  PatientHeightScanned  num_differences\n",
              "count   75555.000000    75555.000000          75555.000000     75555.000000\n",
              "mean      255.521183        1.829720            359.442232         1.031752\n",
              "std       191.898246        0.449865            129.337340         0.175339\n",
              "min       101.000000        1.500000            168.000000         1.000000\n",
              "25%       152.000000        1.500000            282.000000         1.000000\n",
              "50%       194.000000        1.500000            310.500000         1.000000\n",
              "75%       229.000000        2.000000            418.600000         1.000000\n",
              "max       997.000000        3.000000            821.250000         2.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-690c9898-066e-4782-a378-fbc246547a13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_instances</th>\n",
              "      <th>SliceThickness</th>\n",
              "      <th>PatientHeightScanned</th>\n",
              "      <th>num_differences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>75555.000000</td>\n",
              "      <td>75555.000000</td>\n",
              "      <td>75555.000000</td>\n",
              "      <td>75555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>255.521183</td>\n",
              "      <td>1.829720</td>\n",
              "      <td>359.442232</td>\n",
              "      <td>1.031752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>191.898246</td>\n",
              "      <td>0.449865</td>\n",
              "      <td>129.337340</td>\n",
              "      <td>0.175339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>101.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>152.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>282.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>194.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>310.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>229.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>418.600000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>997.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>821.250000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-690c9898-066e-4782-a378-fbc246547a13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-690c9898-066e-4782-a378-fbc246547a13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-690c9898-066e-4782-a378-fbc246547a13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "ct_limited_open_a1_r1.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export SEG and SR data from DICOM datastore to a BQ table. Do not overwrite. \n"
      ],
      "metadata": {
        "id": "dGFlUTrewz8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dicom_metadata_table_full = '.'.join([project_name, dataset_table_id, dicom_metadata_table])\n",
        "print(dicom_metadata_table_full)\n",
        "\n",
        "# Try to export data and create table \n",
        "\n",
        "print ('trying to export data and create table: ' + str(dicom_metadata_table_full))\n",
        "print ('dataset: ' + str(my_dataset_id))\n",
        "print ('dicom store: ' + str(my_dicom_store_id_seg_and_sr))\n",
        "\n",
        "!gcloud healthcare dicom-stores export bq $my_dicom_store_id_seg_and_sr \\\n",
        "  --dataset=$my_dataset_id \\\n",
        "  --location=$location_id \\\n",
        "  --bq-table=bq://$dicom_metadata_table_full --write-disposition=\"write-empty\"\n",
        "# will output an error if the table already exists. \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWMkx7K6nj85",
        "outputId": "7e125d92-36ab-42ed-fcf0-8cf3f2a8b6b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bwh-midrc-rapid-res-1655321320.midrc_dicom_us.midrc_with_seg_sr_dk\n",
            "trying to export data and create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.midrc_with_seg_sr_dk\n",
            "dataset: deepa\n",
            "dicom store: deepa_seg_and_sr\n",
            "Request issued for: [deepa_seg_and_sr]\n",
            "name: projects/bwh-midrc-rapid-res-1655321320/locations/us-central1/datasets/deepa/dicomStores/deepa_seg_and_sr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the tables for the features"
      ],
      "metadata": {
        "id": "k6_ZWp_r0e6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- features --- # \n",
        "\n",
        "### measurements ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/measurement_group_features_query.txt\") as f: \n",
        "  measurement_group_features_query = f.read()\n",
        "# print(measurement_group_features_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, measurement_group_features_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(measurement_group_features_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brpb19YV3YCb",
        "outputId": "60f108ed-3a5e-470a-d0b8-8ba561a3a31f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_features_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_features_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### quantitative ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/quantitative_measurements_features_query.txt\") as f: \n",
        "  quantitative_measurements_features_query = f.read()\n",
        "# print(quantitative_measurements_group_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, quantitative_measurements_features_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(quantitative_measurements_features_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2qZS4oT5Zdr",
        "outputId": "bafb7e0d-ba02-4fe1-afc5-4d25af3764d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.quantitative_measurements_features_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.quantitative_measurements_features_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the tables for the regions"
      ],
      "metadata": {
        "id": "gD6znQJD7x_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- regions --- # \n",
        "\n",
        "### measurements ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/measurement_group_regions_query.txt\") as f: \n",
        "  measurement_group_regions_query = f.read()\n",
        "# print(measurement_group_regions_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, measurement_group_regions_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(measurement_group_regions_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7NEzatz7LfE",
        "outputId": "8cc5a6f7-61e8-4631-9e4f-689e88024b6d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_regions_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_regions_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### qualitative ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/qualitative_measurements_regions_query.txt\") as f: \n",
        "  qualitative_measurements_regions_query = f.read()\n",
        "# print(qualitative_measurements_regions_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_regions_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(qualitative_measurements_regions_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR6akqKy5cL4",
        "outputId": "57911aee-d89c-4910-ae13-f6bf07fdcb4f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.qualitative_measurements_regions_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.qualitative_measurements_regions_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the tables for the landmarks"
      ],
      "metadata": {
        "id": "EJ5Lf59c70oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- landmarks --- # \n",
        "\n",
        "### measurements ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/measurement_group_landmarks_query.txt\") as f: \n",
        "  measurement_group_landmarks_query = f.read()\n",
        "# print(measurement_group_landmarks_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, measurement_group_landmarks_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(measurement_group_landmarks_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdoJYlT67r8q",
        "outputId": "1ce6d44a-907a-4c93-dacf-6a5e69974c8c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_landmarks_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.measurement_group_landmarks_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### qualitative ### \n",
        "\n",
        "# Get the query from the text file \n",
        "with open(\"/content/queries/qualitative_measurements_landmarks_query.txt\") as f: \n",
        "  qualitative_measurements_landmarks_query = f.read()\n",
        "# print(qualitative_measurements_landmarks_query)\n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_landmarks_table])\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id)\n",
        "\n",
        "# Try to create table \n",
        "try: \n",
        "  print ('trying to create table: ' + str(table_id))\n",
        "  query_job = client.query(qualitative_measurements_landmarks_query, job_config=job_config) \n",
        "  result = query_job.result() \n",
        "  print ('created table: ' + str(table_id))\n",
        "# If table already exists, don't do anything\n",
        "except: \n",
        "  print('table ' + str(table_id) + ' already exists')\n",
        "  pass "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgERZl_m5dyx",
        "outputId": "03569c66-cbc2-4053-ad27-e1bd31371d7d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to create table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.qualitative_measurements_landmarks_dk\n",
            "created table: bwh-midrc-rapid-res-1655321320.midrc_dicom_us.qualitative_measurements_landmarks_dk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the quantitative and qualitative tables with the ohif urls as last column"
      ],
      "metadata": {
        "id": "tdEm7qB4aWXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohif_url_base = os.path.join(\"https://idc-tester-1.web.app\",\n",
        "                        \"projects\", project_name, \n",
        "                        \"locations\", location_id, \n",
        "                        \"datasets\", my_dataset_id,\n",
        "                        \"dicomStores\", my_dicom_store_id, \n",
        "                        \"study\" + os.sep) \n",
        "print(ohif_url_base)"
      ],
      "metadata": {
        "id": "LvpQ9Ib0a6Uv",
        "outputId": "90c75bb8-a8d5-4e43-bc15-a98d06fa1b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://idc-tester-1.web.app/projects/bwh-midrc-rapid-res-1655321320/locations/us-central1/datasets/deepa/dicomStores/deepa/study/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the main table as a df \n",
        "\n",
        "client = bigquery.Client(project=project_name)\n",
        "table_id = '.'.join([project_name, dataset_table_id, table_view_id_name])\n",
        "\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    * \n",
        "  FROM \n",
        "    {table_id}\n",
        "  \"\"\" \n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "ct_limited_open_a1_r1 = result.to_dataframe()"
      ],
      "metadata": {
        "id": "ZdFiL64RfaPQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitative"
      ],
      "metadata": {
        "id": "Oqb6Ph7oa0zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First get the table as a df \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, quantitative_measurements_features_table])\n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    *\n",
        "  FROM\n",
        "    {table_id}\n",
        "  \"\"\"\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "quantitative_measurements_features_df = result.to_dataframe()"
      ],
      "metadata": {
        "id": "z1C9uE9Ue20Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the ohif url \n",
        "\n",
        "ohif_viewer_url = []\n",
        "for n in range(0,len(quantitative_measurements_features_df)):\n",
        "  series_id = quantitative_measurements_features_df['sourceSegmentedSeriesUID'].values[n]\n",
        "  study_id = ct_limited_open_a1_r1[ct_limited_open_a1_r1['SeriesInstanceUID']==series_id]['StudyInstanceUID'].values[0]\n",
        "  url = os.path.join(ohif_url_base, study_id)\n",
        "  ohif_viewer_url.append(url)\n",
        "\n",
        "quantitative_measurements_features_with_ohif_df = quantitative_measurements_features_df.copy(deep=True)\n",
        "quantitative_measurements_features_with_ohif_df[\"viewer_url\"] = ohif_viewer_url\n"
      ],
      "metadata": {
        "id": "AAKICv4ZfHfa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the table \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, quantitative_measurements_features_table_with_ohif]) \n",
        "client = bigquery.Client(project=project_name)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = client.load_table_from_dataframe(quantitative_measurements_features_with_ohif_df, \n",
        "                                       table_id, \n",
        "                                       job_config=job_config)"
      ],
      "metadata": {
        "id": "Cw0sg-6Rg-hz",
        "outputId": "8b9f74db-b78d-4e4e-a144-004a6c3059ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:571: UserWarning: Pyarrow could not determine the type of columns: Quantity, Units, finding, findingSite.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative regions"
      ],
      "metadata": {
        "id": "ETxDXnNmdKxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First get the table as a df \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_regions_table])\n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    *\n",
        "  FROM\n",
        "    {table_id}\n",
        "  \"\"\"\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "qualitative_measurements_regions_df = result.to_dataframe()"
      ],
      "metadata": {
        "id": "VdecutaddL1Q"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Already have the ReferencedStudyInstanceUID, so just add ohif url easily \n",
        "\n",
        "ohif_viewer_url = [] \n",
        "for n in range(0,len(qualitative_measurements_regions_df)):\n",
        "  study_id = qualitative_measurements_regions_df['ReferencedStudyInstanceUID'].values[n] \n",
        "  url = os.path.join(ohif_url_base, study_id)\n",
        "  ohif_viewer_url.append(url)\n",
        "\n",
        "qualitative_measurements_regions_with_ohif_df = qualitative_measurements_regions_df.copy(deep=True)\n",
        "qualitative_measurements_regions_with_ohif_df[\"viewer_url\"] = ohif_viewer_url\n"
      ],
      "metadata": {
        "id": "8AhiNmJDeazG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the table \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_regions_table_with_ohif]) \n",
        "client = bigquery.Client(project=project_name)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = client.load_table_from_dataframe(qualitative_measurements_regions_with_ohif_df, \n",
        "                                       table_id, \n",
        "                                       job_config=job_config)"
      ],
      "metadata": {
        "id": "yfQNkeivhnE7",
        "outputId": "c1b0e96a-8c33-49bd-bd6e-95b12c54742d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:571: UserWarning: Pyarrow could not determine the type of columns: ConceptNameCodeSequence, ConceptCodeSequence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative landmarks "
      ],
      "metadata": {
        "id": "CvpDK5iTec1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First get the table as a df \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_landmarks_table])\n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    *\n",
        "  FROM\n",
        "    {table_id}\n",
        "  \"\"\"\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "qualitative_measurements_landmarks_df = result.to_dataframe()"
      ],
      "metadata": {
        "id": "Mtrz4TeOeeFC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Already have the ReferencedStudyInstanceUID, so just add ohif url easily \n",
        "\n",
        "ohif_viewer_url = [] \n",
        "for n in range(0,len(qualitative_measurements_landmarks_df)):\n",
        "  study_id = qualitative_measurements_landmarks_df['ReferencedStudyInstanceUID'].values[n] \n",
        "  url = os.path.join(ohif_url_base, study_id)\n",
        "  ohif_viewer_url.append(url)\n",
        "\n",
        "qualitative_measurements_landmarks_with_ohif_df = qualitative_measurements_landmarks_df.copy(deep=True)\n",
        "qualitative_measurements_landmarks_with_ohif_df[\"viewer_url\"] = ohif_viewer_url"
      ],
      "metadata": {
        "id": "o4qO2BUeegov"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the table \n",
        "\n",
        "table_id = '.'.join([project_name, dataset_table_id, qualitative_measurements_landmarks_table_with_ohif]) \n",
        "client = bigquery.Client(project=project_name)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = client.load_table_from_dataframe(qualitative_measurements_landmarks_with_ohif_df, \n",
        "                                       table_id, \n",
        "                                       job_config=job_config)"
      ],
      "metadata": {
        "id": "1u_z8mhUhzaX",
        "outputId": "69cf0054-12a3-444b-8822-596ee438d9ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:571: UserWarning: Pyarrow could not determine the type of columns: ConceptNameCodeSequence, ConceptCodeSequence, topographical_modifier.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGUh7R9mkfry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}