{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakri201/MIDRC_colab/blob/main/MIDRC_SR_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameterization"
      ],
      "metadata": {
        "id": "iYKQnpDPr9j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project  bwh-midrc-rapid-res-1655321320"
      ],
      "metadata": {
        "id": "QZr5-mEOOxyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585d2071-fde9-42c3-9e41-fe3474198579"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Kk0d3lMmDp3"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bbM0Cf_94rqz"
      },
      "outputs": [],
      "source": [
        "project_name = \"bwh-midrc-rapid-res-1655321320\"\n",
        "# bucket_name = \"midrc-analysis-bwh\"\n",
        "bucket_name = 'midrc-analysis-bwh-dk'\n",
        "bucket_path = \"bpr-results/\"\n",
        "bucket_path_sr = \"structured_reports/\" \n",
        "\n",
        "dataset_table_id = \"midrc_dicom_us\" # This already exists \n",
        "table_view_id_name = \"ct_limited_open_a1_r1_dk\"\n",
        "# table_id = \"bwh-midrc-rapid-res-1655321320.midrc_dicom_us.ct_limited_open_a1_r1_dk\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "rbXDOIMLsAKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZYEUFWcW6pG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ad98d4-d1e0-4e77-a2d0-f6b4941e0cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from pynrrd) (1.21.6)\n",
            "Collecting nptyping\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pynrrd) (4.4.0)\n",
            "Installing collected packages: nptyping, pynrrd\n",
            "Successfully installed nptyping-2.4.1 pynrrd-1.0.0\n"
          ]
        }
      ],
      "source": [
        "#install nrrd\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j79_pPeVRjkB",
        "outputId": "6d41355e-d97a-4a7a-8cf9-c6826d87d454"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBBXdFGY-tqE",
        "outputId": "5c5b862b-8ccc-4f28-80a2-1fae4691d512"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Plastimatch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "!sudo apt install plastimatch \n",
        "!echo $(plastimatch --version)\n",
        "\n",
        "if os.path.isdir('/content/pyplastimatch'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/pyplastimatch')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"pyplastimatch\", err.strerror)) \n",
        "# !git clone https://github.com/denbonte/pyplastimatch/ pyplastimatch\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch.git \n",
        "\n",
        "# from pyplastimatch import pyplastimatch as pypla\n",
        "from pyplastimatch.pyplastimatch import pyplastimatch as pypla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIxqEpe5MOXS",
        "outputId": "339b48bc-e0a4-4b80-ed71-4cd8b82418bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libdcmtk14 libdlib-data libdlib19 libfftw3-single3 libinsighttoolkit4.13\n",
            "  libminc2-5.2.0 libnifti2\n",
            "Suggested packages:\n",
            "  libfftw3-bin libfftw3-dev\n",
            "The following NEW packages will be installed:\n",
            "  libdcmtk14 libdlib-data libdlib19 libfftw3-single3 libinsighttoolkit4.13\n",
            "  libminc2-5.2.0 libnifti2 plastimatch\n",
            "0 upgraded, 8 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 80.1 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdcmtk14 amd64 3.6.4-2.1build2 [4,682 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdlib-data all 19.10-3build1 [63.4 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdlib19 amd64 19.10-3build1 [3,773 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libfftw3-single3 amd64 3.3.8-2ubuntu1 [756 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libminc2-5.2.0 amd64 2.4.03-2build3 [209 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libnifti2 amd64 2.0.0-3ubuntu1 [102 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libinsighttoolkit4.13 amd64 4.13.2-dfsg1-8 [4,426 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 plastimatch amd64 1.8.0+dfsg.1-2build1 [2,699 kB]\n",
            "Fetched 80.1 MB in 5s (15.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdcmtk14.\n",
            "(Reading database ... 129504 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdcmtk14_3.6.4-2.1build2_amd64.deb ...\n",
            "Unpacking libdcmtk14 (3.6.4-2.1build2) ...\n",
            "Selecting previously unselected package libdlib-data.\n",
            "Preparing to unpack .../1-libdlib-data_19.10-3build1_all.deb ...\n",
            "Unpacking libdlib-data (19.10-3build1) ...\n",
            "Selecting previously unselected package libdlib19:amd64.\n",
            "Preparing to unpack .../2-libdlib19_19.10-3build1_amd64.deb ...\n",
            "Unpacking libdlib19:amd64 (19.10-3build1) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../3-libfftw3-single3_3.3.8-2ubuntu1_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.8-2ubuntu1) ...\n",
            "Selecting previously unselected package libminc2-5.2.0:amd64.\n",
            "Preparing to unpack .../4-libminc2-5.2.0_2.4.03-2build3_amd64.deb ...\n",
            "Unpacking libminc2-5.2.0:amd64 (2.4.03-2build3) ...\n",
            "Selecting previously unselected package libnifti2.\n",
            "Preparing to unpack .../5-libnifti2_2.0.0-3ubuntu1_amd64.deb ...\n",
            "Unpacking libnifti2 (2.0.0-3ubuntu1) ...\n",
            "Selecting previously unselected package libinsighttoolkit4.13.\n",
            "Preparing to unpack .../6-libinsighttoolkit4.13_4.13.2-dfsg1-8_amd64.deb ...\n",
            "Unpacking libinsighttoolkit4.13 (4.13.2-dfsg1-8) ...\n",
            "Selecting previously unselected package plastimatch.\n",
            "Preparing to unpack .../7-plastimatch_1.8.0+dfsg.1-2build1_amd64.deb ...\n",
            "Unpacking plastimatch (1.8.0+dfsg.1-2build1) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.8-2ubuntu1) ...\n",
            "Setting up libdcmtk14 (3.6.4-2.1build2) ...\n",
            "Setting up libminc2-5.2.0:amd64 (2.4.03-2build3) ...\n",
            "Setting up libnifti2 (2.0.0-3ubuntu1) ...\n",
            "Setting up libinsighttoolkit4.13 (4.13.2-dfsg1-8) ...\n",
            "Setting up libdlib-data (19.10-3build1) ...\n",
            "Setting up libdlib19:amd64 (19.10-3build1) ...\n",
            "Setting up plastimatch (1.8.0+dfsg.1-2build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "plastimatch version 1.8.0\n",
            "Cloning into 'pyplastimatch'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 97 (delta 32), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.58 MiB | 20.55 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyradiomics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrIXcD9fMVLH",
        "outputId": "893a1e15-3f9a-447f-dfc9-ebdd28725c47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyradiomics\n",
            "  Downloading pyradiomics-3.0.1.tar.gz (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.8/dist-packages (from pyradiomics) (1.21.6)\n",
            "Requirement already satisfied: SimpleITK>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from pyradiomics) (2.2.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from pyradiomics) (1.4.1)\n",
            "Collecting pykwalify>=1.6.0\n",
            "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pyradiomics) (1.15.0)\n",
            "Collecting ruamel.yaml>=0.16.0\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.2)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (555 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m555.3/555.3 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyradiomics, docopt\n",
            "  Building wheel for pyradiomics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyradiomics: filename=pyradiomics-3.0.1-cp38-cp38-linux_x86_64.whl size=190633 sha256=9f59541f5efc579e46f1905f8155fd511456f0a2f30b7b9f05b4edb82949f583\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/be/06/5d09092e41d20673137f10ae62fb8d9da9adf14ce2552d7bea\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=fb71052836179b5853143953944ac3558930cf9a20d03d2e03ee9b7d740b0bf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built pyradiomics docopt\n",
            "Installing collected packages: docopt, ruamel.yaml.clib, ruamel.yaml, pykwalify, pyradiomics\n",
            "Successfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dcmqi \n",
        "!wget https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\n",
        "!tar zxvf dcmqi-1.2.5-linux.tar.gz\n",
        "!cp dcmqi-1.2.5-linux/bin/* /usr/local/bin/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTnwCWDdMb-z",
        "outputId": "8b35a1df-a529-45de-c782-2f1631411572"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:52:22--  https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/79d3ad95-9f0c-42a4-a1c5-bf5a63461894?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230122T055223Z&X-Amz-Expires=300&X-Amz-Signature=a1457ea272363ad197637667453757bb4f1194eeb914991237df20376643087d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.5-linux.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-01-22 05:52:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/79d3ad95-9f0c-42a4-a1c5-bf5a63461894?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230122T055223Z&X-Amz-Expires=300&X-Amz-Signature=a1457ea272363ad197637667453757bb4f1194eeb914991237df20376643087d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.5-linux.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21102129 (20M) [application/octet-stream]\n",
            "Saving to: ‘dcmqi-1.2.5-linux.tar.gz’\n",
            "\n",
            "dcmqi-1.2.5-linux.t 100%[===================>]  20.12M  16.8MB/s    in 1.2s    \n",
            "\n",
            "2023-01-22 05:52:24 (16.8 MB/s) - ‘dcmqi-1.2.5-linux.tar.gz’ saved [21102129/21102129]\n",
            "\n",
            "dcmqi-1.2.5-linux/bin/\n",
            "dcmqi-1.2.5-linux/bin/itkimage2segimage\n",
            "dcmqi-1.2.5-linux/bin/tid1500reader\n",
            "dcmqi-1.2.5-linux/bin/tid1500reader.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2segimage.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2paramap.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2paramap\n",
            "dcmqi-1.2.5-linux/bin/segimage2itkimage.xml\n",
            "dcmqi-1.2.5-linux/bin/segimage2itkimage\n",
            "dcmqi-1.2.5-linux/bin/tid1500writer.xml\n",
            "dcmqi-1.2.5-linux/bin/tid1500writer\n",
            "dcmqi-1.2.5-linux/bin/paramap2itkimage\n",
            "dcmqi-1.2.5-linux/bin/paramap2itkimage.xml\n",
            "dcmqi-1.2.5-linux/share/\n",
            "dcmqi-1.2.5-linux/share/doc/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/itksys/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/itksys/Copyright.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QImQTiwVnvxr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dicomweb-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HKGUrG1iAo5f"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import six\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.cloud import storage\n",
        "import nrrd\n",
        "import SimpleITK as sitk\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import transforms\n",
        "from matplotlib.colors import ListedColormap\n",
        "import csv\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "\n",
        "import radiomics\n",
        "from radiomics import featureextractor  # This module is used for interaction with pyradiomics\n",
        "\n",
        "import nibabel as nib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall highdicom\n",
        "!git clone https://github.com/herrmannlab/highdicom.git\n",
        "#!cd highdicom && python setup.py instally\n",
        "!cd highdicom && pip install ."
      ],
      "metadata": {
        "id": "juNDFA2y988w",
        "outputId": "930fd010-a43a-4004-8971-11926396efe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping highdicom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'highdicom'...\n",
            "remote: Enumerating objects: 5916, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 5916 (delta 17), reused 23 (delta 6), pack-reused 5871\u001b[K\n",
            "Receiving objects: 100% (5916/5916), 3.15 MiB | 5.48 MiB/s, done.\n",
            "Resolving deltas: 100% (3850/3850), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/highdicom\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydicom>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from highdicom==0.20.0) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from highdicom==0.20.0) (1.21.6)\n",
            "Requirement already satisfied: pillow>=8.3 in /usr/local/lib/python3.8/dist-packages (from highdicom==0.20.0) (9.4.0)\n",
            "Collecting pillow-jpls>=1.0\n",
            "  Downloading pillow_jpls-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (340 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.3/340.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: highdicom\n",
            "  Building wheel for highdicom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for highdicom: filename=highdicom-0.20.0-py3-none-any.whl size=801104 sha256=47e1f0c2c86278d140d2ad97feffe21cebc63ac4e48f0cefde229b0d40d2a306\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_6fvm6od/wheels/39/b1/b2/df4c5325095a74de2c73a50c272e69b2dfa6245f4dc2af6b10\n",
            "Successfully built highdicom\n",
            "Installing collected packages: pillow-jpls, highdicom\n",
            "Successfully installed highdicom-0.20.0 pillow-jpls-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install dcmtk \n",
        "!cp /usr/bin/dcmodify /usr/local/bin/dcmodify"
      ],
      "metadata": {
        "id": "CN8-qVDJbT_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a52a50-3989-4090-af84-212eb479b5af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  dcmtk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 864 kB of archives.\n",
            "After this operation, 3,727 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 dcmtk amd64 3.6.4-2.1build2 [864 kB]\n",
            "Fetched 864 kB in 1s (605 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package dcmtk.\n",
            "(Reading database ... 130041 files and directories currently installed.)\n",
            "Preparing to unpack .../dcmtk_3.6.4-2.1build2_amd64.deb ...\n",
            "Unpacking dcmtk (3.6.4-2.1build2) ...\n",
            "Setting up dcmtk (3.6.4-2.1build2) ...\n",
            "Adding `dcmtk' group to system ...\n",
            "Adding `dcmtk' user to system ...\n",
            "adduser: Warning: The home directory `/var/lib/dcmtk/db' does not belong to the user you are currently creating.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for the structured report \n",
        "\n",
        "import highdicom\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import highdicom as hd\n",
        "\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "\n",
        "from highdicom.sr.content import (\n",
        "    FindingSite,\n",
        "    ImageRegion,\n",
        "    ImageRegion3D,\n",
        "    SourceImageForRegion,\n",
        "    SourceImageForMeasurement,\n",
        "    SourceImageForMeasurementGroup\n",
        ")\n",
        "from highdicom.sr.enum import GraphicTypeValues3D\n",
        "from highdicom.sr.enum import GraphicTypeValues\n",
        "from highdicom.sr.sop import Comprehensive3DSR, ComprehensiveSR\n",
        "from highdicom.sr.templates import (\n",
        "    DeviceObserverIdentifyingAttributes,\n",
        "    Measurement,\n",
        "    MeasurementProperties,\n",
        "    MeasurementReport,\n",
        "    MeasurementsAndQualitativeEvaluations,\n",
        "    ObservationContext,\n",
        "    ObserverContext,\n",
        "    PersonObserverIdentifyingAttributes,\n",
        "    PlanarROIMeasurementsAndQualitativeEvaluations,\n",
        "    RelationshipTypeValues,\n",
        "    TrackingIdentifier,\n",
        "    QualitativeEvaluation,\n",
        "    ImageLibrary,\n",
        "    ImageLibraryEntryDescriptors\n",
        ")\n",
        "from highdicom.sr.value_types import (\n",
        "    CodedConcept,\n",
        "    CodeContentItem,\n",
        ")\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"highdicom.sr.sop\")\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "GqSSJZ0ULbWt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "duQSF98RvxIv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MudNQIu_SwqD",
        "outputId": "398c6188-53dd-4426-ef7d-c6565a7ce414"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip\n",
            "  Downloading https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m50.4 kB\u001b[0m \u001b[31m412.8 kB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.8/dist-packages (from dcmweb==1.0.2) (2.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from dcmweb==1.0.2) (2.25.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hurry.filesize\n",
            "  Downloading hurry.filesize-0.9.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire->dcmweb==1.0.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->dcmweb==1.0.2) (2.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth->dcmweb==1.0.2) (4.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from hurry.filesize->dcmweb==1.0.2) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->dcmweb==1.0.2) (2.10)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators->dcmweb==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth->dcmweb==1.0.2) (0.4.8)\n",
            "Building wheels for collected packages: dcmweb, fire, hurry.filesize, validators\n",
            "  Building wheel for dcmweb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dcmweb: filename=dcmweb-1.0.2-py3-none-any.whl size=26439 sha256=07eef71802628834a2d4106546efaea76751a52478bdcc3c44da538442595459\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gnniv1hq/wheels/72/cb/c0/8e1b6fd05e3af6d751c98b4a316bbe3c5658baedd3f5af98ad\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=fe2d58d55510b56ae4ab3b50dc2eb93129eccbf13253596c2c1aceb1f07a83be\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4134 sha256=2617f6c170caba3a5c5c7f3b0a2e4a1348a995c07fdcbfef4bf4c18993f43a05\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/4b/2b/e1eaf7375b72542a9a3f3c3fa66b7098cc9e8048fe345deace\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=184b70dd5034658dcc88757e728281adccd9e215ecb6afb96c07be5d3608461a\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "Successfully built dcmweb fire hurry.filesize validators\n",
            "Installing collected packages: validators, hurry.filesize, fire, dcmweb\n",
            "Successfully installed dcmweb-1.0.2 fire-0.5.0 hurry.filesize-0.9 validators-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload files:\n",
        "# - bpr_regions_code_mapping\n",
        "# - bpr_landmarks_code_mapping\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ZjUBvc61FIuB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload files:\n",
        "# - parameter file for radiomics \n",
        "# - meta json for segmentation\n",
        "# - segment code mapping\n",
        "# - shape feature code mapping\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ZXLeRAamNJpj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/lung_seg_meta.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYn-yzRcCnu9",
        "outputId": "994a3d88-40e5-4e59-fa2f-9cb001cafb19"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:53:30--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/lung_seg_meta.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1998 (2.0K) [text/plain]\n",
            "Saving to: ‘lung_seg_meta.json’\n",
            "\n",
            "\rlung_seg_meta.json    0%[                    ]       0  --.-KB/s               \rlung_seg_meta.json  100%[===================>]   1.95K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:31 (19.1 MB/s) - ‘lung_seg_meta.json’ saved [1998/1998]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r1lGxWNoR-Mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b88453b-901c-447a-b10b-7fbd091ca86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:53:31--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/segments_code_mapping.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 408 [text/plain]\n",
            "Saving to: ‘segments_code_mapping.csv’\n",
            "\n",
            "segments_code_mappi 100%[===================>]     408  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:31 (27.1 MB/s) - ‘segments_code_mapping.csv’ saved [408/408]\n",
            "\n",
            "--2023-01-22 05:53:31--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/shape_features_code_mapping.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1044 (1.0K) [text/plain]\n",
            "Saving to: ‘shape_features_code_mapping.csv’\n",
            "\n",
            "shape_features_code 100%[===================>]   1.02K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:31 (41.2 MB/s) - ‘shape_features_code_mapping.csv’ saved [1044/1044]\n",
            "\n",
            "--2023-01-22 05:53:31--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/param_ct.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3404 (3.3K) [text/plain]\n",
            "Saving to: ‘param_ct.yaml’\n",
            "\n",
            "param_ct.yaml       100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:32 (39.6 MB/s) - ‘param_ct.yaml’ saved [3404/3404]\n",
            "\n",
            "--2023-01-22 05:53:32--  https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/lung_seg_meta.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1998 (2.0K) [text/plain]\n",
            "Saving to: ‘/content/lung_seg_meta.json’\n",
            "\n",
            "lung_seg_meta.json  100%[===================>]   1.95K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:32 (29.0 MB/s) - ‘/content/lung_seg_meta.json’ saved [1998/1998]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the segments_code_mapping.csv file \n",
        "!wget -N https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/segments_code_mapping.csv\n",
        "\n",
        "# Get the shape_features_code_mapping.csv file \n",
        "!wget -N https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/shape_features_code_mapping.csv\n",
        "\n",
        "# Get the params_ct.yaml file \n",
        "!wget -N https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/param_ct.yaml\n",
        "\n",
        "# Get the lung_seg_meta.json file \n",
        "# !wget -N https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/lung_seg_meta.json\n",
        "!wget -N -P '/content/' https://raw.githubusercontent.com/deepakri201/MIDRC_colab/main/params/lung_seg_meta.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLoYZWLtu048"
      },
      "source": [
        "Get the csv that will contain the mapping for the BPR regions - this is needed for the creation of the structured report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002bf0e4-dc72-497b-b043-6b09eace6004",
        "id": "-cxp_ZyUu048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:53:32--  https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_regions_code_mapping.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228 [text/plain]\n",
            "Saving to: ‘bpr_regions_code_mapping.csv’\n",
            "\n",
            "bpr_regions_code_ma 100%[===================>]     228  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:32 (12.8 MB/s) - ‘bpr_regions_code_mapping.csv’ saved [228/228]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_regions_code_mapping.csv\n",
        "bpr_regions_df = pd.read_csv(\"bpr_regions_code_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52EPGSwMu048"
      },
      "source": [
        "Get the csv that will contain the mapping for the BPR landmarks -- this is needed for the creation of the structured report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10eb966f-7c21-4d30-c4b2-dfe8030bc79b",
        "id": "3Cl3YqTau048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:53:32--  https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_landmarks_code_mapping.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1797 (1.8K) [text/plain]\n",
            "Saving to: ‘bpr_landmarks_code_mapping.csv’\n",
            "\n",
            "bpr_landmarks_code_ 100%[===================>]   1.75K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-01-22 05:53:32 (25.8 MB/s) - ‘bpr_landmarks_code_mapping.csv’ saved [1797/1797]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_landmarks_code_mapping.csv\n",
        "landmarks_df = pd.read_csv(\"bpr_landmarks_code_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the DICOM validator "
      ],
      "metadata": {
        "id": "o4bkJV6V0G36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBI5fmb70GR4",
        "outputId": "62b9e5e9-d3b0-4990-b090-2d28830ed944"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 openjdk-8-jre-headless amd64 8u352-ga-1~20.04 [28.2 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 openjdk-8-jdk-headless amd64 8u352-ga-1~20.04 [8,293 kB]\n",
            "Fetched 36.5 MB in 3s (10.9 MB/s)\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 130215 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "hoM-vDAH0TGA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.dclunie.com/pixelmed/software/20221004_current/pixelmedjavadicom_binaryrelease.20221004.tar.bz2\n",
        "!wget http://www.dclunie.com/pixelmed/software/20221004_current/pixelmedjavadicom_dependencyrelease.20221004.tar.bz2\n",
        "\n",
        "!bunzip2  '/content/pixelmedjavadicom_binaryrelease.20221004.tar.bz2'\n",
        "!bunzip2 '/content/pixelmedjavadicom_dependencyrelease.20221004.tar.bz2' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOEK3E3x0Vpo",
        "outputId": "859c60cb-f678-4ca3-a861-f00fcf1f4a36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-21 11:10:34--  http://www.dclunie.com/pixelmed/software/20221004_current/pixelmedjavadicom_binaryrelease.20221004.tar.bz2\n",
            "Resolving www.dclunie.com (www.dclunie.com)... 52.84.162.98, 52.84.162.67, 52.84.162.4, ...\n",
            "Connecting to www.dclunie.com (www.dclunie.com)|52.84.162.98|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3407406 (3.2M) [application/x-bzip2]\n",
            "Saving to: ‘pixelmedjavadicom_binaryrelease.20221004.tar.bz2’\n",
            "\n",
            "pixelmedjavadicom_b 100%[===================>]   3.25M  6.76MB/s    in 0.5s    \n",
            "\n",
            "2023-01-21 11:10:35 (6.76 MB/s) - ‘pixelmedjavadicom_binaryrelease.20221004.tar.bz2’ saved [3407406/3407406]\n",
            "\n",
            "--2023-01-21 11:10:35--  http://www.dclunie.com/pixelmed/software/20221004_current/pixelmedjavadicom_dependencyrelease.20221004.tar.bz2\n",
            "Resolving www.dclunie.com (www.dclunie.com)... 52.84.162.98, 52.84.162.67, 52.84.162.4, ...\n",
            "Connecting to www.dclunie.com (www.dclunie.com)|52.84.162.98|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2714559 (2.6M) [application/x-bzip2]\n",
            "Saving to: ‘pixelmedjavadicom_dependencyrelease.20221004.tar.bz2’\n",
            "\n",
            "pixelmedjavadicom_d 100%[===================>]   2.59M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-01-21 11:10:35 (49.4 MB/s) - ‘pixelmedjavadicom_dependencyrelease.20221004.tar.bz2’ saved [2714559/2714559]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('/content/binaries'):\n",
        "  os.mkdir('/content/binaries')\n",
        "if not os.path.isdir('/content/dependencies'):\n",
        "  os.mkdir('/content/dependencies')\n",
        "!tar -xvf '/content/pixelmedjavadicom_binaryrelease.20221004.tar' -C '/content/binaries'\n",
        "!tar -xvf '/content/pixelmedjavadicom_dependencyrelease.20221004.tar' -C '/content/dependencies'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU23yK-x0Xc_",
        "outputId": "8b2d05db-9d22-4d50-ffbb-a84ba5e0ef6c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixelmed.jar\n",
            "BUILDDATE\n",
            "COPYRIGHT\n",
            "DeidentifyAndRedact.bat\n",
            "DeidentifyAndRedact.sh\n",
            "DeidentifyAndRedactWithOriginalFileName.bat\n",
            "DicomAttributeBrowser.sh\n",
            "DicomBrowser.sh\n",
            "DicomCleanerAssumingJREInstalled.bat\n",
            "DicomCleanerWithOwnJRE.bat\n",
            "DicomImageViewer.bat\n",
            "DicomImageViewer.sh\n",
            "DicomImageViewerWithCDJRE.bat\n",
            "DicomImageViewerWithOwnJRE.bat\n",
            "DicomInstanceValidator.sh\n",
            "DicomSRValidator.sh\n",
            "DoseUtilityAssumingJREInstalled.bat\n",
            "DoseUtilityWithOwnJRE.bat\n",
            "ECGViewer.bat\n",
            "ECGViewer.sh\n",
            "MoveDicomFilesIntoHierarchy.sh\n",
            "NetworkMediaImporter.bat\n",
            "NetworkMediaImporter.sh\n",
            "StructuredReportBrowser.sh\n",
            "StudyReceiver.bat\n",
            "studyreceiver.properties\n",
            "Makefile\n",
            "Makefile.common.mk\n",
            "README\n",
            "sample.com.pixelmed.display.DicomImageViewer.properties\n",
            "lib/additional/pixelmed_codec.jar\n",
            "lib/additional/pixelmed_imageio.jar\n",
            "./lib/additional/commons-codec-1.3.jar\n",
            "lib/additional/commons-net-ftp-2.0.jar\n",
            "./lib/additional/commons-compress-1.12.jar\n",
            "lib/additional/hsqldb.jar\n",
            "lib/additional/vecmath1.2-1.14.jar\n",
            "lib/additional/jmdns.jar\n",
            "lib/additional/aiviewer.jar\n",
            "lib/additional/javax.json-1.0.4.jar\n",
            "lib/additional/javax.json-api-1.0.jar\n",
            "lib/additional/opencsv-2.4.jar\n",
            "lib/junit/junit-4.8.1.jar\n",
            "LICENSES/\n",
            "LICENSES/hsqldb_lic.txt\n",
            "LICENSES/junit_cpl-v10.html\n",
            "LICENSES/aiviewer_COPYRIGHT.htm\n",
            "LICENSES/LICENSE-commons-net.txt\n",
            "LICENSES/jpedal.GPL.LICENSE.txt\n",
            "LICENSES/THIRD-PARTY-LICENSE-README-jai_imageio.txt\n",
            "LICENSES/LICENSE-jai_imageio.txt\n",
            "LICENSES/COPYRIGHT-jai_imageio.txt\n",
            "README\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('/content/binaries/lib'):\n",
        "  os.mkdir('/content/binaries/lib')\n",
        "!cp -r /content/dependencies/lib/* /content/binaries/lib"
      ],
      "metadata": {
        "id": "3l-lfEtl0anI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/binaries/\n",
        "!pwd\n",
        "!java -Xmx512m -Xms512m -cp \"./pixelmed.jar:./lib/additional/hsqldb.jar:./lib/additional/excalibur-bzip2-1.0.jar:./lib/additional/vecmath1.2-1.14.jar:./lib/additional/jmdns.jar:./lib/additional/commons-codec-1.3.jar:./lib/additional/jai_imageio.jar\" com.pixelmed.dicom.MultiFrameImageFactory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWW7VbBW0cpY",
        "outputId": "41def8c6-4ece-42dc-e33d-68e76276ff1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/binaries\n",
            "/content/binaries\n",
            "Error: Incorrect number of arguments\n",
            "Usage: MultiFrameImageFactory inputPaths outputPath\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !bash DicomSRValidator.sh '/content/data/sr_regions.dcm'"
      ],
      "metadata": {
        "id": "Yqs-vKTk0e15"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for generating SRs"
      ],
      "metadata": {
        "id": "nwkXpgpHshMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR regions"
      ],
      "metadata": {
        "id": "lpxpOp2-yNWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes as input a json file and slice_index, returns the regions assigned to the slice \n",
        "def convert_slice_to_region(bpr_data, slice_index):\n",
        "\n",
        "  \"\"\" \n",
        "  Given the slice_index, this returns a list of corresponding regions that match\n",
        "\n",
        "  Inputs: \n",
        "    bpr_data    : a dictionary, where for each of the six regions, a list of \n",
        "                  slice indices are provided \n",
        "    slice_index : slice number you want to obtain the list of classified regions \n",
        "                  for\n",
        "  Returns\n",
        "    regions     : list of regions for that slice_index\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # find where the slice_index appears across all regions\n",
        "  # get the names of the regions\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  region_names = list(bpr_data.keys())\n",
        "  regions = [] \n",
        "\n",
        "  for n in range(0,num_regions):\n",
        "    vals = bpr_data[region_names[n]]\n",
        "    if slice_index in vals:\n",
        "      regions.append(region_names[n])\n",
        "\n",
        "  return regions"
      ],
      "metadata": {
        "id": "1KHbBceCei7Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_structured_report_for_body_part_regression_regions(files, \n",
        "#                                                               json_file, \n",
        "#                                                               output_SR_file, \n",
        "#                                                               bpr_revision_number,\n",
        "#                                                               bpr_regions_df):\n",
        "\n",
        "#   \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "#      and writes a structured report (SR) to disk\n",
        "     \n",
        "#   Inputs: \n",
        "#     files               : list of CT dicom files \n",
        "#     json_file           : the json file created from the BodyPartRegression prediction\n",
        "#     output_SR_file      : output filename for the structured report \n",
        "#     bpr_revision_number : specific revision number of the bpr repo \n",
        "#     bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "#   Outputs:\n",
        "#     writes the SR out to the output_SR_file.    \n",
        "     \n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "#   num_files = len(files)\n",
        "#   # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "#   pos_all = []  \n",
        "#   sop_all = [] \n",
        "\n",
        "#   for n in range(0,num_files):\n",
        "#     # read dcm file \n",
        "#     filename = files[n]\n",
        "#     ds = dcmread(filename)\n",
        "#     # print(ds)\n",
        "\n",
        "#     # get ImageOrientation (0020, 0037)\n",
        "#     # print(ds['0x0020','0x0037'].value)\n",
        "#     ImageOrientation = ds['0x0020','0x0037'].value\n",
        "\n",
        "#     # get ImagePositionPatient (0020, 0032) \n",
        "#     ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "\n",
        "#     # calculate z value\n",
        "#     x_vector = ImageOrientation[0:3]\n",
        "#     y_vector = ImageOrientation[3:]\n",
        "#     z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "#     # multiple z_vector by ImagePositionPatient\n",
        "#     pos = np.dot(z_vector,ImagePositionPatient)\n",
        "#     pos_all.append(pos)\n",
        "\n",
        "#     # get the SOPInstanceUID \n",
        "#     sop = ds['0x0008', '0x0018'].value\n",
        "#     sop_all.append(sop)\n",
        "\n",
        "# #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "#   sorted_ind = np.argsort(pos_all)\n",
        "#   pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "#   sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "#   files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "#   #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "#   f = open(json_file)\n",
        "#   json_data = json.load(f)\n",
        "#   bpr_data = json_data['body part examined']\n",
        "\n",
        "#   # return a list where each entry is per slice and is a array of possible regions \n",
        "#   bpr_slice_scores = json_data['cleaned slice scores']\n",
        "#   num_slices = len(bpr_slice_scores)\n",
        "\n",
        "#   num_regions = len(bpr_data)\n",
        "#   regions = [] \n",
        "\n",
        "#   # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "#   for slice_index in range(0,num_slices):\n",
        "#     region = convert_slice_to_region(bpr_data, slice_index)\n",
        "#     regions.append(region)\n",
        "\n",
        "#   # ----- Create the structured report ----- # \n",
        "\n",
        "#   # Create the report content\n",
        "\n",
        "#   procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "#                                 meaning=\"Imaging procedure\")\n",
        "\n",
        "#   # Describe the context of reported observations: the person that reported\n",
        "#   # the observations and the device that was used to make the observations\n",
        "#   observer_person_context = ObserverContext(\n",
        "#       observer_type=codes.DCM.Person,\n",
        "#       observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "#           name='Anonymous^Reader'\n",
        "#       )\n",
        "#   )\n",
        "#   # observer_device_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Device,\n",
        "#   #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#   #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "#   #     )\n",
        "#   observer_device_context = ObserverContext(\n",
        "#     observer_type=codes.DCM.Device,\n",
        "#     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#         uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "#         model_name = bpr_revision_number\n",
        "#     )\n",
        "#   )\n",
        "#   observation_context = ObservationContext(\n",
        "#       #observer_person_context=observer_person_context,\n",
        "#       observer_device_context=observer_device_context,\n",
        "#   )\n",
        "\n",
        "#   imaging_measurements = []\n",
        "#   evidence = []\n",
        "\n",
        "#   tracking_uid = generate_uid()\n",
        "\n",
        "#   qualitative_evaluations = []\n",
        "\n",
        "#   print('num_slices: ' + str(num_slices))\n",
        "\n",
        "# #----------- Per slice ---------#\n",
        "\n",
        "#   for n in range(0,num_slices):\n",
        "\n",
        "#     slice_region = regions[n]\n",
        "\n",
        "#     # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "#     # ----- per region ---- # \n",
        "#     qualitative_evaluations = [] \n",
        "#     # num_regions = len(regions)\n",
        "#     num_regions = len(slice_region)\n",
        "\n",
        "#     # for region in regions: \n",
        "#     for region in slice_region: \n",
        "#       row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "#       qualitative_evaluations.append(\n",
        "#           QualitativeEvaluation(\n",
        "#               CodedConcept(\n",
        "#                             value=str(row[\"target_CodeValue\"].values[0]),\n",
        "#                             meaning=str(row[\"target_CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "#                             # meaning = \"Target Region\",\n",
        "#                             scheme_designator=str(row[\"target_CodingSchemeDesignator\"].values[0])\n",
        "#                             ), \n",
        "#               CodedConcept(\n",
        "#                             value=str(row[\"CodeValue\"].values[0]),\n",
        "#                             meaning=str(row[\"CodeMeaning\"].values[0]),\n",
        "#                             scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "#                           )\n",
        "#               )\n",
        "#           )\n",
        "\n",
        "#     # In the correct order \n",
        "#     reference_dcm_file = files_sorted[n]\n",
        "#     image_dataset = dcmread(reference_dcm_file)\n",
        "#     evidence.append(image_dataset)\n",
        "\n",
        "#     # NS\n",
        "#     #print(evidence[0])\n",
        "\n",
        "#     src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "#     # tracking_id = \"Annotations group x\"\n",
        "#     tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "#     measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "#                   tracking_identifier=TrackingIdentifier(\n",
        "#                       uid=tracking_uid,\n",
        "#                       identifier=tracking_id\n",
        "#                   ),\n",
        "#                   qualitative_evaluations=qualitative_evaluations,\n",
        "#                   source_images=[src_image]\n",
        "#               )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     imaging_measurements.append(\n",
        "#       measurements_group\n",
        "#             )\n",
        "    \n",
        "#   #-------------------#\n",
        "    \n",
        "#   measurement_report = MeasurementReport(\n",
        "#       observation_context=observation_context,\n",
        "#       procedure_reported=procedure_code,\n",
        "#       imaging_measurements=imaging_measurements\n",
        "#   )\n",
        "\n",
        "#   # Create the Structured Report instance\n",
        "#   series_instance_uid = generate_uid()\n",
        "#   sr_dataset = Comprehensive3DSR(\n",
        "#       evidence=evidence,\n",
        "#       content=measurement_report[0],\n",
        "#       series_number=100,\n",
        "#       series_instance_uid=series_instance_uid,\n",
        "#       sop_instance_uid=generate_uid(),\n",
        "#       instance_number=1,\n",
        "#       manufacturer='MIDRC',\n",
        "#       is_complete = True,\n",
        "#       is_final=True,\n",
        "#       series_description='BPR region annotations'\n",
        "#   )\n",
        "  \n",
        "\n",
        "\n",
        "#   pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "#   return sr_dataset"
      ],
      "metadata": {
        "id": "1opa_coW8an5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_regions(files, \n",
        "                                                              json_file, \n",
        "                                                              output_SR_file, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              bpr_regions_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "  # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # print(ds['0x0020','0x0037'].value)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value \n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  bpr_data = json_data['body part examined']\n",
        "\n",
        "  # return a list where each entry is per slice and is a array of possible regions \n",
        "  bpr_slice_scores = json_data['cleaned slice scores']\n",
        "  num_slices = len(bpr_slice_scores)\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  regions = [] \n",
        "\n",
        "  # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "  for slice_index in range(0,num_slices):\n",
        "    region = convert_slice_to_region(bpr_data, slice_index)\n",
        "    regions.append(region)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  observer_person_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Person,\n",
        "      observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "          name='Anonymous^Reader'\n",
        "      )\n",
        "  )\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "  #     )\n",
        "  observer_device_context = ObserverContext(\n",
        "    observer_type=codes.DCM.Device,\n",
        "    observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "        uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "        model_name = bpr_revision_number\n",
        "    )\n",
        "  )\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  # tracking_uid = generate_uid()\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  print('num_slices: ' + str(num_slices))\n",
        "\n",
        "\n",
        "  #----------- Per slice ---------#\n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    slice_region = regions[n]\n",
        "\n",
        "    # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "    # ----- per region ---- # \n",
        "    qualitative_evaluations = [] \n",
        "    finding_sites = [] \n",
        "    # num_regions = len(regions)\n",
        "    num_regions = len(slice_region)\n",
        "\n",
        "    # for region in regions: \n",
        "    for region in slice_region: \n",
        "      row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "\n",
        "      finding_sites.append(\n",
        "          FindingSite(\n",
        "              CodedConcept(\n",
        "                            value=str(row[\"CodeValue\"].values[0]), \n",
        "                            meaning=str(row[\"CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "                            scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "                            )\n",
        "              )\n",
        "          )\n",
        "      \n",
        "\n",
        "    # In the correct order \n",
        "    reference_dcm_file = files_sorted[n]\n",
        "    image_dataset = dcmread(reference_dcm_file)\n",
        "    evidence.append(image_dataset)\n",
        "\n",
        "    src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "    # tracking_id = \"Annotations group x\"\n",
        "    tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "    tracking_uid = generate_uid() # unique with tracking_id\n",
        "\n",
        "    measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                  tracking_identifier=TrackingIdentifier(\n",
        "                      uid=tracking_uid,\n",
        "                      identifier=tracking_id\n",
        "                  ),\n",
        "                  # qualitative_evaluations=qualitative_evaluations,\n",
        "                  finding_sites=finding_sites, \n",
        "                  source_images=[src_image]\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    imaging_measurements.append(\n",
        "      measurements_group\n",
        "            )\n",
        "    \n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=100,\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR region annotations'\n",
        "  )\n",
        "  # series_description='BPR region annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset"
      ],
      "metadata": {
        "id": "ziuVQi00ybPv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR landmarks"
      ],
      "metadata": {
        "id": "tU89HKmmyAYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(json_file):\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "WmzQFV7_63U3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_scores(scores, start_score, end_score):\n",
        "    scores = np.array(scores)\n",
        "    min_scores = np.where(scores < start_score)[0]\n",
        "    max_scores = np.where(scores > end_score)[0]\n",
        "\n",
        "    min_index = 0\n",
        "    max_index = len(scores)\n",
        "\n",
        "    if len(min_scores) > 0:\n",
        "        min_index = np.nanmax(min_scores)\n",
        "\n",
        "    if len(max_scores) > 0:\n",
        "        max_index = np.nanmin(max_scores)\n",
        "\n",
        "    return min_index, max_index"
      ],
      "metadata": {
        "id": "1PC6Zrvdh04p"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indices_from_json(filename, tag_start, tag_end):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of the particular anatomy specified by the tag_start and \n",
        "  tag_end for a particular patient. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "    tag_start : required - the string for the start of the anatomical region \n",
        "    tag_end   : required - the string for the end of the anatomical region \n",
        "\n",
        "  Outputs:\n",
        "    min_index : the minimum index in the patient coordinate system\n",
        "    max_index : the maximum index in the patient coordinate system \n",
        "  \"\"\"\n",
        "\n",
        "  # These scores are the same for all patients  \n",
        "  x = load_json(filename)\n",
        "\n",
        "  start_score = x[\"look-up table\"][tag_start][\"mean\"]\n",
        "  end_score = x[\"look-up table\"][tag_end][\"mean\"]\n",
        "\n",
        "  # The actual indices \n",
        "  min_index, max_index = crop_scores(x[\"cleaned slice scores\"], start_score, end_score)\n",
        "\n",
        "  return min_index, max_index"
      ],
      "metadata": {
        "id": "9dkNVhl_xMIW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_from_json(filename):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of each landmark in the patient json filename and converts \n",
        "  it to slice indices. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "\n",
        "  Outputs:\n",
        "    list of the corresponding landmark slices in the space of the patient. \n",
        "    Landmarks at the extreme ends - most inferior and most superior axial slices\n",
        "    are removed. \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the cleaned slice scores to determine the number of slices \n",
        "  x = load_json(filename)\n",
        "  num_slices = len(x[\"cleaned slice scores\"])\n",
        "\n",
        "  # Get the list of landmarks \n",
        "  landmarks = list(x[\"look-up table\"].keys())\n",
        "  num_landmarks = len(landmarks)\n",
        "\n",
        "  # Get the expected z_spacing - if less than 0, slices are in reverse order \n",
        "  valid_z_spacing = x[\"valid z-spacing\"]\n",
        "\n",
        "  # Get values for all tags \n",
        "  # Reorder the landmarks according to the mean values in ascending order \n",
        "  landmarks_dict_sorted = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    landmarks_dict_sorted[landmark] = x[\"look-up table\"][landmark]['mean']\n",
        "  landmark_dict_sorted = dict(sorted(landmarks_dict_sorted.items(), key=lambda item: item[1]))\n",
        "  landmarks = list(landmark_dict_sorted.keys())\n",
        "\n",
        "  # Calculate the actual slice indices of each landmark \n",
        "  landmark_indices = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    score = landmark_dict_sorted[landmark] # x[\"look-up table\"][landmark][\"mean\"]\n",
        "    min_index, max_index = crop_scores(x[\"cleaned slice scores\"], score, score)\n",
        "    if (valid_z_spacing > 0): \n",
        "      landmark_indices[landmark] = min_index \n",
        "    else: \n",
        "      landmark_indices[landmark] = num_slices - min_index \n",
        "\n",
        "  # Programmatically remove values from dictionary if it is at most inferior\n",
        "  # or most superior slice \n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    if (landmark_indices[landmark]==0):\n",
        "      del landmark_indices[landmark]\n",
        "    elif (landmark_indices[landmark]==num_slices-1):\n",
        "      del landmark_indices[landmark]\n",
        "\n",
        "  return landmark_indices"
      ],
      "metadata": {
        "id": "9XwAA1btxUen"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_list_in_slice(landmark_indices, slice_number):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the list of landmarks that are assigned to a particular slice number.\n",
        "\n",
        "  Arguments:\n",
        "    flandmark_indices  : the dictionary holding the slice number for each \n",
        "                         landmark\n",
        "    slice_number       : the particular slice to obtain the list of landmarks\n",
        "                         for\n",
        "\n",
        "  Outputs:\n",
        "    list of the landmarks that correspond to the slice_number\n",
        "  \"\"\"\n",
        "\n",
        "  key_list = list()\n",
        "  items_list = landmark_indices.items()\n",
        "  for item in items_list:\n",
        "    if item[1] == slice_number:\n",
        "        key_list.append(item[0])\n",
        "\n",
        "  return key_list\n",
        "\n",
        "def convert_landmarks_list_to_qualitative_evaluations(landmark_list,\n",
        "                                                      landmarks_df):\n",
        "\n",
        "  \"\"\"\n",
        "  Converts the list of landmarks to a qualitative_evaluations for the \n",
        "  structured report. \n",
        "\n",
        "  Arguments:\n",
        "    landmark_list  : list of landmarks that correspond to a particular slice\n",
        "    landmarks_df   : the dataframe holding the bpr landmarks metadata needed to\n",
        "                     create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    list of QualitativeEvaluation\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  qualitative_evaluations = [] \n",
        "  num_landmarks_in_slice = len(landmark_list)\n",
        "  for landmark in landmark_list: \n",
        "    if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "      print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "      break\n",
        "    else:\n",
        "      landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "      # landmark_code = CodedConcept(value = str(int(landmark_row[\"CodeValue\"].values[0])),\n",
        "      #                              meaning = landmark_row[\"CodeMeaning\"].values[0],\n",
        "      #                              scheme_designator = landmark_row[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "      #print(landmarks_df[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_modifier_code = None\n",
        "      if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "        # landmark_modifier_code = CodedConcept(value = str(int(landmark_row[\"modifier_CodeValue\"].values[0])),\n",
        "        #                              meaning = landmark_row[\"modifier_CodeMeaning\"].values[0],\n",
        "        #                              scheme_designator = landmark_row[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "        #print(landmarks_df[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        qual = QualitativeEvaluation(CodedConcept(\n",
        "                value=\"123014\",\n",
        "                meaning=\"Target Region\",\n",
        "                scheme_designator=\"DCM\"  \n",
        "                ), \n",
        "                landmark_code\n",
        "              )\n",
        "    \n",
        "    if landmark_modifier_code is not None:\n",
        "      qual_modifier = CodeContentItem(\n",
        "          name=CodedConcept(\n",
        "              value='106233006',\n",
        "              meaning='Topographical modifier',\n",
        "              scheme_designator='SCT',\n",
        "          ),\n",
        "          value=landmark_modifier_code,\n",
        "          relationship_type=RelationshipTypeValues.HAS_CONCEPT_MOD\n",
        "      )\n",
        "      qual.append(qual_modifier)\n",
        "\n",
        "    qualitative_evaluations.append(qual)\n",
        "\n",
        "  return qualitative_evaluations\n"
      ],
      "metadata": {
        "id": "REfEiAnUxeop"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "#                                                                 json_file, \n",
        "#                                                                 output_SR_file, \n",
        "#                                                                 bpr_revision_number,\n",
        "#                                                                 landmarks_df):\n",
        "\n",
        "#   \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "#      and writes a structured report (SR) to disk\n",
        "     \n",
        "#   Inputs: \n",
        "#     files               : list of CT dicom files \n",
        "#     json_file           : the json file created from the BodyPartRegression prediction\n",
        "#     output_SR_file      : output filename for the structured report \n",
        "#     bpr_revision_number : specific revision number of the bpr repo \n",
        "#     landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "#                           create the structured report\n",
        "\n",
        "#   Outputs:\n",
        "#     writes the SR out to the output_SR_file.    \n",
        "     \n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "#   num_files = len(files)\n",
        "\n",
        "#   pos_all = []  \n",
        "#   sop_all = [] \n",
        "\n",
        "#   for n in range(0,num_files):\n",
        "#     # read dcm file \n",
        "#     filename = files[n]\n",
        "#     ds = dcmread(filename)\n",
        "#     # print(ds)\n",
        "\n",
        "#     # get ImageOrientation (0020, 0037)\n",
        "#     # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "#     ImageOrientation = ds.ImageOrientationPatient\n",
        "#     #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "#     # get ImagePositionPatient (0020, 0032) \n",
        "#     # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "#     ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "#     # calculate z value\n",
        "#     x_vector = ImageOrientation[0:3]\n",
        "#     y_vector = ImageOrientation[3:]\n",
        "#     z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "#     # multiple z_vector by ImagePositionPatient\n",
        "#     pos = np.dot(z_vector,ImagePositionPatient)\n",
        "#     pos_all.append(pos)\n",
        "\n",
        "#     # get the SOPInstanceUID \n",
        "#     sop = ds['0x0008', '0x0018'].value\n",
        "#     sop_all.append(sop)\n",
        "\n",
        "#     #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "#   sorted_ind = np.argsort(pos_all)\n",
        "#   pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "#   sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "#   files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "#   #----- Get the landmarks as indices -----# \n",
        "\n",
        "#   landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "#   # ----- Create the structured report ----- # \n",
        "\n",
        "#   # Create the report content\n",
        "\n",
        "#   procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "#                                 meaning=\"Imaging procedure\")\n",
        "\n",
        "#   # Describe the context of reported observations: the person that reported\n",
        "#   # the observations and the device that was used to make the observations\n",
        "#   # observer_person_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Person,\n",
        "#   #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "#   #         name='Anonymous^Reader'\n",
        "#   #     )\n",
        "#   # )\n",
        "\n",
        "#   # observer_device_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Device,\n",
        "#   #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#   #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "#   #     )\n",
        "#   # )\n",
        "\n",
        "#   observer_device_context = ObserverContext(\n",
        "#       observer_type=codes.DCM.Device,\n",
        "#       observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#           uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "#           model_name = bpr_revision_number\n",
        "#       )\n",
        "#   )\n",
        "#   # inputMetadata[\"observerContext\"] = {\n",
        "#   #                   \"ObserverType\": \"DEVICE\",\n",
        "#   #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "#   #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "#   #                 }\n",
        "\n",
        "#   observation_context = ObservationContext(\n",
        "#       #observer_person_context=observer_person_context,\n",
        "#       observer_device_context=observer_device_context,\n",
        "#   )\n",
        "\n",
        "#   imaging_measurements = []\n",
        "#   evidence = []\n",
        "\n",
        "#   qualitative_evaluations = []\n",
        "\n",
        "#   tracking_uid = generate_uid()\n",
        "\n",
        "#   #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "#   num_slices = len(files_sorted)\n",
        "#   annotation_count = 1 \n",
        "\n",
        "#   for n in range(0,num_slices):\n",
        "\n",
        "#     # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "#     landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "#     # Only include if there is a landmark for a slice \n",
        "#     if (landmark_indices_list):\n",
        "    \n",
        "#       # Create QualitativeEvaluations\n",
        "#       qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "#                                                                                   landmarks_df)\n",
        "\n",
        "#       # In the correct order \n",
        "#       reference_dcm_file = files_sorted[n]\n",
        "#       image_dataset = dcmread(reference_dcm_file)\n",
        "#       evidence.append(image_dataset)\n",
        "\n",
        "#       src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "#       # tracking_id = \"Annotations group x\"\n",
        "#       # tracking_id = \"Annotations group landmarks\"\n",
        "#       # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "#       tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "#       measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "#                     tracking_identifier=TrackingIdentifier(\n",
        "#                         uid=tracking_uid,\n",
        "#                         identifier=tracking_id\n",
        "#                     ),\n",
        "#                     qualitative_evaluations=qualitative_evaluations,\n",
        "#                     source_images=[src_image]\n",
        "#                 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       imaging_measurements.append(\n",
        "#         measurements_group\n",
        "#               )\n",
        "      \n",
        "#       annotation_count += 1 # keep track of number of annotations\n",
        "\n",
        "#        #-------------------#\n",
        "    \n",
        "#   measurement_report = MeasurementReport(\n",
        "#       observation_context=observation_context,\n",
        "#       procedure_reported=procedure_code,\n",
        "#       imaging_measurements=imaging_measurements\n",
        "#   )\n",
        "\n",
        "#   # Create the Structured Report instance\n",
        "#   series_instance_uid = generate_uid()\n",
        "#   sr_dataset = Comprehensive3DSR(\n",
        "#       evidence=evidence,\n",
        "#       content=measurement_report[0],\n",
        "#       series_number=101, # was 100 for regions\n",
        "#       series_instance_uid=series_instance_uid,\n",
        "#       sop_instance_uid=generate_uid(),\n",
        "#       instance_number=1,\n",
        "#       manufacturer='IDC',\n",
        "#       is_complete = True,\n",
        "#       is_final=True,\n",
        "#       series_description='BPR landmark annotations'\n",
        "#   )\n",
        "#   # series_description='BPR landmark annotations'\n",
        "\n",
        "#   pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "#   return sr_dataset"
      ],
      "metadata": {
        "id": "R6qri9J-xo3o"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "                                                                json_file, \n",
        "                                                                output_SR_file, \n",
        "                                                                bpr_revision_number,\n",
        "                                                                landmarks_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "                          create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "    #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID \n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #----- Get the landmarks as indices -----# \n",
        "\n",
        "  landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  # observer_person_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Person,\n",
        "  #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "  #         name='Anonymous^Reader'\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  observer_device_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Device,\n",
        "      observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "          uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "          model_name = bpr_revision_number\n",
        "      )\n",
        "  )\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                   \"ObserverType\": \"DEVICE\",\n",
        "  #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                 }\n",
        "\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  # qualitative_evaluations = []\n",
        "\n",
        "  # tracking_uid = generate_uid()\n",
        "\n",
        "  #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "  num_slices = len(files_sorted)\n",
        "  annotation_count = 1 \n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    finding_sites = []  \n",
        "\n",
        "    # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "    landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "    # Only include if there is a landmark for a slice \n",
        "    if (landmark_indices_list):\n",
        "      # Create QualitativeEvaluations\n",
        "      # qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "      #                                                                             landmarks_df)\n",
        "      num_landmarks_in_slice = len(landmark_indices_list)\n",
        "\n",
        "      for landmark in landmark_indices_list: \n",
        "        if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "          print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "          break\n",
        "        else:\n",
        "          landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "          landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                      meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                      scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "          landmark_modifier_code = None \n",
        "          # If there is a modifier, add it to finding_sites\n",
        "          if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "            landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                                  meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                                  scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "            finding_sites.append(\n",
        "                FindingSite(\n",
        "                    anatomic_location=landmark_code,\n",
        "                    topographical_modifier=landmark_modifier_code\n",
        "                )\n",
        "            )\n",
        "          # If no modifier, do not add to finding_sites \n",
        "          else: \n",
        "            finding_sites.append(\n",
        "                FindingSite(\n",
        "                    anatomic_location = landmark_code\n",
        "                    )\n",
        "                )\n",
        "\n",
        "      \n",
        "\n",
        "      # In the correct order \n",
        "      reference_dcm_file = files_sorted[n]\n",
        "      image_dataset = dcmread(reference_dcm_file)\n",
        "      evidence.append(image_dataset)\n",
        "\n",
        "      src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "      # tracking_id = \"Annotations group x\"\n",
        "      # tracking_id = \"Annotations group landmarks\"\n",
        "      # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "      tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "      tracking_uid = generate_uid()\n",
        "\n",
        "      measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                    tracking_identifier=TrackingIdentifier(\n",
        "                        uid=tracking_uid,\n",
        "                        identifier=tracking_id\n",
        "                    ),\n",
        "                    # qualitative_evaluations=qualitative_evaluations,\n",
        "                    finding_sites=finding_sites,\n",
        "                    source_images=[src_image]\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      imaging_measurements.append(\n",
        "        measurements_group\n",
        "              )\n",
        "      \n",
        "      annotation_count += 1 # keep track of number of annotations\n",
        "    \n",
        "\n",
        "\n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=101, # was 100 for regions\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR landmark annotations'\n",
        "  )\n",
        "  # series_description='BPR landmark annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "WR17ipkayize"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR radiomics"
      ],
      "metadata": {
        "id": "ovHTLDmYNb87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_and_names_from_metadata_json(dicomseg_json):\n",
        "\n",
        "  \"\"\"Returns two lists containing the label values and the corresponding\n",
        "     CodeMeaning values\n",
        "\n",
        "  Inputs: \n",
        "    dicomseg_json : metajson file\n",
        "\n",
        "  Outputs:\n",
        "    label_values  : label values from the metajson file \n",
        "    label_names   : the corresponding CodeMeaning values \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  f = open(dicomseg_json)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  #print(meta_json)\n",
        "\n",
        "  num_regions = len(meta_json['segmentAttributes'][0])\n",
        "  print ('num_regions: ' + str(num_regions))\n",
        "\n",
        "  label_values = []\n",
        "  label_names = [] \n",
        "  for n in range(0,num_regions):\n",
        "    # label_values.append(n)\n",
        "    label_value = meta_json['segmentAttributes'][0][n]['labelID']\n",
        "    #label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning']\n",
        "    # NS - \n",
        "    #label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning'] +'_'+str(label_value)\n",
        "    label_name = meta_json['segmentAttributes'][0][n]['SegmentDescription'] # Left lung, Right lung\n",
        "    label_values.append(label_value)\n",
        "    label_names.append(label_name)\n",
        "\n",
        "  return label_values, label_names"
      ],
      "metadata": {
        "id": "iO0T1wRcFVou"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_nii(input_file, output_directory, label_names):\n",
        "\n",
        "  \"\"\"Function to split a single multilabel nii into individual nii files. Used\n",
        "     for pyradiomics feature extraction. \n",
        "\n",
        "  Inputs: \n",
        "    input_file       : input multi-label nii file \n",
        "    output_directory : where to save the individual nii segments \n",
        "    label_names      : the names of the labels that correspond to the order of \n",
        "                       the values in the nii input_file \n",
        "\n",
        "  Outputs:\n",
        "    saves the individual nii files to the output_directory \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.isdir(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "  # save with the values in the files \n",
        "  nii = nib.load(input_file)\n",
        "  header = nii.header \n",
        "  img = nii.get_fdata() \n",
        "  unique_labels = list(np.unique(img))\n",
        "  unique_labels.remove(0) # remove the background \n",
        "\n",
        "  # split and save \n",
        "  num_labels = len(unique_labels)\n",
        "  for n in range(0,num_labels):\n",
        "    ind = np.where(img==unique_labels[n])\n",
        "    vol = np.zeros((img.shape))\n",
        "    vol[ind] = 1\n",
        "    new_img = nib.Nifti1Image(vol, nii.affine, nii.header)\n",
        "    output_filename = os.path.join(output_directory, label_names[n] + '.nii.gz')\n",
        "    nib.save(new_img, output_filename)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "Rm49POmEFeQe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    split_pred_nifti_path, \n",
        "                                    nnunet_shape_features_code_mapping_df):\n",
        "\n",
        "  \"\"\"Function to compute pyradiomics 3D features for each label in a nifti file. \n",
        "     \n",
        "\n",
        "  Inputs: \n",
        "    ct_nifti_path            : the CT nifti file \n",
        "    label_values             : the label value for each of the segments from the json file \n",
        "    label_names              : the corresponding label name for each of the segments \n",
        "    split_pred_nifti_path    : where to save the individual nii segments needed \n",
        "                               for pyradiomics\n",
        "    nnunet_shape_features_code_mapping_df : the df where we will obtain the \n",
        "                                            list of the shape features to \n",
        "                                            compute\n",
        "\n",
        "  Outputs:\n",
        "    Writes the features_csv_path_nnunet to disk. \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the names of the features from the nnunet_shape_features_code_mapping_df\n",
        "  shape_features = list(nnunet_shape_features_code_mapping_df['shape_feature'].values)\n",
        "\n",
        "  # Instantiate the extractor and modify the settings to keep the 3D shape features\n",
        "  extractor = featureextractor.RadiomicsFeatureExtractor(fn_param)\n",
        "  extractor.settings['minimumROIDimensions'] = 3 \n",
        "  extractor.disableAllFeatures()\n",
        "  extractor.enableFeaturesByName(shape=shape_features) \n",
        "\n",
        "  # Calculate features for each label and create a dataframe\n",
        "  num_labels = len([f for f in os.listdir(split_pred_nifti_path) if f.endswith('.nii.gz')]) # was .nii.gz\n",
        "  print(num_labels)\n",
        "  df_list = [] \n",
        "  for n in range(0,num_labels):\n",
        "    mask_path = os.path.join(split_pred_nifti_path, label_names[n] + '.nii.gz')  # was .nii.gz\n",
        "    \n",
        "    # Run the extractor \n",
        "    result = extractor.execute(ct_nifti_path, mask_path) # dictionary\n",
        "    # keep only the features we want\n",
        "    # Get the corresponding label number -- all might not be present \n",
        "    corresponding_label_value = label_values[label_names.index(label_names[n])] \n",
        "    dict_keep = {'ReferencedSegment': corresponding_label_value, \n",
        "                 'label_name': label_names[n]}\n",
        "    keys_keep = [f for f in result.keys() if 'original_shape' in f]\n",
        "    # Just keep the feature keys we want\n",
        "    dict_keep_new_values = {key_keep: result[key_keep] for key_keep in keys_keep}\n",
        "    dict_keep.update(dict_keep_new_values)\n",
        "    df1 = pd.DataFrame([dict_keep])\n",
        "    # change values of columns to remove original_shape_\n",
        "    df1.columns = df1.columns.str.replace('original_shape_', '')\n",
        "    # Append to the ReferencedSegment and label_name df \n",
        "    df_list.append(df1)\n",
        "\n",
        "  # concat all label features \n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "SBIdR3ZpKhd9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted"
      ],
      "metadata": {
        "id": "HYjMey5mKq6O"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted"
      ],
      "metadata": {
        "id": "-bnpAyyWN8of"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                         SOPInstanceUID_seg,\n",
        "                                                         seg_file, \n",
        "                                                         dcm_directory, \n",
        "                                                         segments_code_mapping_df,\n",
        "                                                         shape_features_code_mapping_df,\n",
        "                                                         df_features, \n",
        "                                                         SegmentAlgorithmName\n",
        "                                                         ):\n",
        "  \n",
        "  \"\"\"Function that creates the metajson necessary for the creation of a\n",
        "  structured report from a pandas dataframe of label names and features for \n",
        "  each. \n",
        "\n",
        "  Inputs: \n",
        "    SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                      file \n",
        "    SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "    seg_file                        : filename of SEG DCM file \n",
        "    dcm_directory                   : ct directory that will be sorted in \n",
        "                                      terms of axial ordering according to the \n",
        "                                      ImagePositionPatient and ImageOrientation \n",
        "                                      fields\n",
        "    segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                      segments and the associated code values etc.\n",
        "    shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                      features and the associated code values etc. \n",
        "    df_features                     : a pandas dataframe holding the segments and a \n",
        "                                      set of 3D shape features for each \n",
        "    SegmentAlgorithmName            : the name of the algorithm used to create the \n",
        "                                      segmentations - e.g. '3d_fullres_tta_nnUnet'\n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "  \"\"\" \n",
        "\n",
        "  # --- Get the version number for the pyradiomics package --- #\n",
        "\n",
        "  pyradiomics_version_number = str(radiomics.__version__)\n",
        "  \n",
        "  # --- Sort the dcm files first according to --- # \n",
        "  # --- ImagePositionPatient and ImageOrientation --- #\n",
        "\n",
        "  files_sorted, sop_all_sorted, pos_all_sorted = order_dicom_files_image_position(dcm_directory)\n",
        "  files_sorted = [os.path.basename(f) for f in files_sorted]\n",
        "\n",
        "  # --- Create the header for the json --- # \n",
        "  \n",
        "  inputMetadata = {}\n",
        "  inputMetadata[\"@schema\"]= \"https://raw.githubusercontent.com/qiicr/dcmqi/master/doc/schemas/sr-tid1500-schema.json#\"\n",
        "  # inputMetadata[\"SeriesDescription\"] = \"Measurements\"\n",
        "  inputMetadata[\"SeriesDescription\"] = SegmentAlgorithmName + '_' + \"Measurements\"\n",
        "  inputMetadata[\"SeriesNumber\"] = \"1001\"\n",
        "  inputMetadata[\"InstanceNumber\"] = \"1\"\n",
        "\n",
        "  inputMetadata[\"compositeContext\"] = [seg_file] # not full path\n",
        "\n",
        "  inputMetadata[\"imageLibrary\"] = files_sorted # not full path \n",
        "\n",
        "   # inputMetadata[\"observerContext\"] = {\n",
        "  #                                     \"ObserverType\": \"PERSON\",\n",
        "  #                                     \"PersonObserverName\": \"Reader1\"\n",
        "  #                                   }\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                     \"ObserverType\": \"DEVICE\",\n",
        "  #                     \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                     \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                   }\n",
        "  inputMetadata[\"observerContext\"] = {\n",
        "                      \"ObserverType\": \"DEVICE\",\n",
        "                      \"DeviceObserverName\": \"pyradiomics\",\n",
        "                      \"DeviceObserverModelName\": pyradiomics_version_number\n",
        "                    }\n",
        "\n",
        "  inputMetadata[\"VerificationFlag\"]  = \"UNVERIFIED\"\n",
        "  inputMetadata[\"CompletionFlag\"] =  \"COMPLETE\"\n",
        "  inputMetadata[\"activitySession\"] = \"1\"\n",
        "  inputMetadata[\"timePoint\"] = \"1\"\n",
        "\n",
        "  # ------------------------------------------------------------------------- # \n",
        "  # --- Create the measurement_dict for each segment - holds all features --- # \n",
        "\n",
        "  measurement = [] \n",
        "\n",
        "  # --- Now create the dict for all features and all segments --- #\n",
        "\n",
        "  # --- Loop over the number of segments --- #\n",
        "\n",
        "  # number of rows in the df_features \n",
        "  num_segments = df_features.shape[0]\n",
        "\n",
        "  # Array of dictionaries - one dictionary for each segment \n",
        "  measurement_across_segments_combined = [] \n",
        "\n",
        "  laterality = [\"Right\", \"Left\"]\n",
        "\n",
        "  for segment_id in range(0,num_segments):\n",
        "\n",
        "    ReferencedSegment = df_features['ReferencedSegment'].values[segment_id]\n",
        "    FindingSite = df_features['label_name'].values[segment_id]\n",
        "\n",
        "    print('segment_id: ' + str(segment_id))\n",
        "    print('ReferencedSegment: ' + str(ReferencedSegment))\n",
        "    print('FindingSite: ' + str(FindingSite))\n",
        "\n",
        "    # --- Create the dict for the Measurements group --- # \n",
        "    # TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment)\n",
        "    TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment) + \" - \" + str(laterality[segment_id])\n",
        "\n",
        "    segment_row = segments_code_mapping_df[segments_code_mapping_df[\"segment\"] == FindingSite]\n",
        "    # print(segment_row)\n",
        "        \n",
        "    my_dict = {\n",
        "      \"TrackingIdentifier\": str(TrackingIdentifier),\n",
        "      \"ReferencedSegment\": int(ReferencedSegment),\n",
        "      \"SourceSeriesForImageSegmentation\": str(SeriesInstanceUID),\n",
        "      \"segmentationSOPInstanceUID\": str(SOPInstanceUID_seg),\n",
        "      \"Finding\": {\n",
        "        \"CodeValue\": \"113343008\",\n",
        "        \"CodingSchemeDesignator\": \"SCT\",\n",
        "        \"CodeMeaning\": \"Organ\"\n",
        "      }, \n",
        "      \"FindingSite\": {\n",
        "        \"CodeValue\": str(segment_row[\"FindingSite_CodeValue\"].values[0]),\n",
        "        \"CodingSchemeDesignator\": str(segment_row[\"FindingSite_CodingSchemeDesignator\"].values[0]),\n",
        "        \"CodeMeaning\": str(segment_row[\"FindingSite_CodeMeaning\"].values[0])\n",
        "      }\n",
        "      # \"Finding\": {\n",
        "      #   \"CodeValue\": str(segment_row[\"Finding_CodeValue\"].values[0]),\n",
        "      #   \"CodingSchemeDesignator\": str(segment_row[\"Finding_CodingSchemeDesignator\"].values[0]),\n",
        "      #   \"CodeMeaning\": str(segment_row[\"Finding_CodeMeaning\"].values[0])\n",
        "      # }, \n",
        "      # \"FindingSite\": {\n",
        "      #   \"CodeValue\": str(segment_row[\"FindingSite_CodeValue\"].values[0]),\n",
        "      #   \"CodingSchemeDesignator\": str(segment_row[\"FindingSite_CodingSchemeDesignator\"].values[0]),\n",
        "      #   \"CodeMeaning\": str(segment_row[\"FindingSite_CodeMeaning\"].values[0])\n",
        "      # }\n",
        "    }\n",
        "\n",
        "    measurement = []  \n",
        "    # number of features - number of columns in df_features - 2 (label_name and ReferencedSegment)\n",
        "    num_values = len(df_features.columns)-2 \n",
        "\n",
        "    feature_list = df_features.columns[2:] # remove first two \n",
        "\n",
        "\n",
        "    # For each measurement per region segment\n",
        "    for n in range(0,num_values): \n",
        "      measurement_dict = {}\n",
        "      row = df_features.loc[df_features['label_name'] == FindingSite]\n",
        "      feature_row = shape_features_code_mapping_df.loc[shape_features_code_mapping_df[\"shape_feature\"] == feature_list[n]]\n",
        "      value = str(np.round(row[feature_list[n]].values[0],3))\n",
        "      measurement_dict[\"value\"] = value\n",
        "      measurement_dict[\"quantity\"] = {}\n",
        "      measurement_dict[\"quantity\"][\"CodeValue\"] = str(feature_row[\"quantity_CodeValue\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodingSchemeDesignator\"] = str(feature_row[\"quantity_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodeMeaning\"] = str(feature_row[\"quantity_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"units\"] = {}\n",
        "      measurement_dict[\"units\"][\"CodeValue\"] = str(feature_row[\"units_CodeValue\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodingSchemeDesignator\"] = str(feature_row[\"units_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodeMeaning\"] = str(feature_row[\"units_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"] = {}\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmName\"] = \"pyradiomics\"\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmVersion\"] = str(pyradiomics_version_number)\n",
        "      measurement.append(measurement_dict) \n",
        "\n",
        "    measurement_combined_dict = {}\n",
        "    measurement_combined_dict['measurementItems'] = measurement # measurement is an array of dictionaries \n",
        "\n",
        "    output_dict_one_segment = {**my_dict, **measurement_combined_dict}\n",
        "\n",
        "    # append to array for all segments \n",
        "\n",
        "    measurement_across_segments_combined.append(output_dict_one_segment)\n",
        "\n",
        "  # --- Add the measurement data --- # \n",
        "\n",
        "  inputMetadata[\"Measurements\"] = {}\n",
        "  inputMetadata[\"Measurements\"] = measurement_across_segments_combined\n",
        "\n",
        "  return inputMetadata\n"
      ],
      "metadata": {
        "id": "CDU5sEH3F9QK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_structured_report( inputMetadata, sr_json_path, dcm_directory, sr_path, pred_dicomseg_path):\n",
        "  with open(sr_json_path, 'w') as f:\n",
        "    json.dump(inputMetadata, f, indent=2)\n",
        "  print ('wrote out json for shape features')\n",
        "\n",
        "  # --- Save the SR for nnUNet shape features --- # \n",
        "  # inputImageLibraryDirectory = os.path.join(\"/content\", \"raw\")\n",
        "  # outputDICOM = os.path.join(\"/content\",\"features_sr.dcm\")\n",
        "  # inputCompositeContextDirectory = os.path.join(\"/content\",\"seg\")\n",
        "  inputImageLibraryDirectory = dcm_directory\n",
        "  # outputDICOM = sr_json_path\n",
        "  outputDICOM = sr_path\n",
        "  # the name of the folder where the seg files are located \n",
        "  inputCompositeContextDirectory = pred_dicomseg_path  # os.path.basename(pred_dicomseg_path) # might need to check this\n",
        "  inputMetadata_json = sr_json_path \n",
        "\n",
        "  print ('inputImageLibraryDirectory: ' + str(inputImageLibraryDirectory))\n",
        "  print ('outputDICOM: ' + str(outputDICOM))\n",
        "  print ('inputCompositeContextDirectory: ' + str(inputCompositeContextDirectory))\n",
        "  print ('inputMetadata_json: ' + str(inputMetadata_json)) \n",
        "  !tid1500writer --inputImageLibraryDirectory $inputImageLibraryDirectory \\\n",
        "                --outputDICOM $outputDICOM  \\\n",
        "                --inputCompositeContextDirectory $inputCompositeContextDirectory \\\n",
        "                --inputMetadata $inputMetadata_json\n",
        "  print ('wrote out SR for shape features')\n"
      ],
      "metadata": {
        "id": "wIhPGH0HYkUB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "3TrfBR8HsXgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sR8c3D6j1Vvh"
      },
      "outputs": [],
      "source": [
        "# #%%bigquery --project=bwh-midrc-rapid-res-1655321320 ct_limited_open_a1_r1\n",
        "\n",
        "# # python API is the most flexible way to query IDC BigQuery metadata tables\n",
        "# from google.cloud import bigquery\n",
        "# bq_client = bigquery.Client(project_name)\n",
        "\n",
        "# selection_query = \"\"\"\n",
        "\n",
        "# WITH\n",
        "#   nlst_instances_per_series AS (\n",
        "#     SELECT\n",
        "#       DISTINCT(StudyInstanceUID),\n",
        "#       SeriesInstanceUID,\n",
        "#       COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "#       MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "#       MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#       MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "#       MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "#       STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#     FROM\n",
        "#       bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "#     WHERE\n",
        "#       (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "#     GROUP BY\n",
        "#       StudyInstanceUID,\n",
        "#       SeriesInstanceUID\n",
        "#       ), \n",
        "#   nlst_values_per_series AS (\n",
        "#     SELECT \n",
        "#     ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#     ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "#     ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_instances_per_series\n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "#   WHERE\n",
        "#     min_SliceThickness >= 1.5 \n",
        "#     AND max_SliceThickness <= 3.5 \n",
        "#     AND nlst_instances_per_series.num_instances > 100\n",
        "#     AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "#     AND nlst_instances_per_series.orientation_count = 1\n",
        "#     AND has_localizer = \"false\"\n",
        "#   GROUP BY\n",
        "#     SeriesInstanceUID\n",
        "#   )\n",
        "#   SELECT \n",
        "#     dicom_all.PatientID,\n",
        "#     dicom_all.StudyInstanceUID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     dicom_all.SOPInstanceUID,\n",
        "#     dicom_all.collection_id,\n",
        "#     dicom_all.PatientAge,\n",
        "#     dicom_all.PatientWeight,\n",
        "#     nlst_values_per_series.num_instances,\n",
        "#     nlst_values_per_series.SliceThickness,\n",
        "#     nlst_values_per_series.PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_values_per_series \n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID\n",
        "#  \"\"\"\n",
        "# selection_result = bq_client.query(selection_query)\n",
        "# ct_limited_open_a1_r1 = selection_result.result().to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the dataframe from the table instead of creating again. "
      ],
      "metadata": {
        "id": "5SY7AopGbn5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=project_name)\n",
        "table_id = '.'.join([project_name, dataset_table_id, table_view_id_name])\n",
        "\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    * \n",
        "  FROM\n",
        "    {table_id};\n",
        "  \"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "ct_limited_open_a1_r1 = result.to_dataframe(create_bqstorage_client=True)\n"
      ],
      "metadata": {
        "id": "W00I_l2bbS71"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "6-cXmi6_s8sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def listdir_bucket(project_name, bucket_name, dir_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - directory GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_list : list of files in the specified GCS bucket.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_dir_relative = dir_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "\n",
        "  print(\"Getting the list of files at `%s`...\"%(dir_gs_uri))\n",
        "\n",
        "  file_list = list()\n",
        "\n",
        "  for blob in storage_client.list_blobs(bucket_name,  prefix = path_to_dir_relative):\n",
        "    fn = os.path.basename(blob.name)\n",
        "    file_list.append(fn)\n",
        "\n",
        "  return file_list"
      ],
      "metadata": {
        "id": "WP6ZZrAt1mLI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_series(study_instance_uid, series_instance_uid, sop_instance_uids, dest_dir):\n",
        "  import pydicom\n",
        "  token = !gcloud auth print-access-token\n",
        "  token = token[0]\n",
        "\n",
        "  PROJECT_ID=\"bwh-midrc-rapid-res-1655321320\"\n",
        "  REGION=\"us-central1\"\n",
        "\n",
        "  DATASET_ID=\"midrc\"\n",
        "  DICOM_STORE_ID=\"midrc-dicom\"\n",
        "  \n",
        "  my_project = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  location = \"us-central1\"\n",
        "  dataset_id = \"midrc\"\n",
        "  dicom_store_id = \"midrc-dicom\"\n",
        "\n",
        "  # url = f\"https://healthcare.googleapis.com/v1/projects/{my_project}/locations/{location}/datasets/{dataset_id}/dicomStores/{dicom_store_id}/dicomWeb\"\n",
        "  # headers = {\n",
        "  #     \"Authorization\" : \"Bearer %s\" % token\n",
        "  # }\n",
        "\n",
        "  # import dicomweb_client\n",
        "\n",
        "  # client = dicomweb_client.api.DICOMwebClient(url, headers=headers)\n",
        "\n",
        "  # idx=0\n",
        "  # for sop_instance_uid in sop_instance_uids:\n",
        "  #   retrievedInstance = client.retrieve_instance(\n",
        "  #               study_instance_uid=study_instance_uid,\n",
        "  #               series_instance_uid=series_instance_uid,\n",
        "  #               sop_instance_uid=sop_instance_uid)\n",
        "  #   pydicom.filewriter.write_file(f\"{dest_dir}/file{idx}.dcm\", retrievedInstance)\n",
        "  #   idx+=1\n",
        "\n",
        "  url = os.path.join('https://healthcare.googleapis.com/v1', \n",
        "                     'projects', my_project, \n",
        "                     'locations', 'us-central1', \n",
        "                     'datasets', dataset_id, \n",
        "                     'dicomStores', dicom_store_id, \n",
        "                     'dicomWeb/')\n",
        "  url_study_and_series = os.path.join('studies', \n",
        "                                      study_instance_uid,\n",
        "                                      'series', \n",
        "                                      series_instance_uid)\n",
        "  print (url)\n",
        "  print(url_study_and_series)\n",
        "\n",
        "  !dcmweb -m $url retrieve $url_study_and_series $dest_dir\n",
        "  \n",
        "  input_dir = os.path.join(dest_dir, study_instance_uid, series_instance_uid)\n",
        "  !mv $input_dir/* $dest_dir"
      ],
      "metadata": {
        "id": "KE05zkHQnxrn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0q6ovBXnRWeY"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching %s for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "  \n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "urllib3_logger = logging.getLogger('urllib3')\n",
        "urllib3_logger.setLevel(logging.CRITICAL) # suppress messages upon download"
      ],
      "metadata": {
        "id": "RQjDEsQN_z71"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.cloud import storage\n",
        "\n",
        "# storage_client = storage.Client(project = project_name)\n",
        "# #bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "# series_instance_uids = []\n",
        "\n",
        "# #blobs = storage_client.list_blobs(bucket)\n",
        "# blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path, delimiter='/')\n",
        "# for blob in blobs:\n",
        "#     bn = blob.name \n",
        "#     # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "#     (r, ext) = os.path.splitext(bn)\n",
        "#     #print(r,ext)\n",
        "#     if ext == '.json':\n",
        "#       u = os.path.split(r)[-1]\n",
        "#       #print(u)\n",
        "#       series_instance_uids.append(u)\n",
        "\n",
        "# print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOCZFuo4YG6C",
        "outputId": "9dbace31-bdd8-445f-f579-9b7daee7d2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = ct_limited_open_a1_r1\n",
        "# k=0\n",
        "\n",
        "# good_seriesuids = []\n",
        "\n",
        "# #series_instance_uids = ['1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715']\n",
        "\n",
        "# for seriesuid in series_instance_uids:\n",
        "#   #print(seriesuid)\n",
        "#   studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "#   if len(studyuids)>0:\n",
        "#     k=k+1\n",
        "#     #print(seriesuid)\n",
        "#     studyuid = studyuids[0]\n",
        "#     sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "#     print(len(sops))\n",
        "#     #download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "#     good_seriesuids.append(seriesuid)\n",
        "# print(k)"
      ],
      "metadata": {
        "id": "4xxeSrvYoouz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of dcmseg files from the bucket, this will tell us the series to process. \n"
      ],
      "metadata": {
        "id": "33uca58n3RC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series_instance_uids = list(set(ct_limited_open_a1_r1['SeriesInstanceUID'].values))\n",
        "\n",
        "bucket_seg_folder_uri_bpr = os.path.join(\"gs://\", bucket_name, bucket_path)\n",
        "print(\"bucket_seg_folder_uri_bpr: \" + str(bucket_seg_folder_uri_bpr))\n",
        "\n",
        "seg_bucket_list = listdir_bucket(project_name = project_name, # or gcs.listdir_bucket\n",
        "                                 bucket_name = bucket_name,\n",
        "                                 dir_gs_uri = bucket_seg_folder_uri_bpr)\n",
        "\n",
        "good_seriesuids = [f.split(\".dcm\")[0] for f in seg_bucket_list if f.endswith(\".dcm\")] # or seg_ if ends with .nrrd\n",
        "good_seriesuids = [f.split(\"dcmseg_\")[1] for f in good_seriesuids]\n",
        "\n",
        "print(len(good_seriesuids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv5qKGhW17NO",
        "outputId": "e293b288-ee45-46cd-a16b-a188bf42d22c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bucket_seg_folder_uri_bpr: gs://midrc-analysis-bwh-dk/bpr-results/\n",
            "Getting the list of files at `gs://midrc-analysis-bwh-dk/bpr-results/`...\n",
            "377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now get the files that should be processed that don't have DICOM SR files."
      ],
      "metadata": {
        "id": "LwkwQChR3Us_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series_id_list = good_seriesuids\n",
        "\n",
        "# exclude from processing all the patients for which a DICOM SEG object was exported already\n",
        "# (stored in the specified Google Cloud Storage Bucket)\n",
        "bucket_sr_folder_uri_bpr = os.path.join(\"gs://\", bucket_name, bucket_path_sr)\n",
        "print(\"bucket_sr_folder_uri_bpr: \" + str(bucket_sr_folder_uri_bpr))\n",
        "\n",
        "sr_bucket_list = listdir_bucket(project_name = project_name, # or gcs.listdir_bucket\n",
        "                                  bucket_name = bucket_name,\n",
        "                                  dir_gs_uri = bucket_sr_folder_uri_bpr)\n",
        "\n",
        "# already_processed_id_list = [f.split(\"_SEG\")[0] for f in json_bucket_list if f.endswith(\".json\")]\n",
        "# already_processed_id_list = [f.split(\".json\")[0] for f in json_bucket_list if f.endswith(\".json\")]\n",
        "\n",
        "# already_processed_id_list = [f.split(\".dcm\")[0] for f in json_bucket_list if f.endswith(\".dcm\")] # or seg_ if ends with .nrrd\n",
        "# already_processed_id_list = [f.split(\"dcmseg_\")[1] for f in already_processed_id_list]\n",
        "\n",
        "already_processed_id_list = [f.split(\"sr_landmarks_\")[1] for f in sr_bucket_list if f.startswith(\"sr_landmarks\")]\n",
        "already_processed_id_list = [f.split(\".dcm\")[0] for f in already_processed_id_list]\n",
        "\n",
        "print(\"\\nFound %g series already processed.\"%(len(already_processed_id_list)))\n",
        "\n",
        "series_to_process_id_list = sorted(list(set(series_id_list) - set(already_processed_id_list)))\n",
        "\n",
        "print(\"Moving on with the remaining %g...\"%(len(series_to_process_id_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuyfFHyvcha1",
        "outputId": "23a45fc2-4eaa-4394-fa0a-b176187bcfb3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bucket_sr_folder_uri_bpr: gs://midrc-analysis-bwh-dk/structured_reports/\n",
            "Getting the list of files at `gs://midrc-analysis-bwh-dk/structured_reports/`...\n",
            "\n",
            "Found 375 series already processed.\n",
            "Moving on with the remaining 2...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing series\n",
        "\n",
        "path_downloaded = '/content/downloaded_data'\n",
        "path_nifti = '/content/nifti_data'\n",
        "path_json =  '/content/json_data'\n",
        "path_nrrd = '/content/nrrd_data'\n",
        "path_sr = '/content/sr_data'\n",
        "path_dcm = '/content/dcm_data'\n",
        "path_labels = '/content/labels'\n",
        "\n",
        "df = ct_limited_open_a1_r1\n",
        "\n",
        "fn_param = os.path.join('/content','param_ct.yaml')\n",
        "fn_seg_code_mapping = os.path.join('/content','segments_code_mapping.csv')\n",
        "fn_feature_code_mapping = os.path.join('/content','shape_features_code_mapping.csv')\n",
        "fn_seg_meta = os.path.join('/content','lung_seg_meta.json')\n",
        "\n",
        "seg_code_mapping_df = pd.read_csv(fn_seg_code_mapping)\n",
        "feature_code_mapping_df = pd.read_csv(fn_feature_code_mapping)\n",
        "\n",
        "df_regions = pd.read_csv('/content/bpr_regions_code_mapping.csv')\n",
        "df_landmarks = pd.read_csv('/content/bpr_landmarks_code_mapping.csv')\n",
        "\n",
        "fn_metadata_json = '/content/structured_report_metadata.json'\n",
        "\n",
        "\n",
        "#good_seriesuids = ['1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952']\n",
        "\n",
        "# for seriesuid in good_seriesuids:\n",
        "for seriesuid in series_to_process_id_list: \n",
        "\n",
        "  print('\\n\\nSeries %d / %d' %( series_to_process_id_list.index(seriesuid), len(series_to_process_id_list)) )\n",
        "\n",
        "  # clean up from previous iterations and recreate temp directories\n",
        "  if 1:\n",
        "    for x in [path_downloaded, path_nifti, path_json, path_nrrd, path_sr, path_dcm, path_labels]:\n",
        "      if os.path.isdir(x):\n",
        "        try:\n",
        "          shutil.rmtree(x)\n",
        "        except OSError as err:\n",
        "          print(\"Error: %s : %s\" % (x, err.strerror))  \n",
        "      os.mkdir(x)\n",
        "\n",
        "  studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "  if len(studyuids)>0:\n",
        "    print(seriesuid)\n",
        "  studyuid = studyuids[0]\n",
        "  sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "  print(len(sops))\n",
        "\n",
        "  # bucket_path_sr = \"structured_reports/\"\n",
        "  \n",
        "  # file_gs_uri = \"gs://%s/%ssr_regions_%s.dcm\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "  # if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  #   print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "  #   continue\n",
        "\n",
        "  ### Download data  ###\n",
        "  download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "  !rmdir $path_downloaded/$studyuid/$seriesuid\n",
        "  !rmdir $path_downloaded/$studyuid/\n",
        "\n",
        "  ### Copy bpr json file ### \n",
        "  file_gs_uri_json = \"gs://%s/%s%s.json\" % (bucket_name, bucket_path, seriesuid)\n",
        "  cmd1 = \"gsutil cp %s %s\" % (file_gs_uri_json, path_json)\n",
        "  print(cmd1)\n",
        "  os.system(cmd1)\n",
        "\n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  #print(dcm_input_list)\n",
        "  \n",
        "  ### patch missing DICOM fields ###\n",
        "\n",
        "  cmd2 = \"dcmodify -i '(0010,0030)=20000101' /content/downloaded_data/*.dcm\"\n",
        "  print(cmd2)\n",
        "  os.system(cmd2)\n",
        "  \n",
        "  cmd3 = \"dcmodify -i '(0010,0040)=O' /content/downloaded_data/*.dcm\"\n",
        "  print(cmd3)\n",
        "  os.system(cmd3)\n",
        "  \n",
        "  ### create output filenames ### \n",
        "\n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  fn_json = os.path.join(path_json, '%s.json' % seriesuid)\n",
        "  fn_sr_regions = os.path.join(path_sr, 'sr_regions_%s.dcm' % seriesuid)\n",
        "  fn_sr_landmarks = os.path.join(path_sr, 'sr_landmarks_%s.dcm' % seriesuid)\n",
        "\n",
        "  file_gs_uri_seg = \"gs://%s/%sseg_%s.nrrd\" % (bucket_name, bucket_path, seriesuid) # \"/\"  exists in the path\n",
        "  file_gs_uri_ct = \"gs://%s/%sct_%s.nrrd\" % (bucket_name, bucket_path, seriesuid)\n",
        "  # file_gs_uri_dcmseg = \"gs://%s/%sdcmseg_%s\" % (bucket_name, bucket_path, seriesuid)\n",
        "  file_gs_uri_dcmseg = \"gs://%s/%sdcmseg_%s\" % (bucket_name, bucket_path, seriesuid + '.dcm')\n",
        "  \n",
        "  ### copy the seg, ct nrrd and dicom seg previously created ### \n",
        "\n",
        "  cmd4 = \"gsutil cp %s %s\" % (file_gs_uri_seg, path_nrrd)\n",
        "  print(cmd4)\n",
        "  os.system(cmd4)\n",
        "  \n",
        "  cmd5 = \"gsutil cp %s %s\" % (file_gs_uri_ct, path_nrrd)\n",
        "  print(cmd5)\n",
        "  os.system(cmd5)\n",
        "  \n",
        "  cmd6 = \"gsutil cp %s %s\" % (file_gs_uri_dcmseg, path_dcm)\n",
        "  print(cmd6)\n",
        "  os.system(cmd6)\n",
        "\n",
        "  fn_seg_nrrd = os.path.join(path_nrrd, 'seg_%s.nrrd' % seriesuid)\n",
        "  fn_ct_nrrd = os.path.join(path_nrrd, 'ct_%s.nrrd' % seriesuid)\n",
        "\n",
        "  # lung masks\n",
        "  lungs, _ = nrrd.read(fn_seg_nrrd)\n",
        "  tempr =np.where(lungs==1) \n",
        "  templ =np.where(lungs==2) \n",
        "\n",
        "  #print(templ)\n",
        "  #print(tempr)\n",
        "  print(len(templ[0]), len(tempr[0]))\n",
        "\n",
        "  if len(templ[0]) == 0 or len(tempr[0]) == 0:\n",
        "    print(\"SKIP NO LUNG\", seriesuid)\n",
        "    continue \n",
        "\n",
        "  print('converting NRRD to NII')\n",
        "  fn_ct_nifti = os.path.join(path_nifti,  \"ct_%s.nii\" % seriesuid)\n",
        "  log_file_path_ct_nifti = os.path.join(path_nifti, 'pypla_ct.log')\n",
        "  convert_args_ct = {\"input\" : fn_ct_nrrd, \"output-img\" : fn_ct_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_ct_nifti, **convert_args_ct)\n",
        "  \n",
        "  fn_seg_nifti = os.path.join(path_nifti,  \"seg_%s.nii\" % seriesuid)\n",
        "  log_file_path_seg_nifti = os.path.join(path_nifti, 'pypla_seg.log')\n",
        "  convert_args_seg = {\"input\" : fn_seg_nrrd, \"interpolation\" : \"nn\", \"output-img\" : fn_seg_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_seg_nifti, **convert_args_seg)\n",
        "\n",
        "  fn_seg_dcm = os.path.join(path_dcm, 'dcmseg_%s' % seriesuid + '.dcm')\n",
        "\n",
        "  label_values, label_names = get_label_and_names_from_metadata_json(fn_seg_meta)\n",
        "  print(label_values, label_names)\n",
        "  split_nii( fn_seg_nifti , path_labels, label_names)\n",
        "\n",
        "  ds = dcmread(fn_seg_dcm)\n",
        "  sop = ds.SOPInstanceUID\n",
        "  print(sop)\n",
        "\n",
        "  create_structured_report_for_body_part_regression_regions(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_regions, \n",
        "                                                              '1.1.0',\n",
        "                                                              df_regions)\n",
        "  \n",
        "  create_structured_report_for_body_part_regression_landmarks(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_landmarks, \n",
        "                                                              '1.1.0',\n",
        "                                                              df_landmarks)\n",
        "  \n",
        "  df_out = compute_pyradiomics_3D_features(fn_ct_nifti, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    path_labels, \n",
        "                                    feature_code_mapping_df)\n",
        "  \n",
        "  # metajson = create_structured_report_metajson_for_shape_features(seriesuid, \n",
        "  #                                                        sop,\n",
        "  #                                                        fn_seg_dcm, \n",
        "  #                                                        path_downloaded, \n",
        "  #                                                        seg_code_mapping_df,\n",
        "  #                                                        feature_code_mapping_df,\n",
        "  #                                                        df_out, \n",
        "  #                                                        'lungmask'\n",
        "  #                                                        )\n",
        "  \n",
        "  # fn_sr_radiomics = os.path.join(path_sr, 'sr_radiomics_%s.dcm' % seriesuid)\n",
        "  # save_structured_report( metajson, fn_metadata_json, path_downloaded, fn_sr_radiomics, path_dcm )\n",
        "\n",
        "  # bucket_path_sr = \"structured_reports/\"\n",
        "  \n",
        "  # file_gs_uri = \"gs://%s/%ssr_regions_%s.nrrd\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "  # if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  #   print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "  #   continue\n",
        "\n",
        "  # cmd7 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_regions, bucket_name, bucket_path_sr)\n",
        "  # print(cmd7)\n",
        "  # os.system(cmd7)\n",
        "\n",
        "  # cmd8 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_landmarks, bucket_name, bucket_path_sr)\n",
        "  # print(cmd8)\n",
        "  # os.system(cmd8)\n",
        "\n",
        "  # cmd9 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_radiomics, bucket_name, bucket_path_sr)\n",
        "  # print(cmd9)\n",
        "  # os.system(cmd9)\n",
        "\n",
        "  metajson = create_structured_report_metajson_for_shape_features(seriesuid, \n",
        "                                                        sop,\n",
        "                                                        fn_seg_dcm, \n",
        "                                                        path_downloaded, \n",
        "                                                        seg_code_mapping_df,\n",
        "                                                        feature_code_mapping_df,\n",
        "                                                        df_out, \n",
        "                                                        'lungmask'\n",
        "                                                        )\n",
        "\n",
        "  fn_sr_radiomics = os.path.join(path_sr, 'sr_radiomics_%s.dcm' % seriesuid)\n",
        "  save_structured_report( metajson, fn_metadata_json, path_downloaded, fn_sr_radiomics, path_dcm )\n",
        "\n",
        "  bucket_path_sr = \"structured_reports/\"\n",
        "\n",
        "  file_gs_uri = \"gs://%s/%ssr_regions_%s.nrrd\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "  if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "    print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "    # continue\n",
        "\n",
        "  cmd7 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_regions, bucket_name, bucket_path_sr)\n",
        "  print(cmd7)\n",
        "  os.system(cmd7)\n",
        "\n",
        "  cmd8 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_landmarks, bucket_name, bucket_path_sr)\n",
        "  print(cmd8)\n",
        "  os.system(cmd8)\n",
        "\n",
        "  cmd9 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_radiomics, bucket_name, bucket_path_sr)\n",
        "  print(cmd9)\n",
        "  os.system(cmd9)\n",
        "\n",
        "  # Validate these SRs \n",
        "  # !bash DicomSRValidator.sh $fn_sr_regions\n",
        "  # !bash DicomSRValidator.sh $fn_sr_landmarks\n",
        "  # !bash DicomSRValidator.sh $fn_sr_radiomics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4owL01weBFvx",
        "outputId": "1913d08c-027e-4e92-8a45-f7d01386f5ee"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Series 0 / 2\n",
            "1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0\n",
            "171\n",
            "https://healthcare.googleapis.com/v1/projects/bwh-midrc-rapid-res-1655321320/locations/us-central1/datasets/midrc/dicomStores/midrc-dicom/dicomWeb/\n",
            "studies/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1865.0/series/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0\n",
            "2023-01-22 05:53:58,230 -- No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2023-01-22 05:53:58,817 -- Saving files into /content/downloaded_data\n",
            "2023-01-22 05:54:00,416 -- Transferred 3M in 6 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:01,417 -- Transferred 9M in 18 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:02,418 -- Transferred 12M in 24 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:03,419 -- Transferred 18M in 36 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:04,420 -- Transferred 21M in 43 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:05,421 -- Transferred 27M in 55 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:06,422 -- Transferred 34M in 68 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:07,423 -- Transferred 38M in 77 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:08,935 -- Transferred 47M in 95 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:09,936 -- Transferred 53M in 106 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:10,937 -- Transferred 59M in 118 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:11,938 -- Transferred 65M in 131 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:12,938 -- Transferred 71M in 142 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:13,939 -- Transferred 77M in 154 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:14,940 -- Transferred 82M in 165 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:15,474 -- Transferred 86M in 171 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:15,478 -- \n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0.json /content/json_data\n",
            "dcmodify -i '(0010,0030)=20000101' /content/downloaded_data/*.dcm\n",
            "dcmodify -i '(0010,0040)=O' /content/downloaded_data/*.dcm\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/seg_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/ct_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/dcmseg_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0.dcm /content/dcm_data\n",
            "0 2\n",
            "SKIP NO LUNG 1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1872.0\n",
            "\n",
            "\n",
            "Series 1 / 2\n",
            "1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0\n",
            "170\n",
            "https://healthcare.googleapis.com/v1/projects/bwh-midrc-rapid-res-1655321320/locations/us-central1/datasets/midrc/dicomStores/midrc-dicom/dicomWeb/\n",
            "studies/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.1865.0/series/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0\n",
            "2023-01-22 05:54:34,108 -- No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2023-01-22 05:54:34,632 -- Saving files into /content/downloaded_data\n",
            "2023-01-22 05:54:36,119 -- Transferred 4M in 8 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:37,120 -- Transferred 10M in 21 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:38,121 -- Transferred 16M in 33 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:39,122 -- Transferred 22M in 44 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:40,124 -- Transferred 28M in 56 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:41,125 -- Transferred 34M in 68 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:42,126 -- Transferred 39M in 79 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:43,465 -- Transferred 47M in 95 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:44,466 -- Transferred 53M in 107 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:45,467 -- Transferred 59M in 119 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:46,468 -- Transferred 65M in 130 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:47,469 -- Transferred 72M in 145 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:48,470 -- Transferred 79M in 159 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:49,298 -- Transferred 85M in 170 files     \u001b[1A\u001b[\u001b[80D\n",
            "2023-01-22 05:54:49,301 -- \n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0.json /content/json_data\n",
            "dcmodify -i '(0010,0030)=20000101' /content/downloaded_data/*.dcm\n",
            "dcmodify -i '(0010,0040)=O' /content/downloaded_data/*.dcm\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/seg_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/ct_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0.nrrd /content/nrrd_data\n",
            "gsutil cp gs://midrc-analysis-bwh-dk/bpr-results/dcmseg_1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0.dcm /content/dcm_data\n",
            "0 13\n",
            "SKIP NO LUNG 1.3.6.1.4.1.5962.99.1.3492129840.282517718.1618866294295.2359.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7a10jWuEET3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "c6Buayc8f9cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Cbx-ldKDRFu_",
        "outputId": "c6ae8afa-9ca1-4908-e707-656398225b3f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ReferencedSegment  label_name  Elongation  Flatness  LeastAxisLength  \\\n",
              "0                  1  Right lung    0.502484  0.248114        47.539884   \n",
              "0                  2   Left lung    0.386416  0.290330        53.063060   \n",
              "\n",
              "   MajorAxisLength   Maximum3DDiameter          MeshVolume  MinorAxisLength  \\\n",
              "0       191.605163    184.967564724197   429726.0416666667        96.278561   \n",
              "0       182.768135  185.16479146965278  190442.79166666666        70.624508   \n",
              "\n",
              "           Sphericity        SurfaceArea   SurfaceVolumeRatio  VoxelVolume  \\\n",
              "0  0.4950695165194799  55626.15658428198  0.12944562626118553     429913.0   \n",
              "0  0.4538582880652275  35270.11533972844  0.18520057929765055     190603.0   \n",
              "\n",
              "   Compactness1         Compactness2 SphericalDisproportion  \n",
              "0      0.018480  0.12133848203222435     2.0199183480945595  \n",
              "0      0.016221  0.09348906405776661      2.203330921338782  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-489423ff-472b-4c7a-8cba-2185c1150039\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReferencedSegment</th>\n",
              "      <th>label_name</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Compactness1</th>\n",
              "      <th>Compactness2</th>\n",
              "      <th>SphericalDisproportion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Right lung</td>\n",
              "      <td>0.502484</td>\n",
              "      <td>0.248114</td>\n",
              "      <td>47.539884</td>\n",
              "      <td>191.605163</td>\n",
              "      <td>184.967564724197</td>\n",
              "      <td>429726.0416666667</td>\n",
              "      <td>96.278561</td>\n",
              "      <td>0.4950695165194799</td>\n",
              "      <td>55626.15658428198</td>\n",
              "      <td>0.12944562626118553</td>\n",
              "      <td>429913.0</td>\n",
              "      <td>0.018480</td>\n",
              "      <td>0.12133848203222435</td>\n",
              "      <td>2.0199183480945595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Left lung</td>\n",
              "      <td>0.386416</td>\n",
              "      <td>0.290330</td>\n",
              "      <td>53.063060</td>\n",
              "      <td>182.768135</td>\n",
              "      <td>185.16479146965278</td>\n",
              "      <td>190442.79166666666</td>\n",
              "      <td>70.624508</td>\n",
              "      <td>0.4538582880652275</td>\n",
              "      <td>35270.11533972844</td>\n",
              "      <td>0.18520057929765055</td>\n",
              "      <td>190603.0</td>\n",
              "      <td>0.016221</td>\n",
              "      <td>0.09348906405776661</td>\n",
              "      <td>2.203330921338782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-489423ff-472b-4c7a-8cba-2185c1150039')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-489423ff-472b-4c7a-8cba-2185c1150039 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-489423ff-472b-4c7a-8cba-2185c1150039');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seg_code_mapping_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "wHCArckRQt9Y",
        "outputId": "53ce989f-36a9-48ef-88f6-efceca177910"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      segment Finding_CodingSchemeDesignator  Finding_CodeValue  \\\n",
              "0  Right lung                            SCT          123037004   \n",
              "1   Left lung                            SCT          123037004   \n",
              "\n",
              "    Finding_CodeMeaning FindingSite_CodingSchemeDesignator  \\\n",
              "0  Anatomical Structure                                SCT   \n",
              "1  Anatomical Structure                                SCT   \n",
              "\n",
              "   FindingSite_CodeValue FindingSite_CodeMeaning  \\\n",
              "0               39607008                    Lung   \n",
              "1               39607008                    Lung   \n",
              "\n",
              "  FindingSite_CodingSchemeDesignator.1  FindingLaterality_CodeValue  \\\n",
              "0                                  SCT                     24028007   \n",
              "1                                  SCT                      7771000   \n",
              "\n",
              "  FindingSite_CodeMeaning.1  \n",
              "0                     Right  \n",
              "1                      Left  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c6db834-1fcb-419f-8110-18d8e5dced5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>Finding_CodingSchemeDesignator</th>\n",
              "      <th>Finding_CodeValue</th>\n",
              "      <th>Finding_CodeMeaning</th>\n",
              "      <th>FindingSite_CodingSchemeDesignator</th>\n",
              "      <th>FindingSite_CodeValue</th>\n",
              "      <th>FindingSite_CodeMeaning</th>\n",
              "      <th>FindingSite_CodingSchemeDesignator.1</th>\n",
              "      <th>FindingLaterality_CodeValue</th>\n",
              "      <th>FindingSite_CodeMeaning.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Right lung</td>\n",
              "      <td>SCT</td>\n",
              "      <td>123037004</td>\n",
              "      <td>Anatomical Structure</td>\n",
              "      <td>SCT</td>\n",
              "      <td>39607008</td>\n",
              "      <td>Lung</td>\n",
              "      <td>SCT</td>\n",
              "      <td>24028007</td>\n",
              "      <td>Right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Left lung</td>\n",
              "      <td>SCT</td>\n",
              "      <td>123037004</td>\n",
              "      <td>Anatomical Structure</td>\n",
              "      <td>SCT</td>\n",
              "      <td>39607008</td>\n",
              "      <td>Lung</td>\n",
              "      <td>SCT</td>\n",
              "      <td>7771000</td>\n",
              "      <td>Left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c6db834-1fcb-419f-8110-18d8e5dced5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c6db834-1fcb-419f-8110-18d8e5dced5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c6db834-1fcb-419f-8110-18d8e5dced5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_code_mapping_df "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "uxZnai-LQv2G",
        "outputId": "34563bf8-861d-44c8-8916-432bcf6cea25"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             shape_feature quantity_CodingSchemeDesignator quantity_CodeValue  \\\n",
              "0               Elongation                            IBSI               Q3CK   \n",
              "1                 Flatness                            IBSI               N17B   \n",
              "2          LeastAxisLength                            IBSI               7J51   \n",
              "3          MajorAxisLength                            IBSI               TDIC   \n",
              "4        Maximum3DDiameter                            IBSI               L0JK   \n",
              "5               MeshVolume                            IBSI               RNU0   \n",
              "6          MinorAxisLength                            IBSI               P9VJ   \n",
              "7               Sphericity                            IBSI               QCFX   \n",
              "8              SurfaceArea                            IBSI               C0JK   \n",
              "9       SurfaceVolumeRatio                            IBSI               2PR5   \n",
              "10             VoxelVolume                            IBSI               YEKZ   \n",
              "11            Compactness1                            IBSI               SKGS   \n",
              "12            Compactness2                            IBSI               BQWJ   \n",
              "13  SphericalDisproportion                            IBSI               KRCK   \n",
              "\n",
              "             quantity_CodeMeaning units_CodingSchemeDesignator  \\\n",
              "0                      Elongation                         UCUM   \n",
              "1                        Flatness                         UCUM   \n",
              "2         Least Axis in 3D Length                         UCUM   \n",
              "3         Major Axis in 3D Length                         UCUM   \n",
              "4   Maximum 3D Diameter of a Mesh                         UCUM   \n",
              "5                  Volume of Mesh                         UCUM   \n",
              "6         Minor Axis in 3D Length                         UCUM   \n",
              "7                      Sphericity                         UCUM   \n",
              "8            Surface Area of Mesh                         UCUM   \n",
              "9         Surface to Volume Ratio                         UCUM   \n",
              "10    Volume from Voxel Summation                         UCUM   \n",
              "11                  Compactness 1                         UCUM   \n",
              "12                  Compactness 2                         UCUM   \n",
              "13        Spherical Disproportion                         UCUM   \n",
              "\n",
              "   units_CodeValue  units_CodeMeaning  \n",
              "0               mm         millimeter  \n",
              "1               mm         millimeter  \n",
              "2               mm         millimeter  \n",
              "3               mm         millimeter  \n",
              "4               mm         millimeter  \n",
              "5              mm3  cubic millimeter   \n",
              "6               mm         millimeter  \n",
              "7                1           no units  \n",
              "8              mm2  square millimeter  \n",
              "9              /mm     per millimeter  \n",
              "10             mm3   cubic millimeter  \n",
              "11               1           no units  \n",
              "12               1           no units  \n",
              "13               1           no units  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30c44030-7efa-4627-b944-f1c0fb35eba9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>shape_feature</th>\n",
              "      <th>quantity_CodingSchemeDesignator</th>\n",
              "      <th>quantity_CodeValue</th>\n",
              "      <th>quantity_CodeMeaning</th>\n",
              "      <th>units_CodingSchemeDesignator</th>\n",
              "      <th>units_CodeValue</th>\n",
              "      <th>units_CodeMeaning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Elongation</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>Q3CK</td>\n",
              "      <td>Elongation</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flatness</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>N17B</td>\n",
              "      <td>Flatness</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LeastAxisLength</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>7J51</td>\n",
              "      <td>Least Axis in 3D Length</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MajorAxisLength</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>TDIC</td>\n",
              "      <td>Major Axis in 3D Length</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Maximum3DDiameter</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>L0JK</td>\n",
              "      <td>Maximum 3D Diameter of a Mesh</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MeshVolume</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>RNU0</td>\n",
              "      <td>Volume of Mesh</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm3</td>\n",
              "      <td>cubic millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MinorAxisLength</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>P9VJ</td>\n",
              "      <td>Minor Axis in 3D Length</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm</td>\n",
              "      <td>millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Sphericity</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>QCFX</td>\n",
              "      <td>Sphericity</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>1</td>\n",
              "      <td>no units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SurfaceArea</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>C0JK</td>\n",
              "      <td>Surface Area of Mesh</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm2</td>\n",
              "      <td>square millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SurfaceVolumeRatio</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>2PR5</td>\n",
              "      <td>Surface to Volume Ratio</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>/mm</td>\n",
              "      <td>per millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>VoxelVolume</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>YEKZ</td>\n",
              "      <td>Volume from Voxel Summation</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>mm3</td>\n",
              "      <td>cubic millimeter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Compactness1</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>SKGS</td>\n",
              "      <td>Compactness 1</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>1</td>\n",
              "      <td>no units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Compactness2</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>BQWJ</td>\n",
              "      <td>Compactness 2</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>1</td>\n",
              "      <td>no units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SphericalDisproportion</td>\n",
              "      <td>IBSI</td>\n",
              "      <td>KRCK</td>\n",
              "      <td>Spherical Disproportion</td>\n",
              "      <td>UCUM</td>\n",
              "      <td>1</td>\n",
              "      <td>no units</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30c44030-7efa-4627-b944-f1c0fb35eba9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30c44030-7efa-4627-b944-f1c0fb35eba9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30c44030-7efa-4627-b944-f1c0fb35eba9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rmdir $path_downloaded/$studyuid/$seriesuid\n",
        "!rmdir $path_downloaded/$studyuid/"
      ],
      "metadata": {
        "id": "SZ5zWL8hFUCx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metajson = create_structured_report_metajson_for_shape_features(seriesuid, \n",
        "                                                        sop,\n",
        "                                                        fn_seg_dcm, \n",
        "                                                        path_downloaded, \n",
        "                                                        seg_code_mapping_df,\n",
        "                                                        feature_code_mapping_df,\n",
        "                                                        df_out, \n",
        "                                                        'lungmask'\n",
        "                                                        )\n",
        "\n",
        "fn_sr_radiomics = os.path.join(path_sr, 'sr_radiomics_%s.dcm' % seriesuid)\n",
        "save_structured_report( metajson, fn_metadata_json, path_downloaded, fn_sr_radiomics, path_dcm )\n",
        "\n",
        "bucket_path_sr = \"structured_reports/\"\n",
        "\n",
        "file_gs_uri = \"gs://%s/%ssr_regions_%s.nrrd\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "  # continue\n",
        "\n",
        "cmd7 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_regions, bucket_name, bucket_path_sr)\n",
        "print(cmd7)\n",
        "os.system(cmd7)\n",
        "\n",
        "cmd8 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_landmarks, bucket_name, bucket_path_sr)\n",
        "print(cmd8)\n",
        "os.system(cmd8)\n",
        "\n",
        "cmd9 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_radiomics, bucket_name, bucket_path_sr)\n",
        "print(cmd9)\n",
        "os.system(cmd9)\n",
        "\n",
        "# Validate these SRs \n",
        "!bash DicomSRValidator.sh $fn_sr_regions\n",
        "!bash DicomSRValidator.sh $fn_sr_landmarks\n",
        "!bash DicomSRValidator.sh $fn_sr_radiomics"
      ],
      "metadata": {
        "id": "8m_3HT77S0EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5e1020-8e94-4095-8700-4df8579fac8c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segment_id: 0\n",
            "ReferencedSegment: 1\n",
            "FindingSite: Right lung\n",
            "segment_id: 1\n",
            "ReferencedSegment: 2\n",
            "FindingSite: Left lung\n",
            "wrote out json for shape features\n",
            "inputImageLibraryDirectory: /content/downloaded_data\n",
            "outputDICOM: /content/sr_data/sr_radiomics_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm\n",
            "inputCompositeContextDirectory: /content/dcm_data\n",
            "inputMetadata_json: /content/structured_report_metadata.json\n",
            "dcmqi repository URL: git@github.com:QIICR/dcmqi.git revision: 1153738 tag: v1.2.5\n",
            "Total measurement groups: 2\n",
            "Adding to compositeContext: /content/dcm_data/dcmseg_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm\n",
            "Composite Context has been initialized\n",
            "SR saved!\n",
            "wrote out SR for shape features\n",
            "Searching gs://midrc-analysis-bwh-dk/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.nrrd\n",
            "\n",
            "gsutil cp /content/sr_data/sr_regions_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm gs://midrc-analysis-bwh-dk/structured_reports/\n",
            "gsutil cp /content/sr_data/sr_landmarks_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm gs://midrc-analysis-bwh-dk/structured_reports/\n",
            "gsutil cp /content/sr_data/sr_radiomics_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm gs://midrc-analysis-bwh-dk/structured_reports/\n",
            "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
            "Found Comprehensive3DSR IOD\n",
            "Found Root Template TID_1500 (MeasurementReport)\n",
            "Root Template Validation Complete\n",
            "IOD validation complete\n",
            "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate these SRs \n",
        "!bash DicomSRValidator.sh $fn_sr_regions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFroFdMuFc7j",
        "outputId": "ac212e9c-4b3e-4777-cb85-2221ffb2d747"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:777)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:371)\n",
            "\tat com.pixelmed.validate.DicomSRValidator.validateFirstPass(DicomSRValidator.java:175)\n",
            "\tat com.pixelmed.validate.DicomSRValidator.validate(DicomSRValidator.java:263)\n",
            "\tat com.pixelmed.validate.DicomSRValidator.validate(DicomSRValidator.java:312)\n",
            "\tat com.pixelmed.validate.DicomSRValidator.main(DicomSRValidator.java:376)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash DicomSRValidator.sh $fn_sr_landmarks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjRqVPOBLNhM",
        "outputId": "12991e8a-2642-4e41-ab97-21573e1a5b3f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Comprehensive3DSR IOD\n",
            "Found Root Template TID_1500 (MeasurementReport)\n",
            "Root Template Validation Complete\n",
            "IOD validation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash DicomSRValidator.sh $fn_sr_radiomics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shOGvbF6L-oO",
        "outputId": "e14c557c-98b6-44da-f81a-0ffaa9685387"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:510)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:183)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:351)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2710)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:605)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:112)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:534)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:888)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:824)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1216)\n",
            "\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:635)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.dom.XSLTCDTMManager.getDTM(XSLTCDTMManager.java:420)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.dom.XSLTCDTMManager.getDTM(XSLTCDTMManager.java:214)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.getDOM(TransformerImpl.java:576)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.retrieveDocument(TransformerImpl.java:1372)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.dom.LoadDocument.document(LoadDocument.java:191)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.dom.LoadDocument.document(LoadDocument.java:149)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.dom.LoadDocument.documentF(LoadDocument.java:132)\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.findCodeMeaningInContextGroup()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.CheckContentItem()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.TID_1602()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.TID_1601()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.TID_1600()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.TID_1500()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.template$dot$15()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.applyTemplates()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.applyTemplates()\n",
            "\tat jdk.translet/die.verwandlung.GregorSamsa.transform()\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.runtime.AbstractTranslet.transform(AbstractTranslet.java:627)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:777)\n",
            "\tat java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:371)\n"
          ]
        }
      ]
    }
  ]
}