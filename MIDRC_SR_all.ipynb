{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakri201/MIDRC_colab/blob/main/MIDRC_SR_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameterization"
      ],
      "metadata": {
        "id": "iYKQnpDPr9j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project  bwh-midrc-rapid-res-1655321320"
      ],
      "metadata": {
        "id": "QZr5-mEOOxyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b9e72f-5c44-4a2c-a8b9-7d6e8d2c361b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kk0d3lMmDp3"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbM0Cf_94rqz"
      },
      "outputs": [],
      "source": [
        "project_name = \"bwh-midrc-rapid-res-1655321320\"\n",
        "# bucket_name = \"midrc-analysis-bwh\"\n",
        "bucket_name = 'midrc-analysis-bwh-dk'\n",
        "bucket_path = \"bpr-results/\"\n",
        "bucket_path_sr = \"structured_reports/\" "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "rbXDOIMLsAKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZYEUFWcW6pG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e1cdbd12-059c-4f8f-eb08-d747952ed8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting nptyping\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from pynrrd) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pynrrd) (4.1.1)\n",
            "Collecting numpy>=1.11.1\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 15.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, nptyping, pynrrd\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "Successfully installed nptyping-2.4.1 numpy-1.21.5 pynrrd-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#install nrrd\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j79_pPeVRjkB",
        "outputId": "3cb13988-ff2e-4324-e98a-cb82145aa726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 52.8 MB 2.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBBXdFGY-tqE",
        "outputId": "0bca3471-f8b9-41e4-a413-7da0f69a5f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Plastimatch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "!sudo apt install plastimatch \n",
        "!echo $(plastimatch --version)\n",
        "\n",
        "if os.path.isdir('/content/pyplastimatch'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/pyplastimatch')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"pyplastimatch\", err.strerror)) \n",
        "# !git clone https://github.com/denbonte/pyplastimatch/ pyplastimatch\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch.git \n",
        "\n",
        "# from pyplastimatch import pyplastimatch as pypla\n",
        "from pyplastimatch.pyplastimatch import pyplastimatch as pypla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIxqEpe5MOXS",
        "outputId": "29117be6-2258-43a9-c4b8-5a1cb9b43318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libdcmtk12 libdlib-data libdlib18 libfftw3-single3 libinsighttoolkit4.12\n",
            "  libnifti2\n",
            "Suggested packages:\n",
            "  libfftw3-bin libfftw3-dev\n",
            "The following NEW packages will be installed:\n",
            "  libdcmtk12 libdlib-data libdlib18 libfftw3-single3 libinsighttoolkit4.12\n",
            "  libnifti2 plastimatch\n",
            "0 upgraded, 7 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 76.0 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdcmtk12 amd64 3.6.2-3build3 [4,499 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdlib-data all 18.18-2build1 [63.4 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdlib18 amd64 18.18-2build1 [251 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-single3 amd64 3.3.7-1 [764 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libnifti2 amd64 2.0.0-2build1 [102 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libinsighttoolkit4.12 amd64 4.12.2-dfsg1-1ubuntu1 [4,301 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 plastimatch amd64 1.7.0+dfsg.1-1 [2,700 kB]\n",
            "Fetched 76.0 MB in 6s (13.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdcmtk12.\n",
            "(Reading database ... 123991 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdcmtk12_3.6.2-3build3_amd64.deb ...\n",
            "Unpacking libdcmtk12 (3.6.2-3build3) ...\n",
            "Selecting previously unselected package libdlib-data.\n",
            "Preparing to unpack .../1-libdlib-data_18.18-2build1_all.deb ...\n",
            "Unpacking libdlib-data (18.18-2build1) ...\n",
            "Selecting previously unselected package libdlib18.\n",
            "Preparing to unpack .../2-libdlib18_18.18-2build1_amd64.deb ...\n",
            "Unpacking libdlib18 (18.18-2build1) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../3-libfftw3-single3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libnifti2.\n",
            "Preparing to unpack .../4-libnifti2_2.0.0-2build1_amd64.deb ...\n",
            "Unpacking libnifti2 (2.0.0-2build1) ...\n",
            "Selecting previously unselected package libinsighttoolkit4.12.\n",
            "Preparing to unpack .../5-libinsighttoolkit4.12_4.12.2-dfsg1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libinsighttoolkit4.12 (4.12.2-dfsg1-1ubuntu1) ...\n",
            "Selecting previously unselected package plastimatch.\n",
            "Preparing to unpack .../6-plastimatch_1.7.0+dfsg.1-1_amd64.deb ...\n",
            "Unpacking plastimatch (1.7.0+dfsg.1-1) ...\n",
            "Setting up libdlib-data (18.18-2build1) ...\n",
            "Setting up libnifti2 (2.0.0-2build1) ...\n",
            "Setting up libdlib18 (18.18-2build1) ...\n",
            "Setting up libdcmtk12 (3.6.2-3build3) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Setting up libinsighttoolkit4.12 (4.12.2-dfsg1-1ubuntu1) ...\n",
            "Setting up plastimatch (1.7.0+dfsg.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "plastimatch version 1.7.0\n",
            "Cloning into 'pyplastimatch'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 97 (delta 32), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.58 MiB | 19.30 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyradiomics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrIXcD9fMVLH",
        "outputId": "20989c79-441f-4204-9f2f-f769a50e65f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyradiomics\n",
            "  Downloading pyradiomics-3.0.1-cp37-cp37m-manylinux1_x86_64.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.3.0)\n",
            "Collecting pykwalify>=1.6.0\n",
            "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (1.21.5)\n",
            "Requirement already satisfied: SimpleITK>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from pyradiomics) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.2)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting ruamel.yaml>=0.16.0\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 37.9 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 34.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c7a2c6aea6317d11e93fdad85595d1877299912d84f6e702104ea84f216d8bca\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, docopt, pykwalify, pyradiomics\n",
            "Successfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dcmqi \n",
        "!wget https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\n",
        "!tar zxvf dcmqi-1.2.5-linux.tar.gz\n",
        "!cp dcmqi-1.2.5-linux/bin/* /usr/local/bin/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTnwCWDdMb-z",
        "outputId": "2659229a-fb08-44fa-c32c-471d3ecc4605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-19 03:38:26--  https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/79d3ad95-9f0c-42a4-a1c5-bf5a63461894?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221119T033826Z&X-Amz-Expires=300&X-Amz-Signature=8c1b5ad6888750fc4d8c322b18bf5d879f1709aa5dcf90fd0181c9c1269ea3e0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.5-linux.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-19 03:38:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/79d3ad95-9f0c-42a4-a1c5-bf5a63461894?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221119T033826Z&X-Amz-Expires=300&X-Amz-Signature=8c1b5ad6888750fc4d8c322b18bf5d879f1709aa5dcf90fd0181c9c1269ea3e0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.5-linux.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21102129 (20M) [application/octet-stream]\n",
            "Saving to: ‘dcmqi-1.2.5-linux.tar.gz’\n",
            "\n",
            "dcmqi-1.2.5-linux.t 100%[===================>]  20.12M  37.3MB/s    in 0.5s    \n",
            "\n",
            "2022-11-19 03:38:27 (37.3 MB/s) - ‘dcmqi-1.2.5-linux.tar.gz’ saved [21102129/21102129]\n",
            "\n",
            "dcmqi-1.2.5-linux/bin/\n",
            "dcmqi-1.2.5-linux/bin/itkimage2segimage\n",
            "dcmqi-1.2.5-linux/bin/tid1500reader\n",
            "dcmqi-1.2.5-linux/bin/tid1500reader.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2segimage.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2paramap.xml\n",
            "dcmqi-1.2.5-linux/bin/itkimage2paramap\n",
            "dcmqi-1.2.5-linux/bin/segimage2itkimage.xml\n",
            "dcmqi-1.2.5-linux/bin/segimage2itkimage\n",
            "dcmqi-1.2.5-linux/bin/tid1500writer.xml\n",
            "dcmqi-1.2.5-linux/bin/tid1500writer\n",
            "dcmqi-1.2.5-linux/bin/paramap2itkimage\n",
            "dcmqi-1.2.5-linux/bin/paramap2itkimage.xml\n",
            "dcmqi-1.2.5-linux/share/\n",
            "dcmqi-1.2.5-linux/share/doc/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/itksys/\n",
            "dcmqi-1.2.5-linux/share/doc/ITK-4.10/itksys/Copyright.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImQTiwVnvxr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dicomweb-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKGUrG1iAo5f"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import six\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.cloud import storage\n",
        "import nrrd\n",
        "import SimpleITK as sitk\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import transforms\n",
        "from matplotlib.colors import ListedColormap\n",
        "import csv\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "\n",
        "import radiomics\n",
        "from radiomics import featureextractor  # This module is used for interaction with pyradiomics\n",
        "\n",
        "import nibabel as nib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall highdicom\n",
        "!git clone https://github.com/herrmannlab/highdicom.git\n",
        "#!cd highdicom && python setup.py instally\n",
        "!cd highdicom && pip install ."
      ],
      "metadata": {
        "id": "juNDFA2y988w",
        "outputId": "12bebdc3-fe02-445a-8bd7-20b2146d3b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping highdicom as it is not installed.\u001b[0m\n",
            "Cloning into 'highdicom'...\n",
            "remote: Enumerating objects: 5668, done.\u001b[K\n",
            "remote: Counting objects: 100% (2221/2221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (565/565), done.\u001b[K\n",
            "remote: Total 5668 (delta 1882), reused 1773 (delta 1576), pack-reused 3447\u001b[K\n",
            "Receiving objects: 100% (5668/5668), 3.13 MiB | 9.25 MiB/s, done.\n",
            "Resolving deltas: 100% (3685/3685), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/highdicom\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: pydicom>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (1.21.5)\n",
            "Requirement already satisfied: pillow>=8.3 in /usr/local/lib/python3.7/dist-packages (from highdicom==0.20.0) (9.3.0)\n",
            "Collecting pillow-jpls>=1.0\n",
            "  Downloading pillow_jpls-1.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 6.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: highdicom\n",
            "  Building wheel for highdicom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for highdicom: filename=highdicom-0.20.0-py3-none-any.whl size=800101 sha256=047f00ec352ee0fbbf01896601fef9f7aed71039fe84b5e391fbc308ff7df044\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xr00_nwk/wheels/2c/3f/a0/cadbe6603e979b07733495973de6e1f45b81d2295bf6a358a3\n",
            "Successfully built highdicom\n",
            "Installing collected packages: pillow-jpls, highdicom\n",
            "Successfully installed highdicom-0.20.0 pillow-jpls-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install dcmtk \n",
        "!cp /usr/bin/dcmodify /usr/local/bin/dcmodify"
      ],
      "metadata": {
        "id": "CN8-qVDJbT_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4882e50-4cf7-422e-e223-bdf66e69e85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  dcmtk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 892 kB of archives.\n",
            "After this operation, 3,511 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dcmtk amd64 3.6.2-3build3 [892 kB]\n",
            "Fetched 892 kB in 1s (667 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package dcmtk.\n",
            "(Reading database ... 124182 files and directories currently installed.)\n",
            "Preparing to unpack .../dcmtk_3.6.2-3build3_amd64.deb ...\n",
            "Unpacking dcmtk (3.6.2-3build3) ...\n",
            "Setting up dcmtk (3.6.2-3build3) ...\n",
            "Adding `dcmtk' group to system ...\n",
            "Adding `dcmtk' user to system ...\n",
            "adduser: Warning: The home directory `/var/lib/dcmtk/db' does not belong to the user you are currently creating.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for the structured report \n",
        "\n",
        "import highdicom\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import highdicom as hd\n",
        "\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "\n",
        "from highdicom.sr.content import (\n",
        "    FindingSite,\n",
        "    ImageRegion,\n",
        "    ImageRegion3D,\n",
        "    SourceImageForRegion,\n",
        "    SourceImageForMeasurement,\n",
        "    SourceImageForMeasurementGroup\n",
        ")\n",
        "from highdicom.sr.enum import GraphicTypeValues3D\n",
        "from highdicom.sr.enum import GraphicTypeValues\n",
        "from highdicom.sr.sop import Comprehensive3DSR, ComprehensiveSR\n",
        "from highdicom.sr.templates import (\n",
        "    DeviceObserverIdentifyingAttributes,\n",
        "    Measurement,\n",
        "    MeasurementProperties,\n",
        "    MeasurementReport,\n",
        "    MeasurementsAndQualitativeEvaluations,\n",
        "    ObservationContext,\n",
        "    ObserverContext,\n",
        "    PersonObserverIdentifyingAttributes,\n",
        "    PlanarROIMeasurementsAndQualitativeEvaluations,\n",
        "    RelationshipTypeValues,\n",
        "    TrackingIdentifier,\n",
        "    QualitativeEvaluation,\n",
        "    ImageLibrary,\n",
        "    ImageLibraryEntryDescriptors\n",
        ")\n",
        "from highdicom.sr.value_types import (\n",
        "    CodedConcept,\n",
        "    CodeContentItem,\n",
        ")\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"highdicom.sr.sop\")\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "GqSSJZ0ULbWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload files:\n",
        "# - bpr_regions_code_mapping\n",
        "# - bpr_landmarks_code_mapping\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ZjUBvc61FIuB",
        "outputId": "c450277a-74fc-49de-bb9e-93d6dda67720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6bb4233-3ad4-47b8-8b1e-7cae6b5090a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6bb4233-3ad4-47b8-8b1e-7cae6b5090a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bpr_landmarks_code_mapping.csv to bpr_landmarks_code_mapping.csv\n",
            "Saving bpr_regions_code_mapping.csv to bpr_regions_code_mapping.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload files:\n",
        "# - parameter file for radiomics \n",
        "# - meta json for segmentation\n",
        "# - segment code mapping\n",
        "# - shape feature code mapping\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "ZXLeRAamNJpj",
        "outputId": "b314932c-1eac-4546-8a89-815a3db5a502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dcf099cb-af56-45b6-8dde-91587b284064\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dcf099cb-af56-45b6-8dde-91587b284064\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lung_seg_meta.json to lung_seg_meta.json\n",
            "Saving param_ct.yaml to param_ct.yaml\n",
            "Saving segments_code_mapping.csv to segments_code_mapping.csv\n",
            "Saving shape_features_code_mapping.csv to shape_features_code_mapping.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1lGxWNoR-Mj"
      },
      "outputs": [],
      "source": [
        "# Wait to get from Nadya \n",
        "\n",
        "# get the param_ct.yaml file. \n",
        "\n",
        "# !wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/nnunet/data/nnunet_segments_code_mapping.csv\n",
        "# nnunet_segments_code_mapping_df = pd.read_csv(\"nnunet_segments_code_mapping.csv\")\n",
        "\n",
        "# !wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/nnunet/data/nnunet_shape_features_code_mapping.csv\n",
        "# nnunet_shape_features_code_mapping_df = pd.read_csv(\"nnunet_shape_features_code_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLoYZWLtu048"
      },
      "source": [
        "Get the csv that will contain the mapping for the BPR regions - this is needed for the creation of the structured report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cxp_ZyUu048"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_regions_code_mapping.csv\n",
        "bpr_regions_df = pd.read_csv(\"bpr_regions_code_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52EPGSwMu048"
      },
      "source": [
        "Get the csv that will contain the mapping for the BPR landmarks -- this is needed for the creation of the structured report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cl3YqTau048"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_landmarks_code_mapping.csv\n",
        "landmarks_df = pd.read_csv(\"bpr_landmarks_code_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for generating SRs"
      ],
      "metadata": {
        "id": "nwkXpgpHshMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR regions"
      ],
      "metadata": {
        "id": "lpxpOp2-yNWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes as input a json file and slice_index, returns the regions assigned to the slice \n",
        "def convert_slice_to_region(bpr_data, slice_index):\n",
        "\n",
        "  \"\"\" \n",
        "  Given the slice_index, this returns a list of corresponding regions that match\n",
        "\n",
        "  Inputs: \n",
        "    bpr_data    : a dictionary, where for each of the six regions, a list of \n",
        "                  slice indices are provided \n",
        "    slice_index : slice number you want to obtain the list of classified regions \n",
        "                  for\n",
        "  Returns\n",
        "    regions     : list of regions for that slice_index\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # find where the slice_index appears across all regions\n",
        "  # get the names of the regions\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  region_names = list(bpr_data.keys())\n",
        "  regions = [] \n",
        "\n",
        "  for n in range(0,num_regions):\n",
        "    vals = bpr_data[region_names[n]]\n",
        "    if slice_index in vals:\n",
        "      regions.append(region_names[n])\n",
        "\n",
        "  return regions"
      ],
      "metadata": {
        "id": "1KHbBceCei7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_structured_report_for_body_part_regression_regions(files, \n",
        "#                                                               json_file, \n",
        "#                                                               output_SR_file, \n",
        "#                                                               bpr_revision_number,\n",
        "#                                                               bpr_regions_df):\n",
        "\n",
        "#   \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "#      and writes a structured report (SR) to disk\n",
        "     \n",
        "#   Inputs: \n",
        "#     files               : list of CT dicom files \n",
        "#     json_file           : the json file created from the BodyPartRegression prediction\n",
        "#     output_SR_file      : output filename for the structured report \n",
        "#     bpr_revision_number : specific revision number of the bpr repo \n",
        "#     bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "#   Outputs:\n",
        "#     writes the SR out to the output_SR_file.    \n",
        "     \n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "#   num_files = len(files)\n",
        "#   # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "#   pos_all = []  \n",
        "#   sop_all = [] \n",
        "\n",
        "#   for n in range(0,num_files):\n",
        "#     # read dcm file \n",
        "#     filename = files[n]\n",
        "#     ds = dcmread(filename)\n",
        "#     # print(ds)\n",
        "\n",
        "#     # get ImageOrientation (0020, 0037)\n",
        "#     # print(ds['0x0020','0x0037'].value)\n",
        "#     ImageOrientation = ds['0x0020','0x0037'].value\n",
        "\n",
        "#     # get ImagePositionPatient (0020, 0032) \n",
        "#     ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "\n",
        "#     # calculate z value\n",
        "#     x_vector = ImageOrientation[0:3]\n",
        "#     y_vector = ImageOrientation[3:]\n",
        "#     z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "#     # multiple z_vector by ImagePositionPatient\n",
        "#     pos = np.dot(z_vector,ImagePositionPatient)\n",
        "#     pos_all.append(pos)\n",
        "\n",
        "#     # get the SOPInstanceUID \n",
        "#     sop = ds['0x0008', '0x0018'].value\n",
        "#     sop_all.append(sop)\n",
        "\n",
        "# #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "#   sorted_ind = np.argsort(pos_all)\n",
        "#   pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "#   sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "#   files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "#   #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "#   f = open(json_file)\n",
        "#   json_data = json.load(f)\n",
        "#   bpr_data = json_data['body part examined']\n",
        "\n",
        "#   # return a list where each entry is per slice and is a array of possible regions \n",
        "#   bpr_slice_scores = json_data['cleaned slice scores']\n",
        "#   num_slices = len(bpr_slice_scores)\n",
        "\n",
        "#   num_regions = len(bpr_data)\n",
        "#   regions = [] \n",
        "\n",
        "#   # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "#   for slice_index in range(0,num_slices):\n",
        "#     region = convert_slice_to_region(bpr_data, slice_index)\n",
        "#     regions.append(region)\n",
        "\n",
        "#   # ----- Create the structured report ----- # \n",
        "\n",
        "#   # Create the report content\n",
        "\n",
        "#   procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "#                                 meaning=\"Imaging procedure\")\n",
        "\n",
        "#   # Describe the context of reported observations: the person that reported\n",
        "#   # the observations and the device that was used to make the observations\n",
        "#   observer_person_context = ObserverContext(\n",
        "#       observer_type=codes.DCM.Person,\n",
        "#       observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "#           name='Anonymous^Reader'\n",
        "#       )\n",
        "#   )\n",
        "#   # observer_device_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Device,\n",
        "#   #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#   #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "#   #     )\n",
        "#   observer_device_context = ObserverContext(\n",
        "#     observer_type=codes.DCM.Device,\n",
        "#     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#         uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "#         model_name = bpr_revision_number\n",
        "#     )\n",
        "#   )\n",
        "#   observation_context = ObservationContext(\n",
        "#       #observer_person_context=observer_person_context,\n",
        "#       observer_device_context=observer_device_context,\n",
        "#   )\n",
        "\n",
        "#   imaging_measurements = []\n",
        "#   evidence = []\n",
        "\n",
        "#   tracking_uid = generate_uid()\n",
        "\n",
        "#   qualitative_evaluations = []\n",
        "\n",
        "#   print('num_slices: ' + str(num_slices))\n",
        "\n",
        "# #----------- Per slice ---------#\n",
        "\n",
        "#   for n in range(0,num_slices):\n",
        "\n",
        "#     slice_region = regions[n]\n",
        "\n",
        "#     # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "#     # ----- per region ---- # \n",
        "#     qualitative_evaluations = [] \n",
        "#     # num_regions = len(regions)\n",
        "#     num_regions = len(slice_region)\n",
        "\n",
        "#     # for region in regions: \n",
        "#     for region in slice_region: \n",
        "#       row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "#       qualitative_evaluations.append(\n",
        "#           QualitativeEvaluation(\n",
        "#               CodedConcept(\n",
        "#                             value=str(row[\"target_CodeValue\"].values[0]),\n",
        "#                             meaning=str(row[\"target_CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "#                             # meaning = \"Target Region\",\n",
        "#                             scheme_designator=str(row[\"target_CodingSchemeDesignator\"].values[0])\n",
        "#                             ), \n",
        "#               CodedConcept(\n",
        "#                             value=str(row[\"CodeValue\"].values[0]),\n",
        "#                             meaning=str(row[\"CodeMeaning\"].values[0]),\n",
        "#                             scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "#                           )\n",
        "#               )\n",
        "#           )\n",
        "\n",
        "#     # In the correct order \n",
        "#     reference_dcm_file = files_sorted[n]\n",
        "#     image_dataset = dcmread(reference_dcm_file)\n",
        "#     evidence.append(image_dataset)\n",
        "\n",
        "#     # NS\n",
        "#     #print(evidence[0])\n",
        "\n",
        "#     src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "#     # tracking_id = \"Annotations group x\"\n",
        "#     tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "#     measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "#                   tracking_identifier=TrackingIdentifier(\n",
        "#                       uid=tracking_uid,\n",
        "#                       identifier=tracking_id\n",
        "#                   ),\n",
        "#                   qualitative_evaluations=qualitative_evaluations,\n",
        "#                   source_images=[src_image]\n",
        "#               )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     imaging_measurements.append(\n",
        "#       measurements_group\n",
        "#             )\n",
        "    \n",
        "#   #-------------------#\n",
        "    \n",
        "#   measurement_report = MeasurementReport(\n",
        "#       observation_context=observation_context,\n",
        "#       procedure_reported=procedure_code,\n",
        "#       imaging_measurements=imaging_measurements\n",
        "#   )\n",
        "\n",
        "#   # Create the Structured Report instance\n",
        "#   series_instance_uid = generate_uid()\n",
        "#   sr_dataset = Comprehensive3DSR(\n",
        "#       evidence=evidence,\n",
        "#       content=measurement_report[0],\n",
        "#       series_number=100,\n",
        "#       series_instance_uid=series_instance_uid,\n",
        "#       sop_instance_uid=generate_uid(),\n",
        "#       instance_number=1,\n",
        "#       manufacturer='MIDRC',\n",
        "#       is_complete = True,\n",
        "#       is_final=True,\n",
        "#       series_description='BPR region annotations'\n",
        "#   )\n",
        "  \n",
        "\n",
        "\n",
        "#   pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "#   return sr_dataset"
      ],
      "metadata": {
        "id": "1opa_coW8an5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_regions(files, \n",
        "                                                              json_file, \n",
        "                                                              output_SR_file, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              bpr_regions_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "  # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # print(ds['0x0020','0x0037'].value)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value \n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  bpr_data = json_data['body part examined']\n",
        "\n",
        "  # return a list where each entry is per slice and is a array of possible regions \n",
        "  bpr_slice_scores = json_data['cleaned slice scores']\n",
        "  num_slices = len(bpr_slice_scores)\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  regions = [] \n",
        "\n",
        "  # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "  for slice_index in range(0,num_slices):\n",
        "    region = convert_slice_to_region(bpr_data, slice_index)\n",
        "    regions.append(region)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  observer_person_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Person,\n",
        "      observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "          name='Anonymous^Reader'\n",
        "      )\n",
        "  )\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "  #     )\n",
        "  observer_device_context = ObserverContext(\n",
        "    observer_type=codes.DCM.Device,\n",
        "    observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "        uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "        model_name = bpr_revision_number\n",
        "    )\n",
        "  )\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  # tracking_uid = generate_uid()\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  print('num_slices: ' + str(num_slices))\n",
        "\n",
        "\n",
        "  #----------- Per slice ---------#\n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    slice_region = regions[n]\n",
        "\n",
        "    # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "    # ----- per region ---- # \n",
        "    qualitative_evaluations = [] \n",
        "    finding_sites = [] \n",
        "    # num_regions = len(regions)\n",
        "    num_regions = len(slice_region)\n",
        "\n",
        "    # for region in regions: \n",
        "    for region in slice_region: \n",
        "      row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "\n",
        "      finding_sites.append(\n",
        "          FindingSite(\n",
        "              CodedConcept(\n",
        "                            value=str(row[\"CodeValue\"].values[0]), \n",
        "                            meaning=str(row[\"CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "                            scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "                            )\n",
        "              )\n",
        "          )\n",
        "      \n",
        "\n",
        "    # In the correct order \n",
        "    reference_dcm_file = files_sorted[n]\n",
        "    image_dataset = dcmread(reference_dcm_file)\n",
        "    evidence.append(image_dataset)\n",
        "\n",
        "    src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "    # tracking_id = \"Annotations group x\"\n",
        "    tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "    tracking_uid = generate_uid() # unique with tracking_id\n",
        "\n",
        "    measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                  tracking_identifier=TrackingIdentifier(\n",
        "                      uid=tracking_uid,\n",
        "                      identifier=tracking_id\n",
        "                  ),\n",
        "                  # qualitative_evaluations=qualitative_evaluations,\n",
        "                  finding_sites=finding_sites, \n",
        "                  source_images=[src_image]\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    imaging_measurements.append(\n",
        "      measurements_group\n",
        "            )\n",
        "    \n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=100,\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR region annotations'\n",
        "  )\n",
        "  # series_description='BPR region annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset"
      ],
      "metadata": {
        "id": "ziuVQi00ybPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR landmarks"
      ],
      "metadata": {
        "id": "tU89HKmmyAYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(json_file):\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "WmzQFV7_63U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_scores(scores, start_score, end_score):\n",
        "    scores = np.array(scores)\n",
        "    min_scores = np.where(scores < start_score)[0]\n",
        "    max_scores = np.where(scores > end_score)[0]\n",
        "\n",
        "    min_index = 0\n",
        "    max_index = len(scores)\n",
        "\n",
        "    if len(min_scores) > 0:\n",
        "        min_index = np.nanmax(min_scores)\n",
        "\n",
        "    if len(max_scores) > 0:\n",
        "        max_index = np.nanmin(max_scores)\n",
        "\n",
        "    return min_index, max_index"
      ],
      "metadata": {
        "id": "1PC6Zrvdh04p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indices_from_json(filename, tag_start, tag_end):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of the particular anatomy specified by the tag_start and \n",
        "  tag_end for a particular patient. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "    tag_start : required - the string for the start of the anatomical region \n",
        "    tag_end   : required - the string for the end of the anatomical region \n",
        "\n",
        "  Outputs:\n",
        "    min_index : the minimum index in the patient coordinate system\n",
        "    max_index : the maximum index in the patient coordinate system \n",
        "  \"\"\"\n",
        "\n",
        "  # These scores are the same for all patients  \n",
        "  x = load_json(filename)\n",
        "\n",
        "  start_score = x[\"look-up table\"][tag_start][\"mean\"]\n",
        "  end_score = x[\"look-up table\"][tag_end][\"mean\"]\n",
        "\n",
        "  # The actual indices \n",
        "  min_index, max_index = crop_scores(x[\"cleaned slice scores\"], start_score, end_score)\n",
        "\n",
        "  return min_index, max_index"
      ],
      "metadata": {
        "id": "9dkNVhl_xMIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_from_json(filename):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of each landmark in the patient json filename and converts \n",
        "  it to slice indices. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "\n",
        "  Outputs:\n",
        "    list of the corresponding landmark slices in the space of the patient. \n",
        "    Landmarks at the extreme ends - most inferior and most superior axial slices\n",
        "    are removed. \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the cleaned slice scores to determine the number of slices \n",
        "  x = load_json(filename)\n",
        "  num_slices = len(x[\"cleaned slice scores\"])\n",
        "\n",
        "  # Get the list of landmarks \n",
        "  landmarks = list(x[\"look-up table\"].keys())\n",
        "  num_landmarks = len(landmarks)\n",
        "\n",
        "  # Get the expected z_spacing - if less than 0, slices are in reverse order \n",
        "  valid_z_spacing = x[\"valid z-spacing\"]\n",
        "\n",
        "  # Get values for all tags \n",
        "  # Reorder the landmarks according to the mean values in ascending order \n",
        "  landmarks_dict_sorted = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    landmarks_dict_sorted[landmark] = x[\"look-up table\"][landmark]['mean']\n",
        "  landmark_dict_sorted = dict(sorted(landmarks_dict_sorted.items(), key=lambda item: item[1]))\n",
        "  landmarks = list(landmark_dict_sorted.keys())\n",
        "\n",
        "  # Calculate the actual slice indices of each landmark \n",
        "  landmark_indices = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    score = landmark_dict_sorted[landmark] # x[\"look-up table\"][landmark][\"mean\"]\n",
        "    min_index, max_index = crop_scores(x[\"cleaned slice scores\"], score, score)\n",
        "    if (valid_z_spacing > 0): \n",
        "      landmark_indices[landmark] = min_index \n",
        "    else: \n",
        "      landmark_indices[landmark] = num_slices - min_index \n",
        "\n",
        "  # Programmatically remove values from dictionary if it is at most inferior\n",
        "  # or most superior slice \n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    if (landmark_indices[landmark]==0):\n",
        "      del landmark_indices[landmark]\n",
        "    elif (landmark_indices[landmark]==num_slices-1):\n",
        "      del landmark_indices[landmark]\n",
        "\n",
        "  return landmark_indices"
      ],
      "metadata": {
        "id": "9XwAA1btxUen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_list_in_slice(landmark_indices, slice_number):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the list of landmarks that are assigned to a particular slice number.\n",
        "\n",
        "  Arguments:\n",
        "    flandmark_indices  : the dictionary holding the slice number for each \n",
        "                         landmark\n",
        "    slice_number       : the particular slice to obtain the list of landmarks\n",
        "                         for\n",
        "\n",
        "  Outputs:\n",
        "    list of the landmarks that correspond to the slice_number\n",
        "  \"\"\"\n",
        "\n",
        "  key_list = list()\n",
        "  items_list = landmark_indices.items()\n",
        "  for item in items_list:\n",
        "    if item[1] == slice_number:\n",
        "        key_list.append(item[0])\n",
        "\n",
        "  return key_list\n",
        "\n",
        "def convert_landmarks_list_to_qualitative_evaluations(landmark_list,\n",
        "                                                      landmarks_df):\n",
        "\n",
        "  \"\"\"\n",
        "  Converts the list of landmarks to a qualitative_evaluations for the \n",
        "  structured report. \n",
        "\n",
        "  Arguments:\n",
        "    landmark_list  : list of landmarks that correspond to a particular slice\n",
        "    landmarks_df   : the dataframe holding the bpr landmarks metadata needed to\n",
        "                     create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    list of QualitativeEvaluation\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  qualitative_evaluations = [] \n",
        "  num_landmarks_in_slice = len(landmark_list)\n",
        "  for landmark in landmark_list: \n",
        "    if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "      print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "      break\n",
        "    else:\n",
        "      landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "      # landmark_code = CodedConcept(value = str(int(landmark_row[\"CodeValue\"].values[0])),\n",
        "      #                              meaning = landmark_row[\"CodeMeaning\"].values[0],\n",
        "      #                              scheme_designator = landmark_row[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "      #print(landmarks_df[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_modifier_code = None\n",
        "      if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "        # landmark_modifier_code = CodedConcept(value = str(int(landmark_row[\"modifier_CodeValue\"].values[0])),\n",
        "        #                              meaning = landmark_row[\"modifier_CodeMeaning\"].values[0],\n",
        "        #                              scheme_designator = landmark_row[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "        #print(landmarks_df[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        qual = QualitativeEvaluation(CodedConcept(\n",
        "                value=\"123014\",\n",
        "                meaning=\"Target Region\",\n",
        "                scheme_designator=\"DCM\"  \n",
        "                ), \n",
        "                landmark_code\n",
        "              )\n",
        "    \n",
        "    if landmark_modifier_code is not None:\n",
        "      qual_modifier = CodeContentItem(\n",
        "          name=CodedConcept(\n",
        "              value='106233006',\n",
        "              meaning='Topographical modifier',\n",
        "              scheme_designator='SCT',\n",
        "          ),\n",
        "          value=landmark_modifier_code,\n",
        "          relationship_type=RelationshipTypeValues.HAS_CONCEPT_MOD\n",
        "      )\n",
        "      qual.append(qual_modifier)\n",
        "\n",
        "    qualitative_evaluations.append(qual)\n",
        "\n",
        "  return qualitative_evaluations\n"
      ],
      "metadata": {
        "id": "REfEiAnUxeop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "#                                                                 json_file, \n",
        "#                                                                 output_SR_file, \n",
        "#                                                                 bpr_revision_number,\n",
        "#                                                                 landmarks_df):\n",
        "\n",
        "#   \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "#      and writes a structured report (SR) to disk\n",
        "     \n",
        "#   Inputs: \n",
        "#     files               : list of CT dicom files \n",
        "#     json_file           : the json file created from the BodyPartRegression prediction\n",
        "#     output_SR_file      : output filename for the structured report \n",
        "#     bpr_revision_number : specific revision number of the bpr repo \n",
        "#     landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "#                           create the structured report\n",
        "\n",
        "#   Outputs:\n",
        "#     writes the SR out to the output_SR_file.    \n",
        "     \n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "#   num_files = len(files)\n",
        "\n",
        "#   pos_all = []  \n",
        "#   sop_all = [] \n",
        "\n",
        "#   for n in range(0,num_files):\n",
        "#     # read dcm file \n",
        "#     filename = files[n]\n",
        "#     ds = dcmread(filename)\n",
        "#     # print(ds)\n",
        "\n",
        "#     # get ImageOrientation (0020, 0037)\n",
        "#     # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "#     ImageOrientation = ds.ImageOrientationPatient\n",
        "#     #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "#     # get ImagePositionPatient (0020, 0032) \n",
        "#     # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "#     ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "#     # calculate z value\n",
        "#     x_vector = ImageOrientation[0:3]\n",
        "#     y_vector = ImageOrientation[3:]\n",
        "#     z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "#     # multiple z_vector by ImagePositionPatient\n",
        "#     pos = np.dot(z_vector,ImagePositionPatient)\n",
        "#     pos_all.append(pos)\n",
        "\n",
        "#     # get the SOPInstanceUID \n",
        "#     sop = ds['0x0008', '0x0018'].value\n",
        "#     sop_all.append(sop)\n",
        "\n",
        "#     #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "#   sorted_ind = np.argsort(pos_all)\n",
        "#   pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "#   sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "#   files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "#   #----- Get the landmarks as indices -----# \n",
        "\n",
        "#   landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "#   # ----- Create the structured report ----- # \n",
        "\n",
        "#   # Create the report content\n",
        "\n",
        "#   procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "#                                 meaning=\"Imaging procedure\")\n",
        "\n",
        "#   # Describe the context of reported observations: the person that reported\n",
        "#   # the observations and the device that was used to make the observations\n",
        "#   # observer_person_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Person,\n",
        "#   #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "#   #         name='Anonymous^Reader'\n",
        "#   #     )\n",
        "#   # )\n",
        "\n",
        "#   # observer_device_context = ObserverContext(\n",
        "#   #     observer_type=codes.DCM.Device,\n",
        "#   #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#   #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "#   #     )\n",
        "#   # )\n",
        "\n",
        "#   observer_device_context = ObserverContext(\n",
        "#       observer_type=codes.DCM.Device,\n",
        "#       observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "#           uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "#           model_name = bpr_revision_number\n",
        "#       )\n",
        "#   )\n",
        "#   # inputMetadata[\"observerContext\"] = {\n",
        "#   #                   \"ObserverType\": \"DEVICE\",\n",
        "#   #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "#   #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "#   #                 }\n",
        "\n",
        "#   observation_context = ObservationContext(\n",
        "#       #observer_person_context=observer_person_context,\n",
        "#       observer_device_context=observer_device_context,\n",
        "#   )\n",
        "\n",
        "#   imaging_measurements = []\n",
        "#   evidence = []\n",
        "\n",
        "#   qualitative_evaluations = []\n",
        "\n",
        "#   tracking_uid = generate_uid()\n",
        "\n",
        "#   #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "#   num_slices = len(files_sorted)\n",
        "#   annotation_count = 1 \n",
        "\n",
        "#   for n in range(0,num_slices):\n",
        "\n",
        "#     # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "#     landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "#     # Only include if there is a landmark for a slice \n",
        "#     if (landmark_indices_list):\n",
        "    \n",
        "#       # Create QualitativeEvaluations\n",
        "#       qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "#                                                                                   landmarks_df)\n",
        "\n",
        "#       # In the correct order \n",
        "#       reference_dcm_file = files_sorted[n]\n",
        "#       image_dataset = dcmread(reference_dcm_file)\n",
        "#       evidence.append(image_dataset)\n",
        "\n",
        "#       src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "#       # tracking_id = \"Annotations group x\"\n",
        "#       # tracking_id = \"Annotations group landmarks\"\n",
        "#       # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "#       tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "#       measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "#                     tracking_identifier=TrackingIdentifier(\n",
        "#                         uid=tracking_uid,\n",
        "#                         identifier=tracking_id\n",
        "#                     ),\n",
        "#                     qualitative_evaluations=qualitative_evaluations,\n",
        "#                     source_images=[src_image]\n",
        "#                 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       imaging_measurements.append(\n",
        "#         measurements_group\n",
        "#               )\n",
        "      \n",
        "#       annotation_count += 1 # keep track of number of annotations\n",
        "\n",
        "#        #-------------------#\n",
        "    \n",
        "#   measurement_report = MeasurementReport(\n",
        "#       observation_context=observation_context,\n",
        "#       procedure_reported=procedure_code,\n",
        "#       imaging_measurements=imaging_measurements\n",
        "#   )\n",
        "\n",
        "#   # Create the Structured Report instance\n",
        "#   series_instance_uid = generate_uid()\n",
        "#   sr_dataset = Comprehensive3DSR(\n",
        "#       evidence=evidence,\n",
        "#       content=measurement_report[0],\n",
        "#       series_number=101, # was 100 for regions\n",
        "#       series_instance_uid=series_instance_uid,\n",
        "#       sop_instance_uid=generate_uid(),\n",
        "#       instance_number=1,\n",
        "#       manufacturer='IDC',\n",
        "#       is_complete = True,\n",
        "#       is_final=True,\n",
        "#       series_description='BPR landmark annotations'\n",
        "#   )\n",
        "#   # series_description='BPR landmark annotations'\n",
        "\n",
        "#   pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "#   return sr_dataset"
      ],
      "metadata": {
        "id": "R6qri9J-xo3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "                                                                json_file, \n",
        "                                                                output_SR_file, \n",
        "                                                                bpr_revision_number,\n",
        "                                                                landmarks_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "                          create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "    #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID \n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #----- Get the landmarks as indices -----# \n",
        "\n",
        "  landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  # observer_person_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Person,\n",
        "  #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "  #         name='Anonymous^Reader'\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  observer_device_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Device,\n",
        "      observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "          uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "          model_name = bpr_revision_number\n",
        "      )\n",
        "  )\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                   \"ObserverType\": \"DEVICE\",\n",
        "  #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                 }\n",
        "\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  # qualitative_evaluations = []\n",
        "\n",
        "  # tracking_uid = generate_uid()\n",
        "\n",
        "  #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "  num_slices = len(files_sorted)\n",
        "  annotation_count = 1 \n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    finding_sites = []  \n",
        "\n",
        "    # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "    landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "    # Only include if there is a landmark for a slice \n",
        "    if (landmark_indices_list):\n",
        "      # Create QualitativeEvaluations\n",
        "      # qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "      #                                                                             landmarks_df)\n",
        "      num_landmarks_in_slice = len(landmark_indices_list)\n",
        "\n",
        "      for landmark in landmark_indices_list: \n",
        "        if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "          print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "          break\n",
        "        else:\n",
        "          landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "          landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                      meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                      scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "          landmark_modifier_code = None \n",
        "          # If there is a modifier, add it to finding_sites\n",
        "          if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "            landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                                  meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                                  scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "            finding_sites.append(\n",
        "                FindingSite(\n",
        "                    anatomic_location=landmark_code,\n",
        "                    topographical_modifier=landmark_modifier_code\n",
        "                )\n",
        "            )\n",
        "          # If no modifier, do not add to finding_sites \n",
        "          else: \n",
        "            finding_sites.append(\n",
        "                FindingSite(\n",
        "                    anatomic_location = landmark_code\n",
        "                    )\n",
        "                )\n",
        "\n",
        "      \n",
        "\n",
        "      # In the correct order \n",
        "      reference_dcm_file = files_sorted[n]\n",
        "      image_dataset = dcmread(reference_dcm_file)\n",
        "      evidence.append(image_dataset)\n",
        "\n",
        "      src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "      # tracking_id = \"Annotations group x\"\n",
        "      # tracking_id = \"Annotations group landmarks\"\n",
        "      # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "      tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "      tracking_uid = generate_uid()\n",
        "\n",
        "      measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                    tracking_identifier=TrackingIdentifier(\n",
        "                        uid=tracking_uid,\n",
        "                        identifier=tracking_id\n",
        "                    ),\n",
        "                    # qualitative_evaluations=qualitative_evaluations,\n",
        "                    finding_sites=finding_sites,\n",
        "                    source_images=[src_image]\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      imaging_measurements.append(\n",
        "        measurements_group\n",
        "              )\n",
        "      \n",
        "      annotation_count += 1 # keep track of number of annotations\n",
        "    \n",
        "\n",
        "\n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=101, # was 100 for regions\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR landmark annotations'\n",
        "  )\n",
        "  # series_description='BPR landmark annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "WR17ipkayize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SR radiomics"
      ],
      "metadata": {
        "id": "ovHTLDmYNb87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_and_names_from_metadata_json(dicomseg_json):\n",
        "\n",
        "  \"\"\"Returns two lists containing the label values and the corresponding\n",
        "     CodeMeaning values\n",
        "\n",
        "  Inputs: \n",
        "    dicomseg_json : metajson file\n",
        "\n",
        "  Outputs:\n",
        "    label_values  : label values from the metajson file \n",
        "    label_names   : the corresponding CodeMeaning values \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  f = open(dicomseg_json)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  #print(meta_json)\n",
        "\n",
        "  num_regions = len(meta_json['segmentAttributes'][0])\n",
        "  print ('num_regions: ' + str(num_regions))\n",
        "\n",
        "  label_values = []\n",
        "  label_names = [] \n",
        "  for n in range(0,num_regions):\n",
        "    # label_values.append(n)\n",
        "    label_value = meta_json['segmentAttributes'][0][n]['labelID']\n",
        "    #label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning']\n",
        "    # NS - \n",
        "    #label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning'] +'_'+str(label_value)\n",
        "    label_name = meta_json['segmentAttributes'][0][n]['SegmentDescription'] # Left lung, Right lung\n",
        "    label_values.append(label_value)\n",
        "    label_names.append(label_name)\n",
        "\n",
        "  return label_values, label_names"
      ],
      "metadata": {
        "id": "iO0T1wRcFVou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_nii(input_file, output_directory, label_names):\n",
        "\n",
        "  \"\"\"Function to split a single multilabel nii into individual nii files. Used\n",
        "     for pyradiomics feature extraction. \n",
        "\n",
        "  Inputs: \n",
        "    input_file       : input multi-label nii file \n",
        "    output_directory : where to save the individual nii segments \n",
        "    label_names      : the names of the labels that correspond to the order of \n",
        "                       the values in the nii input_file \n",
        "\n",
        "  Outputs:\n",
        "    saves the individual nii files to the output_directory \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.isdir(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "  # save with the values in the files \n",
        "  nii = nib.load(input_file)\n",
        "  header = nii.header \n",
        "  img = nii.get_fdata() \n",
        "  unique_labels = list(np.unique(img))\n",
        "  unique_labels.remove(0) # remove the background \n",
        "\n",
        "  # split and save \n",
        "  num_labels = len(unique_labels)\n",
        "  for n in range(0,num_labels):\n",
        "    ind = np.where(img==unique_labels[n])\n",
        "    vol = np.zeros((img.shape))\n",
        "    vol[ind] = 1\n",
        "    new_img = nib.Nifti1Image(vol, nii.affine, nii.header)\n",
        "    output_filename = os.path.join(output_directory, label_names[n] + '.nii.gz')\n",
        "    nib.save(new_img, output_filename)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "Rm49POmEFeQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    split_pred_nifti_path, \n",
        "                                    nnunet_shape_features_code_mapping_df):\n",
        "\n",
        "  \"\"\"Function to compute pyradiomics 3D features for each label in a nifti file. \n",
        "     \n",
        "\n",
        "  Inputs: \n",
        "    ct_nifti_path            : the CT nifti file \n",
        "    label_values             : the label value for each of the segments from the json file \n",
        "    label_names              : the corresponding label name for each of the segments \n",
        "    split_pred_nifti_path    : where to save the individual nii segments needed \n",
        "                               for pyradiomics\n",
        "    nnunet_shape_features_code_mapping_df : the df where we will obtain the \n",
        "                                            list of the shape features to \n",
        "                                            compute\n",
        "\n",
        "  Outputs:\n",
        "    Writes the features_csv_path_nnunet to disk. \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the names of the features from the nnunet_shape_features_code_mapping_df\n",
        "  shape_features = list(nnunet_shape_features_code_mapping_df['shape_feature'].values)\n",
        "\n",
        "  # Instantiate the extractor and modify the settings to keep the 3D shape features\n",
        "  extractor = featureextractor.RadiomicsFeatureExtractor(fn_param)\n",
        "  extractor.settings['minimumROIDimensions'] = 3 \n",
        "  extractor.disableAllFeatures()\n",
        "  extractor.enableFeaturesByName(shape=shape_features) \n",
        "\n",
        "  # Calculate features for each label and create a dataframe\n",
        "  num_labels = len([f for f in os.listdir(split_pred_nifti_path) if f.endswith('.nii.gz')]) # was .nii.gz\n",
        "  print(num_labels)\n",
        "  df_list = [] \n",
        "  for n in range(0,num_labels):\n",
        "    mask_path = os.path.join(split_pred_nifti_path, label_names[n] + '.nii.gz')  # was .nii.gz\n",
        "    \n",
        "    # Run the extractor \n",
        "    result = extractor.execute(ct_nifti_path, mask_path) # dictionary\n",
        "    # keep only the features we want\n",
        "    # Get the corresponding label number -- all might not be present \n",
        "    corresponding_label_value = label_values[label_names.index(label_names[n])] \n",
        "    dict_keep = {'ReferencedSegment': corresponding_label_value, \n",
        "                 'label_name': label_names[n]}\n",
        "    keys_keep = [f for f in result.keys() if 'original_shape' in f]\n",
        "    # Just keep the feature keys we want\n",
        "    dict_keep_new_values = {key_keep: result[key_keep] for key_keep in keys_keep}\n",
        "    dict_keep.update(dict_keep_new_values)\n",
        "    df1 = pd.DataFrame([dict_keep])\n",
        "    # change values of columns to remove original_shape_\n",
        "    df1.columns = df1.columns.str.replace('original_shape_', '')\n",
        "    # Append to the ReferencedSegment and label_name df \n",
        "    df_list.append(df1)\n",
        "\n",
        "  # concat all label features \n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "SBIdR3ZpKhd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted"
      ],
      "metadata": {
        "id": "HYjMey5mKq6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted"
      ],
      "metadata": {
        "id": "-bnpAyyWN8of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                         SOPInstanceUID_seg,\n",
        "                                                         seg_file, \n",
        "                                                         dcm_directory, \n",
        "                                                         segments_code_mapping_df,\n",
        "                                                         shape_features_code_mapping_df,\n",
        "                                                         df_features, \n",
        "                                                         SegmentAlgorithmName\n",
        "                                                         ):\n",
        "  \n",
        "  \"\"\"Function that creates the metajson necessary for the creation of a\n",
        "  structured report from a pandas dataframe of label names and features for \n",
        "  each. \n",
        "\n",
        "  Inputs: \n",
        "    SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                      file \n",
        "    SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "    seg_file                        : filename of SEG DCM file \n",
        "    dcm_directory                   : ct directory that will be sorted in \n",
        "                                      terms of axial ordering according to the \n",
        "                                      ImagePositionPatient and ImageOrientation \n",
        "                                      fields\n",
        "    segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                      segments and the associated code values etc.\n",
        "    shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                      features and the associated code values etc. \n",
        "    df_features                     : a pandas dataframe holding the segments and a \n",
        "                                      set of 3D shape features for each \n",
        "    SegmentAlgorithmName            : the name of the algorithm used to create the \n",
        "                                      segmentations - e.g. '3d_fullres_tta_nnUnet'\n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "  \"\"\" \n",
        "\n",
        "  # --- Get the version number for the pyradiomics package --- #\n",
        "\n",
        "  pyradiomics_version_number = str(radiomics.__version__)\n",
        "  \n",
        "  # --- Sort the dcm files first according to --- # \n",
        "  # --- ImagePositionPatient and ImageOrientation --- #\n",
        "\n",
        "  files_sorted, sop_all_sorted, pos_all_sorted = order_dicom_files_image_position(dcm_directory)\n",
        "  files_sorted = [os.path.basename(f) for f in files_sorted]\n",
        "\n",
        "  # --- Create the header for the json --- # \n",
        "  \n",
        "  inputMetadata = {}\n",
        "  inputMetadata[\"@schema\"]= \"https://raw.githubusercontent.com/qiicr/dcmqi/master/doc/schemas/sr-tid1500-schema.json#\"\n",
        "  # inputMetadata[\"SeriesDescription\"] = \"Measurements\"\n",
        "  inputMetadata[\"SeriesDescription\"] = SegmentAlgorithmName + '_' + \"Measurements\"\n",
        "  inputMetadata[\"SeriesNumber\"] = \"1001\"\n",
        "  inputMetadata[\"InstanceNumber\"] = \"1\"\n",
        "\n",
        "  inputMetadata[\"compositeContext\"] = [seg_file] # not full path\n",
        "\n",
        "  inputMetadata[\"imageLibrary\"] = files_sorted # not full path \n",
        "\n",
        "   # inputMetadata[\"observerContext\"] = {\n",
        "  #                                     \"ObserverType\": \"PERSON\",\n",
        "  #                                     \"PersonObserverName\": \"Reader1\"\n",
        "  #                                   }\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                     \"ObserverType\": \"DEVICE\",\n",
        "  #                     \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                     \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                   }\n",
        "  inputMetadata[\"observerContext\"] = {\n",
        "                      \"ObserverType\": \"DEVICE\",\n",
        "                      \"DeviceObserverName\": \"pyradiomics\",\n",
        "                      \"DeviceObserverModelName\": pyradiomics_version_number\n",
        "                    }\n",
        "\n",
        "  inputMetadata[\"VerificationFlag\"]  = \"UNVERIFIED\"\n",
        "  inputMetadata[\"CompletionFlag\"] =  \"COMPLETE\"\n",
        "  inputMetadata[\"activitySession\"] = \"1\"\n",
        "  inputMetadata[\"timePoint\"] = \"1\"\n",
        "\n",
        "  # ------------------------------------------------------------------------- # \n",
        "  # --- Create the measurement_dict for each segment - holds all features --- # \n",
        "\n",
        "  measurement = [] \n",
        "\n",
        "  # --- Now create the dict for all features and all segments --- #\n",
        "\n",
        "  # --- Loop over the number of segments --- #\n",
        "\n",
        "  # number of rows in the df_features \n",
        "  num_segments = df_features.shape[0]\n",
        "\n",
        "  # Array of dictionaries - one dictionary for each segment \n",
        "  measurement_across_segments_combined = [] \n",
        "\n",
        "  for segment_id in range(0,num_segments):\n",
        "\n",
        "    ReferencedSegment = df_features['ReferencedSegment'].values[segment_id]\n",
        "    FindingSite = df_features['label_name'].values[segment_id]\n",
        "\n",
        "    print('segment_id: ' + str(segment_id))\n",
        "    print('ReferencedSegment: ' + str(ReferencedSegment))\n",
        "    print('FindingSite: ' + str(FindingSite))\n",
        "\n",
        "    # --- Create the dict for the Measurements group --- # \n",
        "    TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment)\n",
        "\n",
        "    segment_row = segments_code_mapping_df[segments_code_mapping_df[\"segment\"] == FindingSite]\n",
        "    # print(segment_row)\n",
        "        \n",
        "    my_dict = {\n",
        "      \"TrackingIdentifier\": str(TrackingIdentifier),\n",
        "      \"ReferencedSegment\": int(ReferencedSegment),\n",
        "      \"SourceSeriesForImageSegmentation\": str(SeriesInstanceUID),\n",
        "      \"segmentationSOPInstanceUID\": str(SOPInstanceUID_seg),\n",
        "      \"Finding\": {\n",
        "        \"CodeValue\": \"113343008\",\n",
        "        \"CodingSchemeDesignator\": \"SCT\",\n",
        "        \"CodeMeaning\": \"Organ\"\n",
        "      }, \n",
        "      \"FindingSite\": {\n",
        "        \"CodeValue\": str(segment_row[\"FindingSite_CodeValue\"].values[0]),\n",
        "        \"CodingSchemeDesignator\": str(segment_row[\"FindingSite_CodingSchemeDesignator\"].values[0]),\n",
        "        \"CodeMeaning\": str(segment_row[\"FindingSite_CodeMeaning\"].values[0])\n",
        "      }\n",
        "    }\n",
        "\n",
        "    measurement = []  \n",
        "    # number of features - number of columns in df_features - 2 (label_name and ReferencedSegment)\n",
        "    num_values = len(df_features.columns)-2 \n",
        "\n",
        "    feature_list = df_features.columns[2:] # remove first two \n",
        "\n",
        "\n",
        "    # For each measurement per region segment\n",
        "    for n in range(0,num_values): \n",
        "      measurement_dict = {}\n",
        "      row = df_features.loc[df_features['label_name'] == FindingSite]\n",
        "      feature_row = shape_features_code_mapping_df.loc[shape_features_code_mapping_df[\"shape_feature\"] == feature_list[n]]\n",
        "      value = str(np.round(row[feature_list[n]].values[0],3))\n",
        "      measurement_dict[\"value\"] = value\n",
        "      measurement_dict[\"quantity\"] = {}\n",
        "      measurement_dict[\"quantity\"][\"CodeValue\"] = str(feature_row[\"quantity_CodeValue\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodingSchemeDesignator\"] = str(feature_row[\"quantity_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodeMeaning\"] = str(feature_row[\"quantity_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"units\"] = {}\n",
        "      measurement_dict[\"units\"][\"CodeValue\"] = str(feature_row[\"units_CodeValue\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodingSchemeDesignator\"] = str(feature_row[\"units_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodeMeaning\"] = str(feature_row[\"units_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"] = {}\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmName\"] = \"pyradiomics\"\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmVersion\"] = str(pyradiomics_version_number)\n",
        "      measurement.append(measurement_dict) \n",
        "\n",
        "    measurement_combined_dict = {}\n",
        "    measurement_combined_dict['measurementItems'] = measurement # measurement is an array of dictionaries \n",
        "\n",
        "    output_dict_one_segment = {**my_dict, **measurement_combined_dict}\n",
        "\n",
        "    # append to array for all segments \n",
        "\n",
        "    measurement_across_segments_combined.append(output_dict_one_segment)\n",
        "\n",
        "  # --- Add the measurement data --- # \n",
        "\n",
        "  inputMetadata[\"Measurements\"] = {}\n",
        "  inputMetadata[\"Measurements\"] = measurement_across_segments_combined\n",
        "\n",
        "  return inputMetadata\n"
      ],
      "metadata": {
        "id": "CDU5sEH3F9QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_structured_report( inputMetadata, sr_json_path, dcm_directory, sr_path, pred_dicomseg_path):\n",
        "  with open(sr_json_path, 'w') as f:\n",
        "    json.dump(inputMetadata, f, indent=2)\n",
        "  print ('wrote out json for shape features')\n",
        "\n",
        "  # --- Save the SR for nnUNet shape features --- # \n",
        "  # inputImageLibraryDirectory = os.path.join(\"/content\", \"raw\")\n",
        "  # outputDICOM = os.path.join(\"/content\",\"features_sr.dcm\")\n",
        "  # inputCompositeContextDirectory = os.path.join(\"/content\",\"seg\")\n",
        "  inputImageLibraryDirectory = dcm_directory\n",
        "  # outputDICOM = sr_json_path\n",
        "  outputDICOM = sr_path\n",
        "  # the name of the folder where the seg files are located \n",
        "  inputCompositeContextDirectory = pred_dicomseg_path  # os.path.basename(pred_dicomseg_path) # might need to check this\n",
        "  inputMetadata_json = sr_json_path \n",
        "\n",
        "  print ('inputImageLibraryDirectory: ' + str(inputImageLibraryDirectory))\n",
        "  print ('outputDICOM: ' + str(outputDICOM))\n",
        "  print ('inputCompositeContextDirectory: ' + str(inputCompositeContextDirectory))\n",
        "  print ('inputMetadata_json: ' + str(inputMetadata_json)) \n",
        "  !tid1500writer --inputImageLibraryDirectory $inputImageLibraryDirectory \\\n",
        "                --outputDICOM $outputDICOM  \\\n",
        "                --inputCompositeContextDirectory $inputCompositeContextDirectory \\\n",
        "                --inputMetadata $inputMetadata_json\n",
        "  print ('wrote out SR for shape features')\n"
      ],
      "metadata": {
        "id": "wIhPGH0HYkUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "3TrfBR8HsXgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR8c3D6j1Vvh"
      },
      "outputs": [],
      "source": [
        "# #%%bigquery --project=bwh-midrc-rapid-res-1655321320 ct_limited_open_a1_r1\n",
        "\n",
        "# # python API is the most flexible way to query IDC BigQuery metadata tables\n",
        "# from google.cloud import bigquery\n",
        "# bq_client = bigquery.Client(project_name)\n",
        "\n",
        "# selection_query = \"\"\"\n",
        "\n",
        "# WITH\n",
        "#   nlst_instances_per_series AS (\n",
        "#     SELECT\n",
        "#       DISTINCT(StudyInstanceUID),\n",
        "#       SeriesInstanceUID,\n",
        "#       COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "#       MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "#       MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#       MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "#       MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "#       STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#     FROM\n",
        "#       bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all\n",
        "#     WHERE\n",
        "#       (collection_id = \"Open-R1\" or collection_id = \"Open-A1\") and Modality = \"CT\"\n",
        "#     GROUP BY\n",
        "#       StudyInstanceUID,\n",
        "#       SeriesInstanceUID\n",
        "#       ), \n",
        "#   nlst_values_per_series AS (\n",
        "#     SELECT \n",
        "#     ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#     ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "#     ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_instances_per_series\n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "#   WHERE\n",
        "#     min_SliceThickness >= 1.5 \n",
        "#     AND max_SliceThickness <= 3.5 \n",
        "#     AND nlst_instances_per_series.num_instances > 100\n",
        "#     AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "#     AND nlst_instances_per_series.orientation_count = 1\n",
        "#     AND has_localizer = \"false\"\n",
        "#   GROUP BY\n",
        "#     SeriesInstanceUID\n",
        "#   )\n",
        "#   SELECT \n",
        "#     dicom_all.PatientID,\n",
        "#     dicom_all.StudyInstanceUID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     dicom_all.SOPInstanceUID,\n",
        "#     dicom_all.collection_id,\n",
        "#     dicom_all.PatientAge,\n",
        "#     dicom_all.PatientWeight,\n",
        "#     nlst_values_per_series.num_instances,\n",
        "#     nlst_values_per_series.SliceThickness,\n",
        "#     nlst_values_per_series.PatientHeightScanned\n",
        "#   FROM\n",
        "#     bwh-midrc-rapid-res-1655321320.midrc_dicom_us.dicom_all AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_values_per_series \n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID\n",
        "#  \"\"\"\n",
        "# selection_result = bq_client.query(selection_query)\n",
        "# ct_limited_open_a1_r1 = selection_result.result().to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the dataframe from the table instead of creating again. "
      ],
      "metadata": {
        "id": "5SY7AopGbn5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=project_name)\n",
        "table_id = '.'.join([project_name, dataset_table_id, table_view_id_name])\n",
        "\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    * \n",
        "  FROM\n",
        "    {table_id};\n",
        "  \"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "ct_limited_open_a1_r1 = result.to_dataframe(create_bqstorage_client=True)\n"
      ],
      "metadata": {
        "id": "W00I_l2bbS71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "6-cXmi6_s8sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_series(study_instance_uid, series_instance_uid, sop_instance_uids, dest_dir):\n",
        "  import pydicom\n",
        "  token = !gcloud auth print-access-token\n",
        "  token = token[0]\n",
        "\n",
        "  PROJECT_ID=\"bwh-midrc-rapid-res-1655321320\"\n",
        "  REGION=\"us-central1\"\n",
        "\n",
        "  DATASET_ID=\"midrc\"\n",
        "  DICOM_STORE_ID=\"midrc-dicom\"\n",
        "  \n",
        "  my_project = \"bwh-midrc-rapid-res-1655321320\"\n",
        "  location = \"us-central1\"\n",
        "  dataset_id = \"midrc\"\n",
        "  dicom_store_id = \"midrc-dicom\"\n",
        "\n",
        "  # url = f\"https://healthcare.googleapis.com/v1/projects/{my_project}/locations/{location}/datasets/{dataset_id}/dicomStores/{dicom_store_id}/dicomWeb\"\n",
        "  # headers = {\n",
        "  #     \"Authorization\" : \"Bearer %s\" % token\n",
        "  # }\n",
        "\n",
        "  # import dicomweb_client\n",
        "\n",
        "  # client = dicomweb_client.api.DICOMwebClient(url, headers=headers)\n",
        "\n",
        "  # idx=0\n",
        "  # for sop_instance_uid in sop_instance_uids:\n",
        "  #   retrievedInstance = client.retrieve_instance(\n",
        "  #               study_instance_uid=study_instance_uid,\n",
        "  #               series_instance_uid=series_instance_uid,\n",
        "  #               sop_instance_uid=sop_instance_uid)\n",
        "  #   pydicom.filewriter.write_file(f\"{dest_dir}/file{idx}.dcm\", retrievedInstance)\n",
        "  #   idx+=1\n",
        "\n",
        "  url = os.path.join('https://healthcare.googleapis.com/v1', \n",
        "                     'projects', my_project, \n",
        "                     'locations', 'us-central1', \n",
        "                     'datasets', dataset_id, \n",
        "                     'dicomStores', dicom_store_id, \n",
        "                     'dicomWeb/')\n",
        "  url_study_and_series = os.path.join('studies', \n",
        "                                      study_instance_uid,\n",
        "                                      'series', \n",
        "                                      series_instance_uid)\n",
        "  print (url)\n",
        "  print(url_study_and_series)\n",
        "\n",
        "  !dcmweb -m $url retrieve $url_study_and_series $dest_dir\n",
        "  \n",
        "  input_dir = os.path.join(dest_dir, study_instance_uid, series_instance_uid)\n",
        "  !mv $input_dir/* $dest_dir"
      ],
      "metadata": {
        "id": "KE05zkHQnxrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q6ovBXnRWeY"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching %s for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "  \n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "urllib3_logger = logging.getLogger('urllib3')\n",
        "urllib3_logger.setLevel(logging.CRITICAL) # suppress messages upon download"
      ],
      "metadata": {
        "id": "RQjDEsQN_z71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.cloud import storage\n",
        "\n",
        "# storage_client = storage.Client(project = project_name)\n",
        "# #bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "# series_instance_uids = []\n",
        "\n",
        "# #blobs = storage_client.list_blobs(bucket)\n",
        "# blobs = storage_client.list_blobs(bucket_name, prefix=bucket_path, delimiter='/')\n",
        "# for blob in blobs:\n",
        "#     bn = blob.name \n",
        "#     # ex: bpr-results/1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.json\n",
        "#     (r, ext) = os.path.splitext(bn)\n",
        "#     #print(r,ext)\n",
        "#     if ext == '.json':\n",
        "#       u = os.path.split(r)[-1]\n",
        "#       #print(u)\n",
        "#       series_instance_uids.append(u)\n",
        "\n",
        "# print(series_instance_uids[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOCZFuo4YG6C",
        "outputId": "9dbace31-bdd8-445f-f579-9b7daee7d2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334', '1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715', '1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952', '1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947', '1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563', '1.2.826.0.1.3680043.10.474.419639.198735019931123383691750806063', '1.2.826.0.1.3680043.10.474.419639.249044315484665760654506668895', '1.2.826.0.1.3680043.10.474.419639.259584978733948574762940092562', '1.2.826.0.1.3680043.10.474.419639.269534881585852761235289087419', '1.2.826.0.1.3680043.10.474.419639.272117657178318732150423166273']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = ct_limited_open_a1_r1\n",
        "# k=0\n",
        "\n",
        "# good_seriesuids = []\n",
        "\n",
        "# #series_instance_uids = ['1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715']\n",
        "\n",
        "# for seriesuid in series_instance_uids:\n",
        "#   #print(seriesuid)\n",
        "#   studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "#   if len(studyuids)>0:\n",
        "#     k=k+1\n",
        "#     #print(seriesuid)\n",
        "#     studyuid = studyuids[0]\n",
        "#     sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "#     print(len(sops))\n",
        "#     #download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "#     good_seriesuids.append(seriesuid)\n",
        "# print(k)"
      ],
      "metadata": {
        "id": "4xxeSrvYoouz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of dcmseg files from the bucket, this will tell us the series to process. \n"
      ],
      "metadata": {
        "id": "33uca58n3RC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series_instance_uids = list(set(ct_limited_open_a1_r1['SeriesInstanceUID'].values))\n",
        "\n",
        "bucket_seg_folder_uri_bpr = os.path.join(\"gs://\", bucket_name, bucket_path)\n",
        "print(\"bucket_seg_folder_uri_bpr: \" + str(bucket_seg_folder_uri_bpr))\n",
        "\n",
        "seg_bucket_list = listdir_bucket(project_name = project_name, # or gcs.listdir_bucket\n",
        "                                 bucket_name = bucket_name,\n",
        "                                 dir_gs_uri = bucket_json_folder_uri_bpr)\n",
        "\n",
        "good_seriesuids = [f.split(\".dcm\")[0] for f in seg_bucket_list if f.endswith(\".dcm\")] # or seg_ if ends with .nrrd\n",
        "good_seriesuids = [f.split(\"dcmseg_\")[1] for f in good_seriesuids]\n",
        "\n",
        "print(len(good_seriesuids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv5qKGhW17NO",
        "outputId": "586ffe81-bfaa-4ddf-a8b5-a0ea9922001a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bucket_json_folder_uri_bpr: gs://midrc-analysis-bwh-dk/bpr-results/\n",
            "Getting the list of files at `gs://midrc-analysis-bwh-dk/bpr-results/`...\n",
            "379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now get the files that should be processed that don't have DICOM SR files."
      ],
      "metadata": {
        "id": "LwkwQChR3Us_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series_id_list = good_seriesuids\n",
        "\n",
        "# exclude from processing all the patients for which a DICOM SEG object was exported already\n",
        "# (stored in the specified Google Cloud Storage Bucket)\n",
        "bucket_sr_folder_uri_bpr = os.path.join(\"gs://\", bucket_name, bucket_path)\n",
        "print(\"bucket_sr_folder_uri_bpr: \" + str(bucket_sr_folder_uri_bpr))\n",
        "\n",
        "sr_bucket_list = listdir_bucket(project_name = project_name, # or gcs.listdir_bucket\n",
        "                                  bucket_name = bucket_name,\n",
        "                                  dir_gs_uri = bucket_sr_folder_uri_bpr)\n",
        "\n",
        "# already_processed_id_list = [f.split(\"_SEG\")[0] for f in json_bucket_list if f.endswith(\".json\")]\n",
        "# already_processed_id_list = [f.split(\".json\")[0] for f in json_bucket_list if f.endswith(\".json\")]\n",
        "\n",
        "# already_processed_id_list = [f.split(\".dcm\")[0] for f in json_bucket_list if f.endswith(\".dcm\")] # or seg_ if ends with .nrrd\n",
        "# already_processed_id_list = [f.split(\"dcmseg_\")[1] for f in already_processed_id_list]\n",
        "\n",
        "already_processed_id_list = [f.split(\".dcm\")[0] for f in sr_bucket_list if f.endswith(\".dcm\")]\n",
        "already_processed_id_list = [f.split(\"sr_landmarks_\")[1] for f in already_processed_id_list]\n",
        "\n",
        "print(\"\\nFound %g series already processed.\"%(len(already_processed_id_list)))\n",
        "\n",
        "series_to_process_id_list = sorted(list(set(series_id_list) - set(already_processed_id_list)))\n",
        "\n",
        "print(\"Moving on with the remaining %g...\"%(len(series_to_process_id_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuyfFHyvcha1",
        "outputId": "0a7abf7f-83a5-4d3f-fcf4-57d83aa157af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bucket_seg_folder_uri_bpr: gs://midrc-analysis-bwh-dk/bpr-results/\n",
            "Getting the list of files at `gs://midrc-analysis-bwh-dk/bpr-results/`...\n",
            "\n",
            "Found 1 series already processed.\n",
            "Moving on with the remaining 378...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing series\n",
        "\n",
        "path_downloaded = '/content/downloaded_data'\n",
        "path_nifti = '/content/nifti_data'\n",
        "path_json =  '/content/json_data'\n",
        "path_nrrd = '/content/nrrd_data'\n",
        "path_sr = '/content/sr_data'\n",
        "path_dcm = '/content/dcm_data'\n",
        "path_labels = '/content/labels'\n",
        "\n",
        "df = ct_limited_open_a1_r1\n",
        "\n",
        "#good_seriesuids = ['1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952']\n",
        "\n",
        "# for seriesuid in good_seriesuids:\n",
        "for seriesuid in series_to_process_id_list[0:1]: \n",
        "\n",
        "  # clean up from previous iterations and recreate temp directories\n",
        "  if 1:\n",
        "    for x in [path_downloaded, path_nifti, path_json, path_nrrd, path_sr, path_dcm, path_labels]:\n",
        "      if os.path.isdir(x):\n",
        "        try:\n",
        "          shutil.rmtree(x)\n",
        "        except OSError as err:\n",
        "          print(\"Error: %s : %s\" % (x, err.strerror))  \n",
        "      os.mkdir(x)\n",
        "\n",
        "  studyuids = list(set(df[df['SeriesInstanceUID']==seriesuid]['StudyInstanceUID'].tolist()))\n",
        "  if len(studyuids)>0:\n",
        "    print(seriesuid)\n",
        "  studyuid = studyuids[0]\n",
        "  sops = df[ (df['StudyInstanceUID']==studyuid) & (df['SeriesInstanceUID']==seriesuid) ]['SOPInstanceUID'].tolist()\n",
        "  print(len(sops))\n",
        "\n",
        "  # bucket_path_sr = \"structured_reports/\"\n",
        "  \n",
        "  # file_gs_uri = \"gs://%s/%ssr_regions_%s.dcm\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "  # if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  #   print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "  #   continue\n",
        "\n",
        "  ### Download data  ###\n",
        "  download_series(studyuid, seriesuid, sops, path_downloaded)\n",
        "\n",
        "  ### Copy bpr json file ### \n",
        "  file_gs_uri_json = \"gs://%s/%s%s.json\" % (bucket_name, bucket_path, seriesuid)\n",
        "  cmd1 = \"gsutil cp %s %s\" % (file_gs_uri_json, path_json)\n",
        "  print(cmd1)\n",
        "  os.system(cmd1)\n",
        "\n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  #print(dcm_input_list)\n",
        "  \n",
        "  ### patch missing DICOM fields ###\n",
        "\n",
        "  cmd2 = \"dcmodify -i '(0010,0030)=20000101' /content/downloaded_data/*.dcm\"\n",
        "  print(cmd2)\n",
        "  os.system(cmd2)\n",
        "  \n",
        "  cmd3 = \"dcmodify -i '(0010,0040)=O' /content/downloaded_data/*.dcm\"\n",
        "  print(cmd3)\n",
        "  os.system(cmd3)\n",
        "  \n",
        "  ### create output filenames ### \n",
        "\n",
        "  dcm_input_list = glob.glob('/content/downloaded_data/*.dcm')\n",
        "  fn_json = os.path.join(path_json, '%s.json' % seriesuid)\n",
        "  fn_sr_regions = os.path.join(path_sr, 'sr_regions_%s.dcm' % seriesuid)\n",
        "  fn_sr_landmarks = os.path.join(path_sr, 'sr_landmarks_%s.dcm' % seriesuid)\n",
        "  df_regions = pd.read_csv('/content/bpr_regions_code_mapping.csv')\n",
        "  df_landmarks = pd.read_csv('/content/bpr_landmarks_code_mapping.csv')\n",
        "\n",
        "  file_gs_uri_seg = \"gs://%s/%sseg_%s.nrrd\" % (bucket_name, bucket_path, seriesuid) # \"/\"  exists in the path\n",
        "  file_gs_uri_ct = \"gs://%s/%sct_%s.nrrd\" % (bucket_name, bucket_path, seriesuid)\n",
        "  file_gs_uri_dcmseg = \"gs://%s/%sdcmseg_%s\" % (bucket_name, bucket_path, seriesuid)\n",
        "  \n",
        "  ### copy the seg, ct nrrd and dicom seg previously created ### \n",
        "\n",
        "  cmd4 = \"gsutil cp %s %s\" % (file_gs_uri_seg, path_nrrd)\n",
        "  print(cmd4)\n",
        "  os.system(cmd4)\n",
        "  \n",
        "  cmd5 = \"gsutil cp %s %s\" % (file_gs_uri_ct, path_nrrd)\n",
        "  print(cmd5)\n",
        "  os.system(cmd5)\n",
        "  \n",
        "  cmd6 = \"gsutil cp %s %s\" % (file_gs_uri_dcmseg, path_dcm)\n",
        "  print(cmd6)\n",
        "  os.system(cmd6)\n",
        "\n",
        "  fn_seg_nrrd = os.path.join(path_nrrd, 'seg_%s.nrrd' % seriesuid)\n",
        "  fn_ct_nrrd = os.path.join(path_nrrd, 'ct_%s.nrrd' % seriesuid)\n",
        "  fn_param = os.path.join('/content','param_ct.yaml')\n",
        "\n",
        "  # lung masks\n",
        "  lungs, _ = nrrd.read(fn_seg_nrrd)\n",
        "  tempr =np.where(lungs==1) \n",
        "  templ =np.where(lungs==2) \n",
        "\n",
        "  #print(templ)\n",
        "  #print(tempr)\n",
        "  print(len(templ[0]), len(tempr[0]))\n",
        "\n",
        "  if len(templ[0]) == 0 or len(tempr[0]) == 0:\n",
        "    print(\"SKIP NO LUNG\", seriesuid)\n",
        "    continue \n",
        "\n",
        "  print('converting NRRD to NII')\n",
        "  fn_ct_nifti = os.path.join(path_nifti,  \"ct_%s.nii\" % seriesuid)\n",
        "  log_file_path_ct_nifti = os.path.join(path_nifti, 'pypla_ct.log')\n",
        "  convert_args_ct = {\"input\" : fn_ct_nrrd, \"output-img\" : fn_ct_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_ct_nifti, **convert_args_ct)\n",
        "  \n",
        "  fn_seg_nifti = os.path.join(path_nifti,  \"seg_%s.nii\" % seriesuid)\n",
        "  log_file_path_seg_nifti = os.path.join(path_nifti, 'pypla_seg.log')\n",
        "  convert_args_seg = {\"input\" : fn_seg_nrrd, \"interpolation\" : \"nn\", \"output-img\" : fn_seg_nifti}\n",
        "  verbose = True\n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_seg_nifti, **convert_args_seg)\n",
        "\n",
        "  fn_seg_dcm = os.path.join(path_dcm, 'dcmseg_%s' % seriesuid)\n",
        "  fn_seg_code_mapping = os.path.join('/content','segments_code_mapping.csv')\n",
        "  fn_feature_code_mapping = os.path.join('/content','shape_features_code_mapping.csv')\n",
        "  fn_seg_meta = os.path.join('/content','lung_seg_meta.json')\n",
        "\n",
        "  seg_code_mapping_df = pd.read_csv(fn_seg_code_mapping)\n",
        "  feature_code_mapping_df = pd.read_csv(fn_feature_code_mapping)\n",
        "\n",
        "  label_values, label_names = get_label_and_names_from_metadata_json(fn_seg_meta)\n",
        "  print(label_values, label_names)\n",
        "  split_nii( fn_seg_nifti , path_labels, label_names)\n",
        "\n",
        "  ds = dcmread(fn_seg_dcm)\n",
        "  sop = ds.SOPInstanceUID\n",
        "  print(sop)\n",
        "\n",
        "  create_structured_report_for_body_part_regression_regions(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_regions, \n",
        "                                                              '1.1.0',\n",
        "                                                              df_regions)\n",
        "  \n",
        "  create_structured_report_for_body_part_regression_landmarks(dcm_input_list, \n",
        "                                                              fn_json, \n",
        "                                                              fn_sr_landmarks, \n",
        "                                                              '1.1.0',\n",
        "                                                              df_landmarks)\n",
        "  \n",
        "  df_out = compute_pyradiomics_3D_features(fn_ct_nifti, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    path_labels, \n",
        "                                    feature_code_mapping_df)\n",
        "  \n",
        "  metajson = create_structured_report_metajson_for_shape_features(seriesuid, \n",
        "                                                         sop,\n",
        "                                                         fn_seg_dcm, \n",
        "                                                         path_downloaded, \n",
        "                                                         seg_code_mapping_df,\n",
        "                                                         feature_code_mapping_df,\n",
        "                                                         df_out, \n",
        "                                                         'lungmask'\n",
        "                                                         )\n",
        "  \n",
        "  fn_sr_radiomics = os.path.join(path_sr, 'sr_radiomics_%s.dcm' % seriesuid)\n",
        "  fn_metadata_json = '/content/structured_report_metadata.json'\n",
        "  save_structured_report( metajson, fn_metadata_json, path_downloaded, fn_sr_radiomics, path_dcm )\n",
        "\n",
        "  bucket_path_sr = \"structured_reports/\"\n",
        "  \n",
        "  file_gs_uri = \"gs://%s/%ssr_regions_%s.nrrd\" % (bucket_name, bucket_path_sr, seriesuid) # \"/\"  exists in the path\n",
        "  if file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "    print(\"SKIP EXISTING GOOD\", seriesuid)\n",
        "    continue\n",
        "\n",
        "  cmd7 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_regions, bucket_name, bucket_path_sr)\n",
        "  print(cmd7)\n",
        "  os.system(cmd7)\n",
        "\n",
        "  cmd8 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_landmarks, bucket_name, bucket_path_sr)\n",
        "  print(cmd8)\n",
        "  os.system(cmd8)\n",
        "\n",
        "  cmd9 = \"gsutil cp %s gs://%s/%s\" % (fn_sr_radiomics, bucket_name, bucket_path_sr)\n",
        "  print(cmd9)\n",
        "  os.system(cmd9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4owL01weBFvx",
        "outputId": "5e387235-bd0b-47e5-de7e-37d3b96e29fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334\n",
            "178\n",
            "Searching gs://midrc-analysis-bwh/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334.dcm\n",
            "\n",
            "SKIP EXISTING GOOD 1.2.826.0.1.3680043.10.474.419639.105799060738901793068313281334\n",
            "1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715\n",
            "180\n",
            "Searching gs://midrc-analysis-bwh/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715.dcm\n",
            "\n",
            "SKIP EXISTING GOOD 1.2.826.0.1.3680043.10.474.419639.106364025147079899289440200715\n",
            "1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952\n",
            "145\n",
            "Searching gs://midrc-analysis-bwh/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952.dcm\n",
            "\n",
            "SKIP EXISTING GOOD 1.2.826.0.1.3680043.10.474.419639.149051607502633615235577977952\n",
            "1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947\n",
            "193\n",
            "Searching gs://midrc-analysis-bwh/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947.dcm\n",
            "\n",
            "SKIP EXISTING GOOD 1.2.826.0.1.3680043.10.474.419639.189346812051260775638656981947\n",
            "1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563\n",
            "111\n",
            "Searching gs://midrc-analysis-bwh/ for: \n",
            "structured_reports/sr_regions_1.2.826.0.1.3680043.10.474.419639.192916356998524553834723357563.dcm\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m_3HT77S0EL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}